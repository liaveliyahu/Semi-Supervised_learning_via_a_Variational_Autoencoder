{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrHJd2uE0NUr",
        "outputId": "382d06f9-453d-4e9f-fb19-d40eab5c9a37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cpu\n"
          ]
        }
      ],
      "source": [
        "#################### Import Modules ####################\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import FashionMNIST, MNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.utils import check_random_state\n",
        "import pickle\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "\n",
        "# Set the device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device: \", device)\n",
        "# Set the device globally\n",
        "torch.set_default_device(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "##################### Random Seed ######################\n",
        "torch.manual_seed(42)\n",
        "random_state = check_random_state(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "###################### Load Data #######################\n",
        "def load_data(labeled_size=100, batch_size=64, dataset=\"FashionMNIST\"):\n",
        "    if dataset == \"FashionMNIST\":\n",
        "        train_dataset = FashionMNIST(root='./data', train=True, transform=ToTensor(), download=True)\n",
        "        test_dataset = FashionMNIST(root='./data', train=False, transform=ToTensor())\n",
        "    elif dataset == \"MNIST\":\n",
        "        train_dataset = MNIST(root='./data', train=True, transform=ToTensor(), download=True)\n",
        "        test_dataset = MNIST(root='./data', train=False, transform=ToTensor())\n",
        "    unlabeled_size = len(train_dataset) - labeled_size\n",
        "    ul_train_dataset, l_train_dataset = random_split(train_dataset, [unlabeled_size, labeled_size])\n",
        "\n",
        "    ul_train_loader = DataLoader(ul_train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    l_train_loader = DataLoader(l_train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    return ul_train_loader, l_train_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Semi-Supervised Variational Autoencoder\n",
        "http://arxiv.org/abs/1406.5298"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "##################### Build Model ######################\n",
        "class M1(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(M1, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.Softplus(),\n",
        "            nn.Linear(hidden_dim, 2 * latent_dim),  # Two outputs for mean and log variance\n",
        "            nn.Softplus()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim),\n",
        "            nn.Softplus(),\n",
        "            nn.Linear(hidden_dim, input_dim),\n",
        "            nn.Sigmoid()  # Assuming input values are normalized between 0 and 1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encode\n",
        "        h = self.encoder(x)\n",
        "        mu, log_var = torch.chunk(h, 2, dim=1)\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "\n",
        "        # Decode\n",
        "        x_recon = self.decoder(z)\n",
        "\n",
        "        return x_recon, mu, log_var\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "class SVM:\n",
        "    def __init__(self, kernel):\n",
        "        self.model = None\n",
        "        self.kernel = kernel\n",
        "\n",
        "    def train(self, X, y):\n",
        "        self.model = SVC(kernel=self.kernel)\n",
        "        self.model.fit(X, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "####################### Helpers ########################\n",
        "def compute_loss(recon_x, x, mu, logvar):\n",
        "    # Reconstruction loss\n",
        "    recon_loss = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "    # KL divergence loss\n",
        "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return recon_loss + kl_loss\n",
        "\n",
        "def extract_m1_features(model, train_loader, device):\n",
        "    X_train_features = []\n",
        "    y_train_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, labels in train_loader:\n",
        "            data = data.view(-1, 784).to(device)\n",
        "            recon_x, mu, logvar = model(data)\n",
        "            X_train_features.append(mu)\n",
        "            y_train_labels.append(labels)\n",
        "\n",
        "    X_train_features = torch.cat(X_train_features)\n",
        "    y_train_labels = torch.cat(y_train_labels)\n",
        "\n",
        "    return X_train_features, y_train_labels\n",
        "\n",
        "def train_svm(svm, X_train_features, y_train_labels):\n",
        "    svm.train(X_train_features, y_train_labels)\n",
        "\n",
        "def evaluate_svm(model, svm, test_loader, device):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, labels in test_loader:\n",
        "            data = data.view(-1, 784).to(device)\n",
        "            recon_x, mu, logvar = model(data)\n",
        "            features = mu.cpu().numpy()\n",
        "            predicted_labels = svm.predict(features)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted_labels == labels.numpy()).sum().item()\n",
        "\n",
        "    return correct, total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "##################### Train Model ######################\n",
        "def train_m1_model(model, optimizer, train_loader, device, num_epochs, log_interval):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch_idx, (data, _) in enumerate(train_loader):\n",
        "            data = data.view(-1, 784).to(device)\n",
        "\n",
        "            recon_x, mu, logvar = model(data)\n",
        "\n",
        "            # Compute loss and optimize\n",
        "            loss = compute_loss(recon_x, data, mu, logvar)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if batch_idx % log_interval == 0:\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(\n",
        "                    epoch+1, num_epochs, batch_idx+1, len(train_loader), loss.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "################### Hyperparameters ####################\n",
        "input_dim = 784\n",
        "hidden_dim = 600\n",
        "latent_dim = 50\n",
        "batch_size = 64\n",
        "learning_rate = 3e-4\n",
        "momentum = 0.1\n",
        "num_epochs = 100\n",
        "log_interval = 100\n",
        "N_labeled = (100, 600, 1000, 3000)\n",
        "kernel = 'rbf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Step [1/936], Loss: 38252.8516\n",
            "Epoch [1/100], Step [101/936], Loss: 21494.0391\n",
            "Epoch [1/100], Step [201/936], Loss: 19981.9492\n",
            "Epoch [1/100], Step [301/936], Loss: 21149.5254\n",
            "Epoch [1/100], Step [401/936], Loss: 20780.9219\n",
            "Epoch [1/100], Step [501/936], Loss: 19600.6875\n",
            "Epoch [1/100], Step [601/936], Loss: 19764.6328\n",
            "Epoch [1/100], Step [701/936], Loss: 18280.0723\n",
            "Epoch [1/100], Step [801/936], Loss: 18461.3223\n",
            "Epoch [1/100], Step [901/936], Loss: 18068.8477\n",
            "Epoch [2/100], Step [1/936], Loss: 18606.6367\n",
            "Epoch [2/100], Step [101/936], Loss: 17816.9629\n",
            "Epoch [2/100], Step [201/936], Loss: 19391.3887\n",
            "Epoch [2/100], Step [301/936], Loss: 18458.3906\n",
            "Epoch [2/100], Step [401/936], Loss: 18550.2168\n",
            "Epoch [2/100], Step [501/936], Loss: 17516.8926\n",
            "Epoch [2/100], Step [601/936], Loss: 18852.3008\n",
            "Epoch [2/100], Step [701/936], Loss: 17672.6562\n",
            "Epoch [2/100], Step [801/936], Loss: 17923.3203\n",
            "Epoch [2/100], Step [901/936], Loss: 18566.1211\n",
            "Epoch [3/100], Step [1/936], Loss: 18058.5449\n",
            "Epoch [3/100], Step [101/936], Loss: 19217.8164\n",
            "Epoch [3/100], Step [201/936], Loss: 18608.8203\n",
            "Epoch [3/100], Step [301/936], Loss: 18348.3711\n",
            "Epoch [3/100], Step [401/936], Loss: 17542.9297\n",
            "Epoch [3/100], Step [501/936], Loss: 17192.4180\n",
            "Epoch [3/100], Step [601/936], Loss: 18328.9453\n",
            "Epoch [3/100], Step [701/936], Loss: 18041.0977\n",
            "Epoch [3/100], Step [801/936], Loss: 18325.8848\n",
            "Epoch [3/100], Step [901/936], Loss: 17946.9512\n",
            "Epoch [4/100], Step [1/936], Loss: 18731.8223\n",
            "Epoch [4/100], Step [101/936], Loss: 18526.1230\n",
            "Epoch [4/100], Step [201/936], Loss: 18391.7871\n",
            "Epoch [4/100], Step [301/936], Loss: 18608.0078\n",
            "Epoch [4/100], Step [401/936], Loss: 17142.0195\n",
            "Epoch [4/100], Step [501/936], Loss: 17986.1953\n",
            "Epoch [4/100], Step [601/936], Loss: 17876.1387\n",
            "Epoch [4/100], Step [701/936], Loss: 17497.7090\n",
            "Epoch [4/100], Step [801/936], Loss: 18325.0742\n",
            "Epoch [4/100], Step [901/936], Loss: 18028.8594\n",
            "Epoch [5/100], Step [1/936], Loss: 18607.6836\n",
            "Epoch [5/100], Step [101/936], Loss: 17383.8730\n",
            "Epoch [5/100], Step [201/936], Loss: 18297.2090\n",
            "Epoch [5/100], Step [301/936], Loss: 18198.3184\n",
            "Epoch [5/100], Step [401/936], Loss: 18660.8516\n",
            "Epoch [5/100], Step [501/936], Loss: 18912.3789\n",
            "Epoch [5/100], Step [601/936], Loss: 18501.6289\n",
            "Epoch [5/100], Step [701/936], Loss: 16948.7070\n",
            "Epoch [5/100], Step [801/936], Loss: 17990.4023\n",
            "Epoch [5/100], Step [901/936], Loss: 18588.8477\n",
            "Epoch [6/100], Step [1/936], Loss: 18415.0508\n",
            "Epoch [6/100], Step [101/936], Loss: 18126.3184\n",
            "Epoch [6/100], Step [201/936], Loss: 17286.3418\n",
            "Epoch [6/100], Step [301/936], Loss: 17979.7891\n",
            "Epoch [6/100], Step [401/936], Loss: 18573.7520\n",
            "Epoch [6/100], Step [501/936], Loss: 16943.6855\n",
            "Epoch [6/100], Step [601/936], Loss: 17484.8223\n",
            "Epoch [6/100], Step [701/936], Loss: 17512.2285\n",
            "Epoch [6/100], Step [801/936], Loss: 17140.8125\n",
            "Epoch [6/100], Step [901/936], Loss: 17863.7129\n",
            "Epoch [7/100], Step [1/936], Loss: 17335.9766\n",
            "Epoch [7/100], Step [101/936], Loss: 17295.0215\n",
            "Epoch [7/100], Step [201/936], Loss: 18077.3145\n",
            "Epoch [7/100], Step [301/936], Loss: 16824.0293\n",
            "Epoch [7/100], Step [401/936], Loss: 18306.8965\n",
            "Epoch [7/100], Step [501/936], Loss: 17067.8418\n",
            "Epoch [7/100], Step [601/936], Loss: 16674.5410\n",
            "Epoch [7/100], Step [701/936], Loss: 17346.4570\n",
            "Epoch [7/100], Step [801/936], Loss: 17658.0918\n",
            "Epoch [7/100], Step [901/936], Loss: 17820.8320\n",
            "Epoch [8/100], Step [1/936], Loss: 17877.6582\n",
            "Epoch [8/100], Step [101/936], Loss: 17228.7324\n",
            "Epoch [8/100], Step [201/936], Loss: 17929.9785\n",
            "Epoch [8/100], Step [301/936], Loss: 17745.4863\n",
            "Epoch [8/100], Step [401/936], Loss: 19010.3145\n",
            "Epoch [8/100], Step [501/936], Loss: 17605.7266\n",
            "Epoch [8/100], Step [601/936], Loss: 18611.5684\n",
            "Epoch [8/100], Step [701/936], Loss: 16435.3770\n",
            "Epoch [8/100], Step [801/936], Loss: 18090.9668\n",
            "Epoch [8/100], Step [901/936], Loss: 18049.1094\n",
            "Epoch [9/100], Step [1/936], Loss: 16433.2207\n",
            "Epoch [9/100], Step [101/936], Loss: 17632.9746\n",
            "Epoch [9/100], Step [201/936], Loss: 17545.8809\n",
            "Epoch [9/100], Step [301/936], Loss: 17996.8535\n",
            "Epoch [9/100], Step [401/936], Loss: 18411.5820\n",
            "Epoch [9/100], Step [501/936], Loss: 17421.3770\n",
            "Epoch [9/100], Step [601/936], Loss: 17529.8184\n",
            "Epoch [9/100], Step [701/936], Loss: 17895.2637\n",
            "Epoch [9/100], Step [801/936], Loss: 18385.9766\n",
            "Epoch [9/100], Step [901/936], Loss: 17493.9961\n",
            "Epoch [10/100], Step [1/936], Loss: 16749.3945\n",
            "Epoch [10/100], Step [101/936], Loss: 18337.6055\n",
            "Epoch [10/100], Step [201/936], Loss: 18320.7812\n",
            "Epoch [10/100], Step [301/936], Loss: 17253.8477\n",
            "Epoch [10/100], Step [401/936], Loss: 16928.2637\n",
            "Epoch [10/100], Step [501/936], Loss: 18411.3008\n",
            "Epoch [10/100], Step [601/936], Loss: 17414.7637\n",
            "Epoch [10/100], Step [701/936], Loss: 18725.7793\n",
            "Epoch [10/100], Step [801/936], Loss: 17321.6387\n",
            "Epoch [10/100], Step [901/936], Loss: 17919.2695\n",
            "Epoch [11/100], Step [1/936], Loss: 16828.0215\n",
            "Epoch [11/100], Step [101/936], Loss: 17593.8047\n",
            "Epoch [11/100], Step [201/936], Loss: 18007.5625\n",
            "Epoch [11/100], Step [301/936], Loss: 16718.2480\n",
            "Epoch [11/100], Step [401/936], Loss: 18240.0977\n",
            "Epoch [11/100], Step [501/936], Loss: 18294.9961\n",
            "Epoch [11/100], Step [601/936], Loss: 17314.0195\n",
            "Epoch [11/100], Step [701/936], Loss: 18015.8945\n",
            "Epoch [11/100], Step [801/936], Loss: 19509.8887\n",
            "Epoch [11/100], Step [901/936], Loss: 17366.4863\n",
            "Epoch [12/100], Step [1/936], Loss: 17614.7598\n",
            "Epoch [12/100], Step [101/936], Loss: 17247.6523\n",
            "Epoch [12/100], Step [201/936], Loss: 17110.8203\n",
            "Epoch [12/100], Step [301/936], Loss: 16694.9824\n",
            "Epoch [12/100], Step [401/936], Loss: 18559.2422\n",
            "Epoch [12/100], Step [501/936], Loss: 17624.0918\n",
            "Epoch [12/100], Step [601/936], Loss: 16761.0371\n",
            "Epoch [12/100], Step [701/936], Loss: 17338.1895\n",
            "Epoch [12/100], Step [801/936], Loss: 17365.1133\n",
            "Epoch [12/100], Step [901/936], Loss: 17451.8945\n",
            "Epoch [13/100], Step [1/936], Loss: 17763.2227\n",
            "Epoch [13/100], Step [101/936], Loss: 17283.1309\n",
            "Epoch [13/100], Step [201/936], Loss: 17639.8262\n",
            "Epoch [13/100], Step [301/936], Loss: 16793.3477\n",
            "Epoch [13/100], Step [401/936], Loss: 17300.1914\n",
            "Epoch [13/100], Step [501/936], Loss: 17574.8242\n",
            "Epoch [13/100], Step [601/936], Loss: 17419.4473\n",
            "Epoch [13/100], Step [701/936], Loss: 17521.6367\n",
            "Epoch [13/100], Step [801/936], Loss: 17675.8184\n",
            "Epoch [13/100], Step [901/936], Loss: 17231.0195\n",
            "Epoch [14/100], Step [1/936], Loss: 17994.3672\n",
            "Epoch [14/100], Step [101/936], Loss: 17884.6914\n",
            "Epoch [14/100], Step [201/936], Loss: 17587.3066\n",
            "Epoch [14/100], Step [301/936], Loss: 17914.4199\n",
            "Epoch [14/100], Step [401/936], Loss: 18007.5508\n",
            "Epoch [14/100], Step [501/936], Loss: 15837.6074\n",
            "Epoch [14/100], Step [601/936], Loss: 17268.2832\n",
            "Epoch [14/100], Step [701/936], Loss: 17985.6914\n",
            "Epoch [14/100], Step [801/936], Loss: 17722.2930\n",
            "Epoch [14/100], Step [901/936], Loss: 17132.1641\n",
            "Epoch [15/100], Step [1/936], Loss: 17859.1309\n",
            "Epoch [15/100], Step [101/936], Loss: 16938.5898\n",
            "Epoch [15/100], Step [201/936], Loss: 18275.4434\n",
            "Epoch [15/100], Step [301/936], Loss: 18577.3594\n",
            "Epoch [15/100], Step [401/936], Loss: 17327.8750\n",
            "Epoch [15/100], Step [501/936], Loss: 17523.5430\n",
            "Epoch [15/100], Step [601/936], Loss: 16745.0059\n",
            "Epoch [15/100], Step [701/936], Loss: 16664.1738\n",
            "Epoch [15/100], Step [801/936], Loss: 17322.5391\n",
            "Epoch [15/100], Step [901/936], Loss: 16342.0244\n",
            "Epoch [16/100], Step [1/936], Loss: 16935.1719\n",
            "Epoch [16/100], Step [101/936], Loss: 17196.3594\n",
            "Epoch [16/100], Step [201/936], Loss: 16809.3828\n",
            "Epoch [16/100], Step [301/936], Loss: 17072.3477\n",
            "Epoch [16/100], Step [401/936], Loss: 17531.4160\n",
            "Epoch [16/100], Step [501/936], Loss: 17593.2031\n",
            "Epoch [16/100], Step [601/936], Loss: 18022.1699\n",
            "Epoch [16/100], Step [701/936], Loss: 17411.3965\n",
            "Epoch [16/100], Step [801/936], Loss: 17954.5957\n",
            "Epoch [16/100], Step [901/936], Loss: 17769.6543\n",
            "Epoch [17/100], Step [1/936], Loss: 17107.1094\n",
            "Epoch [17/100], Step [101/936], Loss: 17136.4609\n",
            "Epoch [17/100], Step [201/936], Loss: 17258.2656\n",
            "Epoch [17/100], Step [301/936], Loss: 17689.9707\n",
            "Epoch [17/100], Step [401/936], Loss: 18165.6445\n",
            "Epoch [17/100], Step [501/936], Loss: 16285.5410\n",
            "Epoch [17/100], Step [601/936], Loss: 17073.2363\n",
            "Epoch [17/100], Step [701/936], Loss: 18081.3262\n",
            "Epoch [17/100], Step [801/936], Loss: 17122.8672\n",
            "Epoch [17/100], Step [901/936], Loss: 18050.0020\n",
            "Epoch [18/100], Step [1/936], Loss: 17055.4570\n",
            "Epoch [18/100], Step [101/936], Loss: 16808.5469\n",
            "Epoch [18/100], Step [201/936], Loss: 17647.8496\n",
            "Epoch [18/100], Step [301/936], Loss: 16272.7705\n",
            "Epoch [18/100], Step [401/936], Loss: 16516.5684\n",
            "Epoch [18/100], Step [501/936], Loss: 17384.3008\n",
            "Epoch [18/100], Step [601/936], Loss: 17187.6211\n",
            "Epoch [18/100], Step [701/936], Loss: 17161.1797\n",
            "Epoch [18/100], Step [801/936], Loss: 17353.5000\n",
            "Epoch [18/100], Step [901/936], Loss: 17489.9004\n",
            "Epoch [19/100], Step [1/936], Loss: 17645.3691\n",
            "Epoch [19/100], Step [101/936], Loss: 16247.2568\n",
            "Epoch [19/100], Step [201/936], Loss: 17696.8809\n",
            "Epoch [19/100], Step [301/936], Loss: 16836.8965\n",
            "Epoch [19/100], Step [401/936], Loss: 17397.7695\n",
            "Epoch [19/100], Step [501/936], Loss: 16954.0078\n",
            "Epoch [19/100], Step [601/936], Loss: 17796.7539\n",
            "Epoch [19/100], Step [701/936], Loss: 17475.1992\n",
            "Epoch [19/100], Step [801/936], Loss: 17028.7070\n",
            "Epoch [19/100], Step [901/936], Loss: 16229.6582\n",
            "Epoch [20/100], Step [1/936], Loss: 17623.6133\n",
            "Epoch [20/100], Step [101/936], Loss: 17450.6523\n",
            "Epoch [20/100], Step [201/936], Loss: 15836.4189\n",
            "Epoch [20/100], Step [301/936], Loss: 17303.1270\n",
            "Epoch [20/100], Step [401/936], Loss: 17715.7012\n",
            "Epoch [20/100], Step [501/936], Loss: 17501.4688\n",
            "Epoch [20/100], Step [601/936], Loss: 17428.5469\n",
            "Epoch [20/100], Step [701/936], Loss: 17977.6367\n",
            "Epoch [20/100], Step [801/936], Loss: 17387.5039\n",
            "Epoch [20/100], Step [901/936], Loss: 16923.0859\n",
            "Epoch [21/100], Step [1/936], Loss: 17807.3789\n",
            "Epoch [21/100], Step [101/936], Loss: 17477.8809\n",
            "Epoch [21/100], Step [201/936], Loss: 16973.4023\n",
            "Epoch [21/100], Step [301/936], Loss: 16993.5918\n",
            "Epoch [21/100], Step [401/936], Loss: 17317.5000\n",
            "Epoch [21/100], Step [501/936], Loss: 16807.4258\n",
            "Epoch [21/100], Step [601/936], Loss: 17070.8262\n",
            "Epoch [21/100], Step [701/936], Loss: 17053.9668\n",
            "Epoch [21/100], Step [801/936], Loss: 15939.8242\n",
            "Epoch [21/100], Step [901/936], Loss: 16047.4199\n",
            "Epoch [22/100], Step [1/936], Loss: 18929.4023\n",
            "Epoch [22/100], Step [101/936], Loss: 17346.5371\n",
            "Epoch [22/100], Step [201/936], Loss: 15938.4062\n",
            "Epoch [22/100], Step [301/936], Loss: 17283.0273\n",
            "Epoch [22/100], Step [401/936], Loss: 17425.3574\n",
            "Epoch [22/100], Step [501/936], Loss: 17895.7168\n",
            "Epoch [22/100], Step [601/936], Loss: 17532.0664\n",
            "Epoch [22/100], Step [701/936], Loss: 17979.1504\n",
            "Epoch [22/100], Step [801/936], Loss: 16807.3164\n",
            "Epoch [22/100], Step [901/936], Loss: 17816.3203\n",
            "Epoch [23/100], Step [1/936], Loss: 16919.0332\n",
            "Epoch [23/100], Step [101/936], Loss: 18451.1973\n",
            "Epoch [23/100], Step [201/936], Loss: 16598.0137\n",
            "Epoch [23/100], Step [301/936], Loss: 17088.9648\n",
            "Epoch [23/100], Step [401/936], Loss: 17623.5352\n",
            "Epoch [23/100], Step [501/936], Loss: 17176.9043\n",
            "Epoch [23/100], Step [601/936], Loss: 18313.2480\n",
            "Epoch [23/100], Step [701/936], Loss: 16898.3418\n",
            "Epoch [23/100], Step [801/936], Loss: 17438.1738\n",
            "Epoch [23/100], Step [901/936], Loss: 16594.5078\n",
            "Epoch [24/100], Step [1/936], Loss: 17430.0410\n",
            "Epoch [24/100], Step [101/936], Loss: 16650.6992\n",
            "Epoch [24/100], Step [201/936], Loss: 17189.9355\n",
            "Epoch [24/100], Step [301/936], Loss: 16746.1055\n",
            "Epoch [24/100], Step [401/936], Loss: 17296.4512\n",
            "Epoch [24/100], Step [501/936], Loss: 16987.1211\n",
            "Epoch [24/100], Step [601/936], Loss: 16547.1738\n",
            "Epoch [24/100], Step [701/936], Loss: 17204.7383\n",
            "Epoch [24/100], Step [801/936], Loss: 16970.3105\n",
            "Epoch [24/100], Step [901/936], Loss: 17978.7324\n",
            "Epoch [25/100], Step [1/936], Loss: 16358.1807\n",
            "Epoch [25/100], Step [101/936], Loss: 16950.0859\n",
            "Epoch [25/100], Step [201/936], Loss: 16735.6250\n",
            "Epoch [25/100], Step [301/936], Loss: 17856.4766\n",
            "Epoch [25/100], Step [401/936], Loss: 16816.2949\n",
            "Epoch [25/100], Step [501/936], Loss: 18019.2168\n",
            "Epoch [25/100], Step [601/936], Loss: 16829.3164\n",
            "Epoch [25/100], Step [701/936], Loss: 17703.5781\n",
            "Epoch [25/100], Step [801/936], Loss: 17657.4395\n",
            "Epoch [25/100], Step [901/936], Loss: 17327.1152\n",
            "Epoch [26/100], Step [1/936], Loss: 17667.7109\n",
            "Epoch [26/100], Step [101/936], Loss: 18182.1934\n",
            "Epoch [26/100], Step [201/936], Loss: 17069.7773\n",
            "Epoch [26/100], Step [301/936], Loss: 18251.1465\n",
            "Epoch [26/100], Step [401/936], Loss: 17560.0254\n",
            "Epoch [26/100], Step [501/936], Loss: 16963.7930\n",
            "Epoch [26/100], Step [601/936], Loss: 16079.9531\n",
            "Epoch [26/100], Step [701/936], Loss: 16599.6035\n",
            "Epoch [26/100], Step [801/936], Loss: 17176.4297\n",
            "Epoch [26/100], Step [901/936], Loss: 16503.2891\n",
            "Epoch [27/100], Step [1/936], Loss: 17289.2285\n",
            "Epoch [27/100], Step [101/936], Loss: 16039.6494\n",
            "Epoch [27/100], Step [201/936], Loss: 16894.7246\n",
            "Epoch [27/100], Step [301/936], Loss: 17566.3242\n",
            "Epoch [27/100], Step [401/936], Loss: 18607.7539\n",
            "Epoch [27/100], Step [501/936], Loss: 17787.0508\n",
            "Epoch [27/100], Step [601/936], Loss: 17923.6250\n",
            "Epoch [27/100], Step [701/936], Loss: 16711.5039\n",
            "Epoch [27/100], Step [801/936], Loss: 17506.3711\n",
            "Epoch [27/100], Step [901/936], Loss: 15649.7158\n",
            "Epoch [28/100], Step [1/936], Loss: 16993.7539\n",
            "Epoch [28/100], Step [101/936], Loss: 17811.3516\n",
            "Epoch [28/100], Step [201/936], Loss: 17876.8320\n",
            "Epoch [28/100], Step [301/936], Loss: 17896.4336\n",
            "Epoch [28/100], Step [401/936], Loss: 18317.6855\n",
            "Epoch [28/100], Step [501/936], Loss: 17332.2422\n",
            "Epoch [28/100], Step [601/936], Loss: 16961.5840\n",
            "Epoch [28/100], Step [701/936], Loss: 17184.7402\n",
            "Epoch [28/100], Step [801/936], Loss: 17274.6152\n",
            "Epoch [28/100], Step [901/936], Loss: 17758.1699\n",
            "Epoch [29/100], Step [1/936], Loss: 16765.4082\n",
            "Epoch [29/100], Step [101/936], Loss: 17869.7227\n",
            "Epoch [29/100], Step [201/936], Loss: 17804.5996\n",
            "Epoch [29/100], Step [301/936], Loss: 16545.1484\n",
            "Epoch [29/100], Step [401/936], Loss: 15273.6689\n",
            "Epoch [29/100], Step [501/936], Loss: 16794.4121\n",
            "Epoch [29/100], Step [601/936], Loss: 17787.6406\n",
            "Epoch [29/100], Step [701/936], Loss: 16901.3672\n",
            "Epoch [29/100], Step [801/936], Loss: 16435.5957\n",
            "Epoch [29/100], Step [901/936], Loss: 17037.2051\n",
            "Epoch [30/100], Step [1/936], Loss: 17853.8320\n",
            "Epoch [30/100], Step [101/936], Loss: 17608.4258\n",
            "Epoch [30/100], Step [201/936], Loss: 17811.5840\n",
            "Epoch [30/100], Step [301/936], Loss: 16461.5117\n",
            "Epoch [30/100], Step [401/936], Loss: 16169.0693\n",
            "Epoch [30/100], Step [501/936], Loss: 16463.2754\n",
            "Epoch [30/100], Step [601/936], Loss: 16942.0332\n",
            "Epoch [30/100], Step [701/936], Loss: 16812.3711\n",
            "Epoch [30/100], Step [801/936], Loss: 17595.2109\n",
            "Epoch [30/100], Step [901/936], Loss: 17485.5000\n",
            "Epoch [31/100], Step [1/936], Loss: 18186.1797\n",
            "Epoch [31/100], Step [101/936], Loss: 18510.1289\n",
            "Epoch [31/100], Step [201/936], Loss: 16923.2148\n",
            "Epoch [31/100], Step [301/936], Loss: 16734.6387\n",
            "Epoch [31/100], Step [401/936], Loss: 18003.6641\n",
            "Epoch [31/100], Step [501/936], Loss: 17862.4141\n",
            "Epoch [31/100], Step [601/936], Loss: 16609.1504\n",
            "Epoch [31/100], Step [701/936], Loss: 16428.1445\n",
            "Epoch [31/100], Step [801/936], Loss: 16530.8145\n",
            "Epoch [31/100], Step [901/936], Loss: 16276.9092\n",
            "Epoch [32/100], Step [1/936], Loss: 18021.1875\n",
            "Epoch [32/100], Step [101/936], Loss: 16205.9111\n",
            "Epoch [32/100], Step [201/936], Loss: 16115.4365\n",
            "Epoch [32/100], Step [301/936], Loss: 17852.4336\n",
            "Epoch [32/100], Step [401/936], Loss: 17781.4375\n",
            "Epoch [32/100], Step [501/936], Loss: 17391.2910\n",
            "Epoch [32/100], Step [601/936], Loss: 16026.9434\n",
            "Epoch [32/100], Step [701/936], Loss: 15939.5801\n",
            "Epoch [32/100], Step [801/936], Loss: 16616.7188\n",
            "Epoch [32/100], Step [901/936], Loss: 17552.1777\n",
            "Epoch [33/100], Step [1/936], Loss: 16728.1836\n",
            "Epoch [33/100], Step [101/936], Loss: 17902.9180\n",
            "Epoch [33/100], Step [201/936], Loss: 16580.2363\n",
            "Epoch [33/100], Step [301/936], Loss: 17163.0000\n",
            "Epoch [33/100], Step [401/936], Loss: 17821.5918\n",
            "Epoch [33/100], Step [501/936], Loss: 17784.6699\n",
            "Epoch [33/100], Step [601/936], Loss: 17268.2188\n",
            "Epoch [33/100], Step [701/936], Loss: 17390.3672\n",
            "Epoch [33/100], Step [801/936], Loss: 16766.0234\n",
            "Epoch [33/100], Step [901/936], Loss: 17956.4199\n",
            "Epoch [34/100], Step [1/936], Loss: 17531.4238\n",
            "Epoch [34/100], Step [101/936], Loss: 17190.6797\n",
            "Epoch [34/100], Step [201/936], Loss: 16747.0977\n",
            "Epoch [34/100], Step [301/936], Loss: 17263.1699\n",
            "Epoch [34/100], Step [401/936], Loss: 16894.0879\n",
            "Epoch [34/100], Step [501/936], Loss: 18001.3906\n",
            "Epoch [34/100], Step [601/936], Loss: 17494.2422\n",
            "Epoch [34/100], Step [701/936], Loss: 16718.4727\n",
            "Epoch [34/100], Step [801/936], Loss: 17485.8809\n",
            "Epoch [34/100], Step [901/936], Loss: 16744.7402\n",
            "Epoch [35/100], Step [1/936], Loss: 16535.5938\n",
            "Epoch [35/100], Step [101/936], Loss: 17344.2383\n",
            "Epoch [35/100], Step [201/936], Loss: 16649.9121\n",
            "Epoch [35/100], Step [301/936], Loss: 17297.3223\n",
            "Epoch [35/100], Step [401/936], Loss: 18173.6191\n",
            "Epoch [35/100], Step [501/936], Loss: 16900.8730\n",
            "Epoch [35/100], Step [601/936], Loss: 17361.2305\n",
            "Epoch [35/100], Step [701/936], Loss: 17059.3457\n",
            "Epoch [35/100], Step [801/936], Loss: 16878.1094\n",
            "Epoch [35/100], Step [901/936], Loss: 16607.3633\n",
            "Epoch [36/100], Step [1/936], Loss: 17196.5312\n",
            "Epoch [36/100], Step [101/936], Loss: 17001.3848\n",
            "Epoch [36/100], Step [201/936], Loss: 15845.0811\n",
            "Epoch [36/100], Step [301/936], Loss: 17249.9180\n",
            "Epoch [36/100], Step [401/936], Loss: 17160.5117\n",
            "Epoch [36/100], Step [501/936], Loss: 16858.9043\n",
            "Epoch [36/100], Step [601/936], Loss: 17385.5996\n",
            "Epoch [36/100], Step [701/936], Loss: 16470.3066\n",
            "Epoch [36/100], Step [801/936], Loss: 17701.1680\n",
            "Epoch [36/100], Step [901/936], Loss: 17379.5625\n",
            "Epoch [37/100], Step [1/936], Loss: 16565.5605\n",
            "Epoch [37/100], Step [101/936], Loss: 17982.2930\n",
            "Epoch [37/100], Step [201/936], Loss: 17998.4258\n",
            "Epoch [37/100], Step [301/936], Loss: 17461.8262\n",
            "Epoch [37/100], Step [401/936], Loss: 16952.0117\n",
            "Epoch [37/100], Step [501/936], Loss: 17030.1074\n",
            "Epoch [37/100], Step [601/936], Loss: 17984.8516\n",
            "Epoch [37/100], Step [701/936], Loss: 16595.7012\n",
            "Epoch [37/100], Step [801/936], Loss: 17047.0352\n",
            "Epoch [37/100], Step [901/936], Loss: 16457.4258\n",
            "Epoch [38/100], Step [1/936], Loss: 16845.7031\n",
            "Epoch [38/100], Step [101/936], Loss: 17465.8828\n",
            "Epoch [38/100], Step [201/936], Loss: 16874.2090\n",
            "Epoch [38/100], Step [301/936], Loss: 16954.7910\n",
            "Epoch [38/100], Step [401/936], Loss: 16480.3691\n",
            "Epoch [38/100], Step [501/936], Loss: 16714.7305\n",
            "Epoch [38/100], Step [601/936], Loss: 16554.4336\n",
            "Epoch [38/100], Step [701/936], Loss: 16220.0068\n",
            "Epoch [38/100], Step [801/936], Loss: 16735.8340\n",
            "Epoch [38/100], Step [901/936], Loss: 17247.1680\n",
            "Epoch [39/100], Step [1/936], Loss: 16474.2109\n",
            "Epoch [39/100], Step [101/936], Loss: 17516.1250\n",
            "Epoch [39/100], Step [201/936], Loss: 17670.2031\n",
            "Epoch [39/100], Step [301/936], Loss: 17582.7695\n",
            "Epoch [39/100], Step [401/936], Loss: 16757.9648\n",
            "Epoch [39/100], Step [501/936], Loss: 16790.7734\n",
            "Epoch [39/100], Step [601/936], Loss: 17435.7754\n",
            "Epoch [39/100], Step [701/936], Loss: 16183.8955\n",
            "Epoch [39/100], Step [801/936], Loss: 17829.1387\n",
            "Epoch [39/100], Step [901/936], Loss: 17236.4355\n",
            "Epoch [40/100], Step [1/936], Loss: 16783.9668\n",
            "Epoch [40/100], Step [101/936], Loss: 17538.5312\n",
            "Epoch [40/100], Step [201/936], Loss: 16961.4336\n",
            "Epoch [40/100], Step [301/936], Loss: 17447.4512\n",
            "Epoch [40/100], Step [401/936], Loss: 16776.9805\n",
            "Epoch [40/100], Step [501/936], Loss: 17416.7441\n",
            "Epoch [40/100], Step [601/936], Loss: 16512.6934\n",
            "Epoch [40/100], Step [701/936], Loss: 16291.5830\n",
            "Epoch [40/100], Step [801/936], Loss: 17114.9121\n",
            "Epoch [40/100], Step [901/936], Loss: 17349.0273\n",
            "Epoch [41/100], Step [1/936], Loss: 16403.9824\n",
            "Epoch [41/100], Step [101/936], Loss: 16490.1699\n",
            "Epoch [41/100], Step [201/936], Loss: 16829.6855\n",
            "Epoch [41/100], Step [301/936], Loss: 16963.0996\n",
            "Epoch [41/100], Step [401/936], Loss: 17701.5312\n",
            "Epoch [41/100], Step [501/936], Loss: 17245.1289\n",
            "Epoch [41/100], Step [601/936], Loss: 17125.4141\n",
            "Epoch [41/100], Step [701/936], Loss: 17177.0215\n",
            "Epoch [41/100], Step [801/936], Loss: 17227.0156\n",
            "Epoch [41/100], Step [901/936], Loss: 15994.3115\n",
            "Epoch [42/100], Step [1/936], Loss: 16582.9805\n",
            "Epoch [42/100], Step [101/936], Loss: 16016.6992\n",
            "Epoch [42/100], Step [201/936], Loss: 17237.4512\n",
            "Epoch [42/100], Step [301/936], Loss: 17159.3809\n",
            "Epoch [42/100], Step [401/936], Loss: 16291.0947\n",
            "Epoch [42/100], Step [501/936], Loss: 18248.0371\n",
            "Epoch [42/100], Step [601/936], Loss: 17394.6328\n",
            "Epoch [42/100], Step [701/936], Loss: 17216.4180\n",
            "Epoch [42/100], Step [801/936], Loss: 16187.7109\n",
            "Epoch [42/100], Step [901/936], Loss: 16580.9824\n",
            "Epoch [43/100], Step [1/936], Loss: 17208.1914\n",
            "Epoch [43/100], Step [101/936], Loss: 17913.5898\n",
            "Epoch [43/100], Step [201/936], Loss: 16665.2500\n",
            "Epoch [43/100], Step [301/936], Loss: 16702.4707\n",
            "Epoch [43/100], Step [401/936], Loss: 17892.9668\n",
            "Epoch [43/100], Step [501/936], Loss: 17652.4805\n",
            "Epoch [43/100], Step [601/936], Loss: 17177.1074\n",
            "Epoch [43/100], Step [701/936], Loss: 16365.9941\n",
            "Epoch [43/100], Step [801/936], Loss: 17077.4043\n",
            "Epoch [43/100], Step [901/936], Loss: 17414.4570\n",
            "Epoch [44/100], Step [1/936], Loss: 16650.5449\n",
            "Epoch [44/100], Step [101/936], Loss: 17551.8750\n",
            "Epoch [44/100], Step [201/936], Loss: 16301.1807\n",
            "Epoch [44/100], Step [301/936], Loss: 17264.9180\n",
            "Epoch [44/100], Step [401/936], Loss: 16736.7812\n",
            "Epoch [44/100], Step [501/936], Loss: 18313.0215\n",
            "Epoch [44/100], Step [601/936], Loss: 16136.5840\n",
            "Epoch [44/100], Step [701/936], Loss: 16456.7578\n",
            "Epoch [44/100], Step [801/936], Loss: 16614.2539\n",
            "Epoch [44/100], Step [901/936], Loss: 17049.3281\n",
            "Epoch [45/100], Step [1/936], Loss: 17897.6367\n",
            "Epoch [45/100], Step [101/936], Loss: 17634.1934\n",
            "Epoch [45/100], Step [201/936], Loss: 17295.5723\n",
            "Epoch [45/100], Step [301/936], Loss: 16943.9004\n",
            "Epoch [45/100], Step [401/936], Loss: 17865.0059\n",
            "Epoch [45/100], Step [501/936], Loss: 17197.9883\n",
            "Epoch [45/100], Step [601/936], Loss: 17646.5762\n",
            "Epoch [45/100], Step [701/936], Loss: 16105.7285\n",
            "Epoch [45/100], Step [801/936], Loss: 17331.2559\n",
            "Epoch [45/100], Step [901/936], Loss: 16937.6309\n",
            "Epoch [46/100], Step [1/936], Loss: 17020.3242\n",
            "Epoch [46/100], Step [101/936], Loss: 16792.6094\n",
            "Epoch [46/100], Step [201/936], Loss: 17571.3066\n",
            "Epoch [46/100], Step [301/936], Loss: 16656.7676\n",
            "Epoch [46/100], Step [401/936], Loss: 17177.6328\n",
            "Epoch [46/100], Step [501/936], Loss: 17095.4160\n",
            "Epoch [46/100], Step [601/936], Loss: 17027.0430\n",
            "Epoch [46/100], Step [701/936], Loss: 16880.7559\n",
            "Epoch [46/100], Step [801/936], Loss: 17240.4727\n",
            "Epoch [46/100], Step [901/936], Loss: 16554.1289\n",
            "Epoch [47/100], Step [1/936], Loss: 17226.7891\n",
            "Epoch [47/100], Step [101/936], Loss: 17426.4004\n",
            "Epoch [47/100], Step [201/936], Loss: 16784.7461\n",
            "Epoch [47/100], Step [301/936], Loss: 15792.2793\n",
            "Epoch [47/100], Step [401/936], Loss: 16802.3926\n",
            "Epoch [47/100], Step [501/936], Loss: 17109.5586\n",
            "Epoch [47/100], Step [601/936], Loss: 16416.1875\n",
            "Epoch [47/100], Step [701/936], Loss: 16465.6484\n",
            "Epoch [47/100], Step [801/936], Loss: 16813.0234\n",
            "Epoch [47/100], Step [901/936], Loss: 16921.8457\n",
            "Epoch [48/100], Step [1/936], Loss: 17947.7793\n",
            "Epoch [48/100], Step [101/936], Loss: 16811.8691\n",
            "Epoch [48/100], Step [201/936], Loss: 15936.8584\n",
            "Epoch [48/100], Step [301/936], Loss: 17035.2188\n",
            "Epoch [48/100], Step [401/936], Loss: 16934.1191\n",
            "Epoch [48/100], Step [501/936], Loss: 16723.4414\n",
            "Epoch [48/100], Step [601/936], Loss: 15960.5088\n",
            "Epoch [48/100], Step [701/936], Loss: 17486.6094\n",
            "Epoch [48/100], Step [801/936], Loss: 17041.1758\n",
            "Epoch [48/100], Step [901/936], Loss: 17665.0039\n",
            "Epoch [49/100], Step [1/936], Loss: 17004.4648\n",
            "Epoch [49/100], Step [101/936], Loss: 17261.6445\n",
            "Epoch [49/100], Step [201/936], Loss: 17929.6309\n",
            "Epoch [49/100], Step [301/936], Loss: 16664.1934\n",
            "Epoch [49/100], Step [401/936], Loss: 17174.5527\n",
            "Epoch [49/100], Step [501/936], Loss: 18540.8145\n",
            "Epoch [49/100], Step [601/936], Loss: 15967.1035\n",
            "Epoch [49/100], Step [701/936], Loss: 16653.7969\n",
            "Epoch [49/100], Step [801/936], Loss: 18196.7559\n",
            "Epoch [49/100], Step [901/936], Loss: 16960.9219\n",
            "Epoch [50/100], Step [1/936], Loss: 17397.0293\n",
            "Epoch [50/100], Step [101/936], Loss: 17748.1777\n",
            "Epoch [50/100], Step [201/936], Loss: 16397.7305\n",
            "Epoch [50/100], Step [301/936], Loss: 17424.4492\n",
            "Epoch [50/100], Step [401/936], Loss: 17889.2129\n",
            "Epoch [50/100], Step [501/936], Loss: 17773.8145\n",
            "Epoch [50/100], Step [601/936], Loss: 17278.3418\n",
            "Epoch [50/100], Step [701/936], Loss: 17601.7598\n",
            "Epoch [50/100], Step [801/936], Loss: 17037.3281\n",
            "Epoch [50/100], Step [901/936], Loss: 18693.4180\n",
            "Epoch [51/100], Step [1/936], Loss: 16272.2021\n",
            "Epoch [51/100], Step [101/936], Loss: 17078.9941\n",
            "Epoch [51/100], Step [201/936], Loss: 16065.1943\n",
            "Epoch [51/100], Step [301/936], Loss: 16409.9453\n",
            "Epoch [51/100], Step [401/936], Loss: 17955.2832\n",
            "Epoch [51/100], Step [501/936], Loss: 16319.1660\n",
            "Epoch [51/100], Step [601/936], Loss: 16407.0996\n",
            "Epoch [51/100], Step [701/936], Loss: 18097.9160\n",
            "Epoch [51/100], Step [801/936], Loss: 16748.0137\n",
            "Epoch [51/100], Step [901/936], Loss: 17718.1699\n",
            "Epoch [52/100], Step [1/936], Loss: 16577.0703\n",
            "Epoch [52/100], Step [101/936], Loss: 15649.1426\n",
            "Epoch [52/100], Step [201/936], Loss: 17325.4746\n",
            "Epoch [52/100], Step [301/936], Loss: 17222.9062\n",
            "Epoch [52/100], Step [401/936], Loss: 17423.0000\n",
            "Epoch [52/100], Step [501/936], Loss: 17549.1387\n",
            "Epoch [52/100], Step [601/936], Loss: 16534.6484\n",
            "Epoch [52/100], Step [701/936], Loss: 16711.5605\n",
            "Epoch [52/100], Step [801/936], Loss: 17840.1875\n",
            "Epoch [52/100], Step [901/936], Loss: 18101.9082\n",
            "Epoch [53/100], Step [1/936], Loss: 17394.4844\n",
            "Epoch [53/100], Step [101/936], Loss: 17724.0703\n",
            "Epoch [53/100], Step [201/936], Loss: 16649.9590\n",
            "Epoch [53/100], Step [301/936], Loss: 16922.6348\n",
            "Epoch [53/100], Step [401/936], Loss: 16452.1973\n",
            "Epoch [53/100], Step [501/936], Loss: 17144.2500\n",
            "Epoch [53/100], Step [601/936], Loss: 16490.5859\n",
            "Epoch [53/100], Step [701/936], Loss: 16964.0664\n",
            "Epoch [53/100], Step [801/936], Loss: 16256.0605\n",
            "Epoch [53/100], Step [901/936], Loss: 16614.5039\n",
            "Epoch [54/100], Step [1/936], Loss: 17560.9180\n",
            "Epoch [54/100], Step [101/936], Loss: 16016.6562\n",
            "Epoch [54/100], Step [201/936], Loss: 16855.5879\n",
            "Epoch [54/100], Step [301/936], Loss: 16133.0146\n",
            "Epoch [54/100], Step [401/936], Loss: 16954.6094\n",
            "Epoch [54/100], Step [501/936], Loss: 17779.9863\n",
            "Epoch [54/100], Step [601/936], Loss: 16320.8926\n",
            "Epoch [54/100], Step [701/936], Loss: 16552.2207\n",
            "Epoch [54/100], Step [801/936], Loss: 16740.5234\n",
            "Epoch [54/100], Step [901/936], Loss: 17351.0195\n",
            "Epoch [55/100], Step [1/936], Loss: 16416.4297\n",
            "Epoch [55/100], Step [101/936], Loss: 15708.3154\n",
            "Epoch [55/100], Step [201/936], Loss: 17214.8809\n",
            "Epoch [55/100], Step [301/936], Loss: 15589.1367\n",
            "Epoch [55/100], Step [401/936], Loss: 15745.5127\n",
            "Epoch [55/100], Step [501/936], Loss: 17478.9414\n",
            "Epoch [55/100], Step [601/936], Loss: 17500.8633\n",
            "Epoch [55/100], Step [701/936], Loss: 16573.6328\n",
            "Epoch [55/100], Step [801/936], Loss: 15833.4502\n",
            "Epoch [55/100], Step [901/936], Loss: 17169.0273\n",
            "Epoch [56/100], Step [1/936], Loss: 17304.9355\n",
            "Epoch [56/100], Step [101/936], Loss: 16813.8066\n",
            "Epoch [56/100], Step [201/936], Loss: 16524.6074\n",
            "Epoch [56/100], Step [301/936], Loss: 17070.6914\n",
            "Epoch [56/100], Step [401/936], Loss: 16282.0781\n",
            "Epoch [56/100], Step [501/936], Loss: 16571.9785\n",
            "Epoch [56/100], Step [601/936], Loss: 17164.6875\n",
            "Epoch [56/100], Step [701/936], Loss: 17235.0020\n",
            "Epoch [56/100], Step [801/936], Loss: 17892.3242\n",
            "Epoch [56/100], Step [901/936], Loss: 17009.8828\n",
            "Epoch [57/100], Step [1/936], Loss: 17685.6094\n",
            "Epoch [57/100], Step [101/936], Loss: 17862.9805\n",
            "Epoch [57/100], Step [201/936], Loss: 16614.6074\n",
            "Epoch [57/100], Step [301/936], Loss: 17557.5254\n",
            "Epoch [57/100], Step [401/936], Loss: 17502.4297\n",
            "Epoch [57/100], Step [501/936], Loss: 17005.1465\n",
            "Epoch [57/100], Step [601/936], Loss: 17035.9141\n",
            "Epoch [57/100], Step [701/936], Loss: 16740.9805\n",
            "Epoch [57/100], Step [801/936], Loss: 17198.5645\n",
            "Epoch [57/100], Step [901/936], Loss: 17172.9688\n",
            "Epoch [58/100], Step [1/936], Loss: 16734.4375\n",
            "Epoch [58/100], Step [101/936], Loss: 16980.0215\n",
            "Epoch [58/100], Step [201/936], Loss: 16806.9121\n",
            "Epoch [58/100], Step [301/936], Loss: 16180.4600\n",
            "Epoch [58/100], Step [401/936], Loss: 17236.5957\n",
            "Epoch [58/100], Step [501/936], Loss: 17849.2520\n",
            "Epoch [58/100], Step [601/936], Loss: 16321.7109\n",
            "Epoch [58/100], Step [701/936], Loss: 17097.4902\n",
            "Epoch [58/100], Step [801/936], Loss: 15877.0723\n",
            "Epoch [58/100], Step [901/936], Loss: 16443.2793\n",
            "Epoch [59/100], Step [1/936], Loss: 16556.8789\n",
            "Epoch [59/100], Step [101/936], Loss: 17571.3145\n",
            "Epoch [59/100], Step [201/936], Loss: 15985.8369\n",
            "Epoch [59/100], Step [301/936], Loss: 17914.0234\n",
            "Epoch [59/100], Step [401/936], Loss: 16446.4883\n",
            "Epoch [59/100], Step [501/936], Loss: 18335.0723\n",
            "Epoch [59/100], Step [601/936], Loss: 17056.3574\n",
            "Epoch [59/100], Step [701/936], Loss: 16035.2705\n",
            "Epoch [59/100], Step [801/936], Loss: 16710.4316\n",
            "Epoch [59/100], Step [901/936], Loss: 17147.3301\n",
            "Epoch [60/100], Step [1/936], Loss: 18103.8809\n",
            "Epoch [60/100], Step [101/936], Loss: 16422.6250\n",
            "Epoch [60/100], Step [201/936], Loss: 17200.5312\n",
            "Epoch [60/100], Step [301/936], Loss: 16062.8633\n",
            "Epoch [60/100], Step [401/936], Loss: 17382.4258\n",
            "Epoch [60/100], Step [501/936], Loss: 17364.5273\n",
            "Epoch [60/100], Step [601/936], Loss: 16887.8008\n",
            "Epoch [60/100], Step [701/936], Loss: 18367.6055\n",
            "Epoch [60/100], Step [801/936], Loss: 17159.0801\n",
            "Epoch [60/100], Step [901/936], Loss: 16956.6855\n",
            "Epoch [61/100], Step [1/936], Loss: 16570.5410\n",
            "Epoch [61/100], Step [101/936], Loss: 16873.5645\n",
            "Epoch [61/100], Step [201/936], Loss: 17553.3379\n",
            "Epoch [61/100], Step [301/936], Loss: 17256.2285\n",
            "Epoch [61/100], Step [401/936], Loss: 16963.6035\n",
            "Epoch [61/100], Step [501/936], Loss: 18055.9883\n",
            "Epoch [61/100], Step [601/936], Loss: 16412.5938\n",
            "Epoch [61/100], Step [701/936], Loss: 16473.5000\n",
            "Epoch [61/100], Step [801/936], Loss: 16724.5820\n",
            "Epoch [61/100], Step [901/936], Loss: 17514.8789\n",
            "Epoch [62/100], Step [1/936], Loss: 16549.0645\n",
            "Epoch [62/100], Step [101/936], Loss: 17915.3203\n",
            "Epoch [62/100], Step [201/936], Loss: 17223.4961\n",
            "Epoch [62/100], Step [301/936], Loss: 16139.2090\n",
            "Epoch [62/100], Step [401/936], Loss: 17418.6035\n",
            "Epoch [62/100], Step [501/936], Loss: 16620.5938\n",
            "Epoch [62/100], Step [601/936], Loss: 17363.1953\n",
            "Epoch [62/100], Step [701/936], Loss: 17429.5879\n",
            "Epoch [62/100], Step [801/936], Loss: 16433.3945\n",
            "Epoch [62/100], Step [901/936], Loss: 15704.5195\n",
            "Epoch [63/100], Step [1/936], Loss: 16811.2129\n",
            "Epoch [63/100], Step [101/936], Loss: 16789.3691\n",
            "Epoch [63/100], Step [201/936], Loss: 16321.3340\n",
            "Epoch [63/100], Step [301/936], Loss: 17542.8223\n",
            "Epoch [63/100], Step [401/936], Loss: 17099.4648\n",
            "Epoch [63/100], Step [501/936], Loss: 17447.6328\n",
            "Epoch [63/100], Step [601/936], Loss: 17431.1133\n",
            "Epoch [63/100], Step [701/936], Loss: 16995.4980\n",
            "Epoch [63/100], Step [801/936], Loss: 18006.1367\n",
            "Epoch [63/100], Step [901/936], Loss: 15692.6465\n",
            "Epoch [64/100], Step [1/936], Loss: 17467.6426\n",
            "Epoch [64/100], Step [101/936], Loss: 16072.0869\n",
            "Epoch [64/100], Step [201/936], Loss: 15996.7305\n",
            "Epoch [64/100], Step [301/936], Loss: 17045.4141\n",
            "Epoch [64/100], Step [401/936], Loss: 16219.8701\n",
            "Epoch [64/100], Step [501/936], Loss: 16399.0781\n",
            "Epoch [64/100], Step [601/936], Loss: 18167.3887\n",
            "Epoch [64/100], Step [701/936], Loss: 16776.8438\n",
            "Epoch [64/100], Step [801/936], Loss: 16973.6270\n",
            "Epoch [64/100], Step [901/936], Loss: 16335.2998\n",
            "Epoch [65/100], Step [1/936], Loss: 16116.0098\n",
            "Epoch [65/100], Step [101/936], Loss: 17434.2715\n",
            "Epoch [65/100], Step [201/936], Loss: 16438.0547\n",
            "Epoch [65/100], Step [301/936], Loss: 17255.0117\n",
            "Epoch [65/100], Step [401/936], Loss: 17071.1426\n",
            "Epoch [65/100], Step [501/936], Loss: 15888.0928\n",
            "Epoch [65/100], Step [601/936], Loss: 17922.6191\n",
            "Epoch [65/100], Step [701/936], Loss: 16157.4307\n",
            "Epoch [65/100], Step [801/936], Loss: 16932.1348\n",
            "Epoch [65/100], Step [901/936], Loss: 17741.2109\n",
            "Epoch [66/100], Step [1/936], Loss: 16037.4473\n",
            "Epoch [66/100], Step [101/936], Loss: 16948.1895\n",
            "Epoch [66/100], Step [201/936], Loss: 16100.5342\n",
            "Epoch [66/100], Step [301/936], Loss: 17880.6055\n",
            "Epoch [66/100], Step [401/936], Loss: 17186.0156\n",
            "Epoch [66/100], Step [501/936], Loss: 17257.9746\n",
            "Epoch [66/100], Step [601/936], Loss: 16366.0635\n",
            "Epoch [66/100], Step [701/936], Loss: 17392.2305\n",
            "Epoch [66/100], Step [801/936], Loss: 16918.1445\n",
            "Epoch [66/100], Step [901/936], Loss: 16952.2070\n",
            "Epoch [67/100], Step [1/936], Loss: 16512.3242\n",
            "Epoch [67/100], Step [101/936], Loss: 17143.5469\n",
            "Epoch [67/100], Step [201/936], Loss: 16748.0742\n",
            "Epoch [67/100], Step [301/936], Loss: 16523.4316\n",
            "Epoch [67/100], Step [401/936], Loss: 17087.8887\n",
            "Epoch [67/100], Step [501/936], Loss: 17284.6816\n",
            "Epoch [67/100], Step [601/936], Loss: 16857.2051\n",
            "Epoch [67/100], Step [701/936], Loss: 17192.0684\n",
            "Epoch [67/100], Step [801/936], Loss: 17037.2559\n",
            "Epoch [67/100], Step [901/936], Loss: 16712.0332\n",
            "Epoch [68/100], Step [1/936], Loss: 17405.1172\n",
            "Epoch [68/100], Step [101/936], Loss: 17095.7656\n",
            "Epoch [68/100], Step [201/936], Loss: 16100.1621\n",
            "Epoch [68/100], Step [301/936], Loss: 16573.0293\n",
            "Epoch [68/100], Step [401/936], Loss: 15852.4365\n",
            "Epoch [68/100], Step [501/936], Loss: 17457.3359\n",
            "Epoch [68/100], Step [601/936], Loss: 17777.1504\n",
            "Epoch [68/100], Step [701/936], Loss: 17011.5586\n",
            "Epoch [68/100], Step [801/936], Loss: 16714.1133\n",
            "Epoch [68/100], Step [901/936], Loss: 16960.4648\n",
            "Epoch [69/100], Step [1/936], Loss: 17160.4883\n",
            "Epoch [69/100], Step [101/936], Loss: 17872.6738\n",
            "Epoch [69/100], Step [201/936], Loss: 18660.7324\n",
            "Epoch [69/100], Step [301/936], Loss: 17158.2617\n",
            "Epoch [69/100], Step [401/936], Loss: 17500.3691\n",
            "Epoch [69/100], Step [501/936], Loss: 17722.8555\n",
            "Epoch [69/100], Step [601/936], Loss: 16373.3398\n",
            "Epoch [69/100], Step [701/936], Loss: 16410.0254\n",
            "Epoch [69/100], Step [801/936], Loss: 16676.6309\n",
            "Epoch [69/100], Step [901/936], Loss: 16230.8076\n",
            "Epoch [70/100], Step [1/936], Loss: 17002.2188\n",
            "Epoch [70/100], Step [101/936], Loss: 16767.9844\n",
            "Epoch [70/100], Step [201/936], Loss: 17649.8066\n",
            "Epoch [70/100], Step [301/936], Loss: 17598.4707\n",
            "Epoch [70/100], Step [401/936], Loss: 16291.0508\n",
            "Epoch [70/100], Step [501/936], Loss: 16968.0215\n",
            "Epoch [70/100], Step [601/936], Loss: 15547.9453\n",
            "Epoch [70/100], Step [701/936], Loss: 16264.6699\n",
            "Epoch [70/100], Step [801/936], Loss: 16048.5537\n",
            "Epoch [70/100], Step [901/936], Loss: 17508.0059\n",
            "Epoch [71/100], Step [1/936], Loss: 16297.1592\n",
            "Epoch [71/100], Step [101/936], Loss: 17855.0996\n",
            "Epoch [71/100], Step [201/936], Loss: 17398.3027\n",
            "Epoch [71/100], Step [301/936], Loss: 16905.9375\n",
            "Epoch [71/100], Step [401/936], Loss: 17186.4141\n",
            "Epoch [71/100], Step [501/936], Loss: 16885.0059\n",
            "Epoch [71/100], Step [601/936], Loss: 16929.8340\n",
            "Epoch [71/100], Step [701/936], Loss: 16643.5566\n",
            "Epoch [71/100], Step [801/936], Loss: 16417.6816\n",
            "Epoch [71/100], Step [901/936], Loss: 17458.9453\n",
            "Epoch [72/100], Step [1/936], Loss: 16327.0303\n",
            "Epoch [72/100], Step [101/936], Loss: 16415.7969\n",
            "Epoch [72/100], Step [201/936], Loss: 17693.8301\n",
            "Epoch [72/100], Step [301/936], Loss: 17067.0605\n",
            "Epoch [72/100], Step [401/936], Loss: 16589.5332\n",
            "Epoch [72/100], Step [501/936], Loss: 17733.8066\n",
            "Epoch [72/100], Step [601/936], Loss: 16986.7520\n",
            "Epoch [72/100], Step [701/936], Loss: 16910.8477\n",
            "Epoch [72/100], Step [801/936], Loss: 17853.6387\n",
            "Epoch [72/100], Step [901/936], Loss: 17446.1855\n",
            "Epoch [73/100], Step [1/936], Loss: 17979.4336\n",
            "Epoch [73/100], Step [101/936], Loss: 17196.2188\n",
            "Epoch [73/100], Step [201/936], Loss: 16920.8867\n",
            "Epoch [73/100], Step [301/936], Loss: 17032.4844\n",
            "Epoch [73/100], Step [401/936], Loss: 16456.4648\n",
            "Epoch [73/100], Step [501/936], Loss: 16936.7773\n",
            "Epoch [73/100], Step [601/936], Loss: 16663.5430\n",
            "Epoch [73/100], Step [701/936], Loss: 16745.2168\n",
            "Epoch [73/100], Step [801/936], Loss: 16100.0654\n",
            "Epoch [73/100], Step [901/936], Loss: 16269.9180\n",
            "Epoch [74/100], Step [1/936], Loss: 17415.4746\n",
            "Epoch [74/100], Step [101/936], Loss: 16939.3066\n",
            "Epoch [74/100], Step [201/936], Loss: 16798.7285\n",
            "Epoch [74/100], Step [301/936], Loss: 17048.7891\n",
            "Epoch [74/100], Step [401/936], Loss: 16277.3789\n",
            "Epoch [74/100], Step [501/936], Loss: 17331.5059\n",
            "Epoch [74/100], Step [601/936], Loss: 16635.9922\n",
            "Epoch [74/100], Step [701/936], Loss: 17304.3535\n",
            "Epoch [74/100], Step [801/936], Loss: 15812.5000\n",
            "Epoch [74/100], Step [901/936], Loss: 16858.7930\n",
            "Epoch [75/100], Step [1/936], Loss: 16556.5332\n",
            "Epoch [75/100], Step [101/936], Loss: 16438.9043\n",
            "Epoch [75/100], Step [201/936], Loss: 17647.1133\n",
            "Epoch [75/100], Step [301/936], Loss: 16408.0488\n",
            "Epoch [75/100], Step [401/936], Loss: 17596.4141\n",
            "Epoch [75/100], Step [501/936], Loss: 15140.6475\n",
            "Epoch [75/100], Step [601/936], Loss: 16937.6191\n",
            "Epoch [75/100], Step [701/936], Loss: 17334.6719\n",
            "Epoch [75/100], Step [801/936], Loss: 18236.5352\n",
            "Epoch [75/100], Step [901/936], Loss: 16313.2578\n",
            "Epoch [76/100], Step [1/936], Loss: 17125.2754\n",
            "Epoch [76/100], Step [101/936], Loss: 17196.4512\n",
            "Epoch [76/100], Step [201/936], Loss: 16497.6992\n",
            "Epoch [76/100], Step [301/936], Loss: 17140.6348\n",
            "Epoch [76/100], Step [401/936], Loss: 16243.3828\n",
            "Epoch [76/100], Step [501/936], Loss: 16866.3867\n",
            "Epoch [76/100], Step [601/936], Loss: 16382.4219\n",
            "Epoch [76/100], Step [701/936], Loss: 17612.3574\n",
            "Epoch [76/100], Step [801/936], Loss: 16894.7969\n",
            "Epoch [76/100], Step [901/936], Loss: 17516.4492\n",
            "Epoch [77/100], Step [1/936], Loss: 18545.3750\n",
            "Epoch [77/100], Step [101/936], Loss: 16441.5176\n",
            "Epoch [77/100], Step [201/936], Loss: 17338.3262\n",
            "Epoch [77/100], Step [301/936], Loss: 16152.2432\n",
            "Epoch [77/100], Step [401/936], Loss: 17048.7305\n",
            "Epoch [77/100], Step [501/936], Loss: 18054.0801\n",
            "Epoch [77/100], Step [601/936], Loss: 16941.6348\n",
            "Epoch [77/100], Step [701/936], Loss: 16344.8867\n",
            "Epoch [77/100], Step [801/936], Loss: 16572.5410\n",
            "Epoch [77/100], Step [901/936], Loss: 16829.9961\n",
            "Epoch [78/100], Step [1/936], Loss: 16905.8086\n",
            "Epoch [78/100], Step [101/936], Loss: 15817.0566\n",
            "Epoch [78/100], Step [201/936], Loss: 16741.3730\n",
            "Epoch [78/100], Step [301/936], Loss: 15908.0781\n",
            "Epoch [78/100], Step [401/936], Loss: 16503.2207\n",
            "Epoch [78/100], Step [501/936], Loss: 16545.1406\n",
            "Epoch [78/100], Step [601/936], Loss: 16414.0156\n",
            "Epoch [78/100], Step [701/936], Loss: 16998.1074\n",
            "Epoch [78/100], Step [801/936], Loss: 16754.2441\n",
            "Epoch [78/100], Step [901/936], Loss: 17762.1191\n",
            "Epoch [79/100], Step [1/936], Loss: 17653.1035\n",
            "Epoch [79/100], Step [101/936], Loss: 16658.9805\n",
            "Epoch [79/100], Step [201/936], Loss: 17430.5488\n",
            "Epoch [79/100], Step [301/936], Loss: 16727.6758\n",
            "Epoch [79/100], Step [401/936], Loss: 17029.2969\n",
            "Epoch [79/100], Step [501/936], Loss: 17130.9121\n",
            "Epoch [79/100], Step [601/936], Loss: 16456.1387\n",
            "Epoch [79/100], Step [701/936], Loss: 17098.7305\n",
            "Epoch [79/100], Step [801/936], Loss: 17067.9980\n",
            "Epoch [79/100], Step [901/936], Loss: 17743.8594\n",
            "Epoch [80/100], Step [1/936], Loss: 16475.7930\n",
            "Epoch [80/100], Step [101/936], Loss: 16705.6797\n",
            "Epoch [80/100], Step [201/936], Loss: 18020.7246\n",
            "Epoch [80/100], Step [301/936], Loss: 18751.4863\n",
            "Epoch [80/100], Step [401/936], Loss: 16512.5254\n",
            "Epoch [80/100], Step [501/936], Loss: 16949.2188\n",
            "Epoch [80/100], Step [601/936], Loss: 16943.5938\n",
            "Epoch [80/100], Step [701/936], Loss: 15948.3721\n",
            "Epoch [80/100], Step [801/936], Loss: 16928.5781\n",
            "Epoch [80/100], Step [901/936], Loss: 15759.8057\n",
            "Epoch [81/100], Step [1/936], Loss: 17640.9062\n",
            "Epoch [81/100], Step [101/936], Loss: 17927.9316\n",
            "Epoch [81/100], Step [201/936], Loss: 16994.2031\n",
            "Epoch [81/100], Step [301/936], Loss: 16449.3652\n",
            "Epoch [81/100], Step [401/936], Loss: 16990.4590\n",
            "Epoch [81/100], Step [501/936], Loss: 16377.0010\n",
            "Epoch [81/100], Step [601/936], Loss: 17527.1172\n",
            "Epoch [81/100], Step [701/936], Loss: 16329.0703\n",
            "Epoch [81/100], Step [801/936], Loss: 18102.1211\n",
            "Epoch [81/100], Step [901/936], Loss: 17085.7422\n",
            "Epoch [82/100], Step [1/936], Loss: 17096.8203\n",
            "Epoch [82/100], Step [101/936], Loss: 16113.7041\n",
            "Epoch [82/100], Step [201/936], Loss: 16037.0107\n",
            "Epoch [82/100], Step [301/936], Loss: 17021.5098\n",
            "Epoch [82/100], Step [401/936], Loss: 17090.0625\n",
            "Epoch [82/100], Step [501/936], Loss: 17626.3906\n",
            "Epoch [82/100], Step [601/936], Loss: 17130.8945\n",
            "Epoch [82/100], Step [701/936], Loss: 16988.0742\n",
            "Epoch [82/100], Step [801/936], Loss: 17121.2090\n",
            "Epoch [82/100], Step [901/936], Loss: 17219.2246\n",
            "Epoch [83/100], Step [1/936], Loss: 16692.5898\n",
            "Epoch [83/100], Step [101/936], Loss: 16960.1367\n",
            "Epoch [83/100], Step [201/936], Loss: 16908.5742\n",
            "Epoch [83/100], Step [301/936], Loss: 16429.4648\n",
            "Epoch [83/100], Step [401/936], Loss: 17419.4375\n",
            "Epoch [83/100], Step [501/936], Loss: 16720.1250\n",
            "Epoch [83/100], Step [601/936], Loss: 14752.3770\n",
            "Epoch [83/100], Step [701/936], Loss: 16116.5859\n",
            "Epoch [83/100], Step [801/936], Loss: 17099.9609\n",
            "Epoch [83/100], Step [901/936], Loss: 18090.6582\n",
            "Epoch [84/100], Step [1/936], Loss: 17278.8164\n",
            "Epoch [84/100], Step [101/936], Loss: 16532.8320\n",
            "Epoch [84/100], Step [201/936], Loss: 17249.8066\n",
            "Epoch [84/100], Step [301/936], Loss: 16733.8125\n",
            "Epoch [84/100], Step [401/936], Loss: 16907.2246\n",
            "Epoch [84/100], Step [501/936], Loss: 16361.9492\n",
            "Epoch [84/100], Step [601/936], Loss: 17282.7949\n",
            "Epoch [84/100], Step [701/936], Loss: 17035.4473\n",
            "Epoch [84/100], Step [801/936], Loss: 17151.4238\n",
            "Epoch [84/100], Step [901/936], Loss: 17528.6211\n",
            "Epoch [85/100], Step [1/936], Loss: 17451.6953\n",
            "Epoch [85/100], Step [101/936], Loss: 16073.5801\n",
            "Epoch [85/100], Step [201/936], Loss: 16558.7852\n",
            "Epoch [85/100], Step [301/936], Loss: 16974.0293\n",
            "Epoch [85/100], Step [401/936], Loss: 16386.2285\n",
            "Epoch [85/100], Step [501/936], Loss: 16957.6914\n",
            "Epoch [85/100], Step [601/936], Loss: 16823.0625\n",
            "Epoch [85/100], Step [701/936], Loss: 16423.5371\n",
            "Epoch [85/100], Step [801/936], Loss: 16988.2559\n",
            "Epoch [85/100], Step [901/936], Loss: 17285.7949\n",
            "Epoch [86/100], Step [1/936], Loss: 16529.8047\n",
            "Epoch [86/100], Step [101/936], Loss: 16516.2598\n",
            "Epoch [86/100], Step [201/936], Loss: 17167.0254\n",
            "Epoch [86/100], Step [301/936], Loss: 17147.0645\n",
            "Epoch [86/100], Step [401/936], Loss: 16278.9199\n",
            "Epoch [86/100], Step [501/936], Loss: 17521.0312\n",
            "Epoch [86/100], Step [601/936], Loss: 17682.7148\n",
            "Epoch [86/100], Step [701/936], Loss: 16592.9375\n",
            "Epoch [86/100], Step [801/936], Loss: 17652.6406\n",
            "Epoch [86/100], Step [901/936], Loss: 16834.7031\n",
            "Epoch [87/100], Step [1/936], Loss: 16798.4453\n",
            "Epoch [87/100], Step [101/936], Loss: 16881.8418\n",
            "Epoch [87/100], Step [201/936], Loss: 18482.4844\n",
            "Epoch [87/100], Step [301/936], Loss: 17437.6777\n",
            "Epoch [87/100], Step [401/936], Loss: 16419.1699\n",
            "Epoch [87/100], Step [501/936], Loss: 16672.3105\n",
            "Epoch [87/100], Step [601/936], Loss: 15241.5674\n",
            "Epoch [87/100], Step [701/936], Loss: 16825.1641\n",
            "Epoch [87/100], Step [801/936], Loss: 16401.6250\n",
            "Epoch [87/100], Step [901/936], Loss: 17703.4805\n",
            "Epoch [88/100], Step [1/936], Loss: 17352.4844\n",
            "Epoch [88/100], Step [101/936], Loss: 17283.1895\n",
            "Epoch [88/100], Step [201/936], Loss: 16859.6855\n",
            "Epoch [88/100], Step [301/936], Loss: 17087.8047\n",
            "Epoch [88/100], Step [401/936], Loss: 17739.9609\n",
            "Epoch [88/100], Step [501/936], Loss: 16567.5469\n",
            "Epoch [88/100], Step [601/936], Loss: 17118.5469\n",
            "Epoch [88/100], Step [701/936], Loss: 17154.0605\n",
            "Epoch [88/100], Step [801/936], Loss: 17708.1445\n",
            "Epoch [88/100], Step [901/936], Loss: 16666.3555\n",
            "Epoch [89/100], Step [1/936], Loss: 16636.6602\n",
            "Epoch [89/100], Step [101/936], Loss: 17136.0977\n",
            "Epoch [89/100], Step [201/936], Loss: 15966.5898\n",
            "Epoch [89/100], Step [301/936], Loss: 16660.6602\n",
            "Epoch [89/100], Step [401/936], Loss: 15892.3418\n",
            "Epoch [89/100], Step [501/936], Loss: 17689.3887\n",
            "Epoch [89/100], Step [601/936], Loss: 17704.4668\n",
            "Epoch [89/100], Step [701/936], Loss: 17181.5527\n",
            "Epoch [89/100], Step [801/936], Loss: 17069.2480\n",
            "Epoch [89/100], Step [901/936], Loss: 16828.5605\n",
            "Epoch [90/100], Step [1/936], Loss: 16974.9629\n",
            "Epoch [90/100], Step [101/936], Loss: 17051.4805\n",
            "Epoch [90/100], Step [201/936], Loss: 16626.2656\n",
            "Epoch [90/100], Step [301/936], Loss: 15450.4639\n",
            "Epoch [90/100], Step [401/936], Loss: 17283.6816\n",
            "Epoch [90/100], Step [501/936], Loss: 17562.7676\n",
            "Epoch [90/100], Step [601/936], Loss: 15885.8906\n",
            "Epoch [90/100], Step [701/936], Loss: 18190.9082\n",
            "Epoch [90/100], Step [801/936], Loss: 17830.1816\n",
            "Epoch [90/100], Step [901/936], Loss: 16734.3613\n",
            "Epoch [91/100], Step [1/936], Loss: 16741.6074\n",
            "Epoch [91/100], Step [101/936], Loss: 17523.2363\n",
            "Epoch [91/100], Step [201/936], Loss: 16415.5254\n",
            "Epoch [91/100], Step [301/936], Loss: 17497.9844\n",
            "Epoch [91/100], Step [401/936], Loss: 17901.9707\n",
            "Epoch [91/100], Step [501/936], Loss: 17333.6328\n",
            "Epoch [91/100], Step [601/936], Loss: 17560.7812\n",
            "Epoch [91/100], Step [701/936], Loss: 17379.5957\n",
            "Epoch [91/100], Step [801/936], Loss: 16549.9297\n",
            "Epoch [91/100], Step [901/936], Loss: 17232.6543\n",
            "Epoch [92/100], Step [1/936], Loss: 16947.4023\n",
            "Epoch [92/100], Step [101/936], Loss: 15849.6221\n",
            "Epoch [92/100], Step [201/936], Loss: 16755.5391\n",
            "Epoch [92/100], Step [301/936], Loss: 16628.3691\n",
            "Epoch [92/100], Step [401/936], Loss: 18078.7812\n",
            "Epoch [92/100], Step [501/936], Loss: 16543.4043\n",
            "Epoch [92/100], Step [601/936], Loss: 17085.0488\n",
            "Epoch [92/100], Step [701/936], Loss: 16792.8945\n",
            "Epoch [92/100], Step [801/936], Loss: 16387.1133\n",
            "Epoch [92/100], Step [901/936], Loss: 16767.9434\n",
            "Epoch [93/100], Step [1/936], Loss: 17476.7402\n",
            "Epoch [93/100], Step [101/936], Loss: 16989.0879\n",
            "Epoch [93/100], Step [201/936], Loss: 17490.7578\n",
            "Epoch [93/100], Step [301/936], Loss: 17527.3652\n",
            "Epoch [93/100], Step [401/936], Loss: 17382.6816\n",
            "Epoch [93/100], Step [501/936], Loss: 17381.9141\n",
            "Epoch [93/100], Step [601/936], Loss: 17462.8281\n",
            "Epoch [93/100], Step [701/936], Loss: 16552.6211\n",
            "Epoch [93/100], Step [801/936], Loss: 17924.7930\n",
            "Epoch [93/100], Step [901/936], Loss: 17568.8105\n",
            "Epoch [94/100], Step [1/936], Loss: 16830.8711\n",
            "Epoch [94/100], Step [101/936], Loss: 16169.4961\n",
            "Epoch [94/100], Step [201/936], Loss: 16270.3574\n",
            "Epoch [94/100], Step [301/936], Loss: 16694.0273\n",
            "Epoch [94/100], Step [401/936], Loss: 17268.6426\n",
            "Epoch [94/100], Step [501/936], Loss: 16503.0566\n",
            "Epoch [94/100], Step [601/936], Loss: 16780.0801\n",
            "Epoch [94/100], Step [701/936], Loss: 17943.6973\n",
            "Epoch [94/100], Step [801/936], Loss: 17141.0293\n",
            "Epoch [94/100], Step [901/936], Loss: 16305.6777\n",
            "Epoch [95/100], Step [1/936], Loss: 17004.7305\n",
            "Epoch [95/100], Step [101/936], Loss: 17200.2324\n",
            "Epoch [95/100], Step [201/936], Loss: 17177.3418\n",
            "Epoch [95/100], Step [301/936], Loss: 16500.9395\n",
            "Epoch [95/100], Step [401/936], Loss: 17470.3926\n",
            "Epoch [95/100], Step [501/936], Loss: 16729.6855\n",
            "Epoch [95/100], Step [601/936], Loss: 17266.9023\n",
            "Epoch [95/100], Step [701/936], Loss: 17000.3750\n",
            "Epoch [95/100], Step [801/936], Loss: 17774.7383\n",
            "Epoch [95/100], Step [901/936], Loss: 17112.7012\n",
            "Epoch [96/100], Step [1/936], Loss: 17273.1699\n",
            "Epoch [96/100], Step [101/936], Loss: 16410.0547\n",
            "Epoch [96/100], Step [201/936], Loss: 16624.9863\n",
            "Epoch [96/100], Step [301/936], Loss: 16947.1035\n",
            "Epoch [96/100], Step [401/936], Loss: 16828.3809\n",
            "Epoch [96/100], Step [501/936], Loss: 17386.3398\n",
            "Epoch [96/100], Step [601/936], Loss: 16529.6016\n",
            "Epoch [96/100], Step [701/936], Loss: 16077.5371\n",
            "Epoch [96/100], Step [801/936], Loss: 17273.5449\n",
            "Epoch [96/100], Step [901/936], Loss: 17740.5566\n",
            "Epoch [97/100], Step [1/936], Loss: 16711.2891\n",
            "Epoch [97/100], Step [101/936], Loss: 16763.9590\n",
            "Epoch [97/100], Step [201/936], Loss: 16141.9697\n",
            "Epoch [97/100], Step [301/936], Loss: 17597.6973\n",
            "Epoch [97/100], Step [401/936], Loss: 16420.2227\n",
            "Epoch [97/100], Step [501/936], Loss: 16196.9268\n",
            "Epoch [97/100], Step [601/936], Loss: 16719.3320\n",
            "Epoch [97/100], Step [701/936], Loss: 16892.3438\n",
            "Epoch [97/100], Step [801/936], Loss: 17007.3184\n",
            "Epoch [97/100], Step [901/936], Loss: 16282.0654\n",
            "Epoch [98/100], Step [1/936], Loss: 17421.7520\n",
            "Epoch [98/100], Step [101/936], Loss: 16834.8496\n",
            "Epoch [98/100], Step [201/936], Loss: 17657.4414\n",
            "Epoch [98/100], Step [301/936], Loss: 17585.0703\n",
            "Epoch [98/100], Step [401/936], Loss: 17509.7695\n",
            "Epoch [98/100], Step [501/936], Loss: 16919.0781\n",
            "Epoch [98/100], Step [601/936], Loss: 16116.5371\n",
            "Epoch [98/100], Step [701/936], Loss: 17273.1328\n",
            "Epoch [98/100], Step [801/936], Loss: 16366.0898\n",
            "Epoch [98/100], Step [901/936], Loss: 17248.9414\n",
            "Epoch [99/100], Step [1/936], Loss: 17588.4805\n",
            "Epoch [99/100], Step [101/936], Loss: 16877.6113\n",
            "Epoch [99/100], Step [201/936], Loss: 16692.9141\n",
            "Epoch [99/100], Step [301/936], Loss: 16248.1426\n",
            "Epoch [99/100], Step [401/936], Loss: 17166.0898\n",
            "Epoch [99/100], Step [501/936], Loss: 16242.9775\n",
            "Epoch [99/100], Step [601/936], Loss: 16715.7500\n",
            "Epoch [99/100], Step [701/936], Loss: 17053.3301\n",
            "Epoch [99/100], Step [801/936], Loss: 16668.9316\n",
            "Epoch [99/100], Step [901/936], Loss: 16320.9697\n",
            "Epoch [100/100], Step [1/936], Loss: 18194.6562\n",
            "Epoch [100/100], Step [101/936], Loss: 17377.0625\n",
            "Epoch [100/100], Step [201/936], Loss: 16353.1006\n",
            "Epoch [100/100], Step [301/936], Loss: 16754.1582\n",
            "Epoch [100/100], Step [401/936], Loss: 17630.6367\n",
            "Epoch [100/100], Step [501/936], Loss: 16313.9385\n",
            "Epoch [100/100], Step [601/936], Loss: 18138.9141\n",
            "Epoch [100/100], Step [701/936], Loss: 17032.5859\n",
            "Epoch [100/100], Step [801/936], Loss: 18178.7383\n",
            "Epoch [100/100], Step [901/936], Loss: 17297.2227\n",
            "Accuracy of the SVM on the test images with 100 labeled images: 65.09%\n",
            "Epoch [1/100], Step [1/929], Loss: 37523.0586\n",
            "Epoch [1/100], Step [101/929], Loss: 20650.6367\n",
            "Epoch [1/100], Step [201/929], Loss: 19917.2285\n",
            "Epoch [1/100], Step [301/929], Loss: 21041.6602\n",
            "Epoch [1/100], Step [401/929], Loss: 19195.0625\n",
            "Epoch [1/100], Step [501/929], Loss: 20014.6523\n",
            "Epoch [1/100], Step [601/929], Loss: 20241.5781\n",
            "Epoch [1/100], Step [701/929], Loss: 19219.4395\n",
            "Epoch [1/100], Step [801/929], Loss: 18658.7520\n",
            "Epoch [1/100], Step [901/929], Loss: 19076.5391\n",
            "Epoch [2/100], Step [1/929], Loss: 18215.0332\n",
            "Epoch [2/100], Step [101/929], Loss: 19838.5195\n",
            "Epoch [2/100], Step [201/929], Loss: 17164.9082\n",
            "Epoch [2/100], Step [301/929], Loss: 19639.5547\n",
            "Epoch [2/100], Step [401/929], Loss: 19495.5762\n",
            "Epoch [2/100], Step [501/929], Loss: 17984.4629\n",
            "Epoch [2/100], Step [601/929], Loss: 17285.9414\n",
            "Epoch [2/100], Step [701/929], Loss: 18366.7930\n",
            "Epoch [2/100], Step [801/929], Loss: 17851.8398\n",
            "Epoch [2/100], Step [901/929], Loss: 17837.0566\n",
            "Epoch [3/100], Step [1/929], Loss: 17753.8945\n",
            "Epoch [3/100], Step [101/929], Loss: 18536.2305\n",
            "Epoch [3/100], Step [201/929], Loss: 19195.2891\n",
            "Epoch [3/100], Step [301/929], Loss: 18467.6406\n",
            "Epoch [3/100], Step [401/929], Loss: 18123.3691\n",
            "Epoch [3/100], Step [501/929], Loss: 16181.7852\n",
            "Epoch [3/100], Step [601/929], Loss: 18661.1895\n",
            "Epoch [3/100], Step [701/929], Loss: 18578.0840\n",
            "Epoch [3/100], Step [801/929], Loss: 17224.8828\n",
            "Epoch [3/100], Step [901/929], Loss: 18197.3594\n",
            "Epoch [4/100], Step [1/929], Loss: 17146.0820\n",
            "Epoch [4/100], Step [101/929], Loss: 17225.6895\n",
            "Epoch [4/100], Step [201/929], Loss: 18053.1836\n",
            "Epoch [4/100], Step [301/929], Loss: 17273.7988\n",
            "Epoch [4/100], Step [401/929], Loss: 17522.6895\n",
            "Epoch [4/100], Step [501/929], Loss: 17369.8477\n",
            "Epoch [4/100], Step [601/929], Loss: 18283.7930\n",
            "Epoch [4/100], Step [701/929], Loss: 17617.1387\n",
            "Epoch [4/100], Step [801/929], Loss: 17523.3066\n",
            "Epoch [4/100], Step [901/929], Loss: 17896.3164\n",
            "Epoch [5/100], Step [1/929], Loss: 17940.6621\n",
            "Epoch [5/100], Step [101/929], Loss: 17545.6074\n",
            "Epoch [5/100], Step [201/929], Loss: 17383.2383\n",
            "Epoch [5/100], Step [301/929], Loss: 18323.4766\n",
            "Epoch [5/100], Step [401/929], Loss: 17720.0156\n",
            "Epoch [5/100], Step [501/929], Loss: 17312.2969\n",
            "Epoch [5/100], Step [601/929], Loss: 18624.5742\n",
            "Epoch [5/100], Step [701/929], Loss: 17835.2305\n",
            "Epoch [5/100], Step [801/929], Loss: 18841.7578\n",
            "Epoch [5/100], Step [901/929], Loss: 18002.4082\n",
            "Epoch [6/100], Step [1/929], Loss: 17930.8906\n",
            "Epoch [6/100], Step [101/929], Loss: 18592.4727\n",
            "Epoch [6/100], Step [201/929], Loss: 18636.6230\n",
            "Epoch [6/100], Step [301/929], Loss: 17688.8398\n",
            "Epoch [6/100], Step [401/929], Loss: 17377.1367\n",
            "Epoch [6/100], Step [501/929], Loss: 17076.4707\n",
            "Epoch [6/100], Step [601/929], Loss: 16750.7656\n",
            "Epoch [6/100], Step [701/929], Loss: 17638.5312\n",
            "Epoch [6/100], Step [801/929], Loss: 17448.6445\n",
            "Epoch [6/100], Step [901/929], Loss: 18118.4258\n",
            "Epoch [7/100], Step [1/929], Loss: 17414.9941\n",
            "Epoch [7/100], Step [101/929], Loss: 17744.2031\n",
            "Epoch [7/100], Step [201/929], Loss: 17190.4473\n",
            "Epoch [7/100], Step [301/929], Loss: 15818.9316\n",
            "Epoch [7/100], Step [401/929], Loss: 16885.0586\n",
            "Epoch [7/100], Step [501/929], Loss: 18085.3359\n",
            "Epoch [7/100], Step [601/929], Loss: 17334.2637\n",
            "Epoch [7/100], Step [701/929], Loss: 18454.0449\n",
            "Epoch [7/100], Step [801/929], Loss: 18694.6895\n",
            "Epoch [7/100], Step [901/929], Loss: 18950.1719\n",
            "Epoch [8/100], Step [1/929], Loss: 18289.1816\n",
            "Epoch [8/100], Step [101/929], Loss: 17453.7871\n",
            "Epoch [8/100], Step [201/929], Loss: 18501.8301\n",
            "Epoch [8/100], Step [301/929], Loss: 17749.7227\n",
            "Epoch [8/100], Step [401/929], Loss: 18234.4043\n",
            "Epoch [8/100], Step [501/929], Loss: 18297.2637\n",
            "Epoch [8/100], Step [601/929], Loss: 18449.3945\n",
            "Epoch [8/100], Step [701/929], Loss: 18236.6680\n",
            "Epoch [8/100], Step [801/929], Loss: 17497.0371\n",
            "Epoch [8/100], Step [901/929], Loss: 17571.4512\n",
            "Epoch [9/100], Step [1/929], Loss: 17536.7090\n",
            "Epoch [9/100], Step [101/929], Loss: 17187.5137\n",
            "Epoch [9/100], Step [201/929], Loss: 17098.4277\n",
            "Epoch [9/100], Step [301/929], Loss: 17768.2012\n",
            "Epoch [9/100], Step [401/929], Loss: 18130.1582\n",
            "Epoch [9/100], Step [501/929], Loss: 17675.7500\n",
            "Epoch [9/100], Step [601/929], Loss: 17503.7363\n",
            "Epoch [9/100], Step [701/929], Loss: 18057.9727\n",
            "Epoch [9/100], Step [801/929], Loss: 17522.8574\n",
            "Epoch [9/100], Step [901/929], Loss: 17163.7793\n",
            "Epoch [10/100], Step [1/929], Loss: 16673.8340\n",
            "Epoch [10/100], Step [101/929], Loss: 17747.3613\n",
            "Epoch [10/100], Step [201/929], Loss: 17836.1895\n",
            "Epoch [10/100], Step [301/929], Loss: 17397.0898\n",
            "Epoch [10/100], Step [401/929], Loss: 17578.6250\n",
            "Epoch [10/100], Step [501/929], Loss: 18415.1543\n",
            "Epoch [10/100], Step [601/929], Loss: 18091.9219\n",
            "Epoch [10/100], Step [701/929], Loss: 17597.5938\n",
            "Epoch [10/100], Step [801/929], Loss: 17511.0840\n",
            "Epoch [10/100], Step [901/929], Loss: 18667.7090\n",
            "Epoch [11/100], Step [1/929], Loss: 16934.8242\n",
            "Epoch [11/100], Step [101/929], Loss: 17878.0625\n",
            "Epoch [11/100], Step [201/929], Loss: 17309.5215\n",
            "Epoch [11/100], Step [301/929], Loss: 17003.4727\n",
            "Epoch [11/100], Step [401/929], Loss: 17028.5820\n",
            "Epoch [11/100], Step [501/929], Loss: 17624.8828\n",
            "Epoch [11/100], Step [601/929], Loss: 18815.3633\n",
            "Epoch [11/100], Step [701/929], Loss: 17645.2910\n",
            "Epoch [11/100], Step [801/929], Loss: 17200.0293\n",
            "Epoch [11/100], Step [901/929], Loss: 17377.2461\n",
            "Epoch [12/100], Step [1/929], Loss: 18423.8418\n",
            "Epoch [12/100], Step [101/929], Loss: 16927.4590\n",
            "Epoch [12/100], Step [201/929], Loss: 16487.2188\n",
            "Epoch [12/100], Step [301/929], Loss: 18450.6191\n",
            "Epoch [12/100], Step [401/929], Loss: 17180.5820\n",
            "Epoch [12/100], Step [501/929], Loss: 16944.1504\n",
            "Epoch [12/100], Step [601/929], Loss: 17464.7559\n",
            "Epoch [12/100], Step [701/929], Loss: 17119.0664\n",
            "Epoch [12/100], Step [801/929], Loss: 16496.0195\n",
            "Epoch [12/100], Step [901/929], Loss: 17612.4590\n",
            "Epoch [13/100], Step [1/929], Loss: 16875.5566\n",
            "Epoch [13/100], Step [101/929], Loss: 16693.9297\n",
            "Epoch [13/100], Step [201/929], Loss: 17143.9121\n",
            "Epoch [13/100], Step [301/929], Loss: 18166.4453\n",
            "Epoch [13/100], Step [401/929], Loss: 17500.8555\n",
            "Epoch [13/100], Step [501/929], Loss: 18032.9883\n",
            "Epoch [13/100], Step [601/929], Loss: 19014.8047\n",
            "Epoch [13/100], Step [701/929], Loss: 18868.0078\n",
            "Epoch [13/100], Step [801/929], Loss: 16603.4219\n",
            "Epoch [13/100], Step [901/929], Loss: 17736.7520\n",
            "Epoch [14/100], Step [1/929], Loss: 18452.6680\n",
            "Epoch [14/100], Step [101/929], Loss: 17770.4297\n",
            "Epoch [14/100], Step [201/929], Loss: 18115.2598\n",
            "Epoch [14/100], Step [301/929], Loss: 18037.0059\n",
            "Epoch [14/100], Step [401/929], Loss: 17917.2793\n",
            "Epoch [14/100], Step [501/929], Loss: 17096.4863\n",
            "Epoch [14/100], Step [601/929], Loss: 17728.5723\n",
            "Epoch [14/100], Step [701/929], Loss: 17922.0684\n",
            "Epoch [14/100], Step [801/929], Loss: 17068.3672\n",
            "Epoch [14/100], Step [901/929], Loss: 17728.6211\n",
            "Epoch [15/100], Step [1/929], Loss: 18363.2734\n",
            "Epoch [15/100], Step [101/929], Loss: 17198.6367\n",
            "Epoch [15/100], Step [201/929], Loss: 17688.0781\n",
            "Epoch [15/100], Step [301/929], Loss: 17426.9434\n",
            "Epoch [15/100], Step [401/929], Loss: 17607.9922\n",
            "Epoch [15/100], Step [501/929], Loss: 17200.2695\n",
            "Epoch [15/100], Step [601/929], Loss: 18658.3887\n",
            "Epoch [15/100], Step [701/929], Loss: 18215.0527\n",
            "Epoch [15/100], Step [801/929], Loss: 17476.6055\n",
            "Epoch [15/100], Step [901/929], Loss: 18229.2227\n",
            "Epoch [16/100], Step [1/929], Loss: 18497.3789\n",
            "Epoch [16/100], Step [101/929], Loss: 17234.4980\n",
            "Epoch [16/100], Step [201/929], Loss: 17660.0039\n",
            "Epoch [16/100], Step [301/929], Loss: 17027.7227\n",
            "Epoch [16/100], Step [401/929], Loss: 17578.3633\n",
            "Epoch [16/100], Step [501/929], Loss: 16414.2461\n",
            "Epoch [16/100], Step [601/929], Loss: 17240.7285\n",
            "Epoch [16/100], Step [701/929], Loss: 17413.7402\n",
            "Epoch [16/100], Step [801/929], Loss: 17864.3887\n",
            "Epoch [16/100], Step [901/929], Loss: 16264.4824\n",
            "Epoch [17/100], Step [1/929], Loss: 17752.4707\n",
            "Epoch [17/100], Step [101/929], Loss: 18142.8477\n",
            "Epoch [17/100], Step [201/929], Loss: 17780.2812\n",
            "Epoch [17/100], Step [301/929], Loss: 16619.0566\n",
            "Epoch [17/100], Step [401/929], Loss: 16664.1504\n",
            "Epoch [17/100], Step [501/929], Loss: 17439.3438\n",
            "Epoch [17/100], Step [601/929], Loss: 16395.9180\n",
            "Epoch [17/100], Step [701/929], Loss: 17379.2129\n",
            "Epoch [17/100], Step [801/929], Loss: 17710.6387\n",
            "Epoch [17/100], Step [901/929], Loss: 17437.1445\n",
            "Epoch [18/100], Step [1/929], Loss: 16937.7559\n",
            "Epoch [18/100], Step [101/929], Loss: 17853.5586\n",
            "Epoch [18/100], Step [201/929], Loss: 17461.3125\n",
            "Epoch [18/100], Step [301/929], Loss: 19262.1855\n",
            "Epoch [18/100], Step [401/929], Loss: 17161.3457\n",
            "Epoch [18/100], Step [501/929], Loss: 17558.7852\n",
            "Epoch [18/100], Step [601/929], Loss: 18452.2930\n",
            "Epoch [18/100], Step [701/929], Loss: 17456.1660\n",
            "Epoch [18/100], Step [801/929], Loss: 17994.1523\n",
            "Epoch [18/100], Step [901/929], Loss: 17460.1641\n",
            "Epoch [19/100], Step [1/929], Loss: 16425.3926\n",
            "Epoch [19/100], Step [101/929], Loss: 18979.8555\n",
            "Epoch [19/100], Step [201/929], Loss: 17472.4199\n",
            "Epoch [19/100], Step [301/929], Loss: 16738.3887\n",
            "Epoch [19/100], Step [401/929], Loss: 16597.9629\n",
            "Epoch [19/100], Step [501/929], Loss: 17319.5332\n",
            "Epoch [19/100], Step [601/929], Loss: 17009.0723\n",
            "Epoch [19/100], Step [701/929], Loss: 17393.0742\n",
            "Epoch [19/100], Step [801/929], Loss: 17019.7988\n",
            "Epoch [19/100], Step [901/929], Loss: 17326.0273\n",
            "Epoch [20/100], Step [1/929], Loss: 17679.9570\n",
            "Epoch [20/100], Step [101/929], Loss: 17717.1641\n",
            "Epoch [20/100], Step [201/929], Loss: 16490.9883\n",
            "Epoch [20/100], Step [301/929], Loss: 18357.0195\n",
            "Epoch [20/100], Step [401/929], Loss: 17669.6621\n",
            "Epoch [20/100], Step [501/929], Loss: 18582.4727\n",
            "Epoch [20/100], Step [601/929], Loss: 16023.6748\n",
            "Epoch [20/100], Step [701/929], Loss: 17156.1914\n",
            "Epoch [20/100], Step [801/929], Loss: 17204.0703\n",
            "Epoch [20/100], Step [901/929], Loss: 17319.8125\n",
            "Epoch [21/100], Step [1/929], Loss: 17234.3105\n",
            "Epoch [21/100], Step [101/929], Loss: 17811.6191\n",
            "Epoch [21/100], Step [201/929], Loss: 17122.1211\n",
            "Epoch [21/100], Step [301/929], Loss: 15951.5020\n",
            "Epoch [21/100], Step [401/929], Loss: 17295.0137\n",
            "Epoch [21/100], Step [501/929], Loss: 17427.0840\n",
            "Epoch [21/100], Step [601/929], Loss: 17786.7656\n",
            "Epoch [21/100], Step [701/929], Loss: 16889.7129\n",
            "Epoch [21/100], Step [801/929], Loss: 16645.1328\n",
            "Epoch [21/100], Step [901/929], Loss: 16180.0137\n",
            "Epoch [22/100], Step [1/929], Loss: 17717.1211\n",
            "Epoch [22/100], Step [101/929], Loss: 17587.5254\n",
            "Epoch [22/100], Step [201/929], Loss: 17375.8926\n",
            "Epoch [22/100], Step [301/929], Loss: 18108.1953\n",
            "Epoch [22/100], Step [401/929], Loss: 16746.4609\n",
            "Epoch [22/100], Step [501/929], Loss: 16483.2344\n",
            "Epoch [22/100], Step [601/929], Loss: 17108.0977\n",
            "Epoch [22/100], Step [701/929], Loss: 16416.0957\n",
            "Epoch [22/100], Step [801/929], Loss: 17536.0566\n",
            "Epoch [22/100], Step [901/929], Loss: 16936.8086\n",
            "Epoch [23/100], Step [1/929], Loss: 17601.4395\n",
            "Epoch [23/100], Step [101/929], Loss: 16652.5195\n",
            "Epoch [23/100], Step [201/929], Loss: 17573.9102\n",
            "Epoch [23/100], Step [301/929], Loss: 16239.6963\n",
            "Epoch [23/100], Step [401/929], Loss: 18013.7324\n",
            "Epoch [23/100], Step [501/929], Loss: 17364.1660\n",
            "Epoch [23/100], Step [601/929], Loss: 16855.2832\n",
            "Epoch [23/100], Step [701/929], Loss: 17093.4668\n",
            "Epoch [23/100], Step [801/929], Loss: 16935.2070\n",
            "Epoch [23/100], Step [901/929], Loss: 17507.4961\n",
            "Epoch [24/100], Step [1/929], Loss: 17070.4355\n",
            "Epoch [24/100], Step [101/929], Loss: 17038.6914\n",
            "Epoch [24/100], Step [201/929], Loss: 17504.5879\n",
            "Epoch [24/100], Step [301/929], Loss: 17616.6738\n",
            "Epoch [24/100], Step [401/929], Loss: 16305.1055\n",
            "Epoch [24/100], Step [501/929], Loss: 16716.6426\n",
            "Epoch [24/100], Step [601/929], Loss: 16916.1484\n",
            "Epoch [24/100], Step [701/929], Loss: 16128.3281\n",
            "Epoch [24/100], Step [801/929], Loss: 16434.0410\n",
            "Epoch [24/100], Step [901/929], Loss: 17703.8223\n",
            "Epoch [25/100], Step [1/929], Loss: 17257.9453\n",
            "Epoch [25/100], Step [101/929], Loss: 16716.8223\n",
            "Epoch [25/100], Step [201/929], Loss: 17812.3672\n",
            "Epoch [25/100], Step [301/929], Loss: 17229.2695\n",
            "Epoch [25/100], Step [401/929], Loss: 16407.8848\n",
            "Epoch [25/100], Step [501/929], Loss: 17296.4121\n",
            "Epoch [25/100], Step [601/929], Loss: 17306.1113\n",
            "Epoch [25/100], Step [701/929], Loss: 17059.3086\n",
            "Epoch [25/100], Step [801/929], Loss: 17238.4004\n",
            "Epoch [25/100], Step [901/929], Loss: 16618.0312\n",
            "Epoch [26/100], Step [1/929], Loss: 16183.0908\n",
            "Epoch [26/100], Step [101/929], Loss: 17760.9062\n",
            "Epoch [26/100], Step [201/929], Loss: 18118.2344\n",
            "Epoch [26/100], Step [301/929], Loss: 17534.4102\n",
            "Epoch [26/100], Step [401/929], Loss: 17148.6543\n",
            "Epoch [26/100], Step [501/929], Loss: 16794.0684\n",
            "Epoch [26/100], Step [601/929], Loss: 17505.0488\n",
            "Epoch [26/100], Step [701/929], Loss: 16296.9658\n",
            "Epoch [26/100], Step [801/929], Loss: 17285.7305\n",
            "Epoch [26/100], Step [901/929], Loss: 16419.7344\n",
            "Epoch [27/100], Step [1/929], Loss: 16593.4922\n",
            "Epoch [27/100], Step [101/929], Loss: 16705.0898\n",
            "Epoch [27/100], Step [201/929], Loss: 16617.1309\n",
            "Epoch [27/100], Step [301/929], Loss: 17771.9492\n",
            "Epoch [27/100], Step [401/929], Loss: 17718.9551\n",
            "Epoch [27/100], Step [501/929], Loss: 17700.3770\n",
            "Epoch [27/100], Step [601/929], Loss: 16518.1133\n",
            "Epoch [27/100], Step [701/929], Loss: 17623.4414\n",
            "Epoch [27/100], Step [801/929], Loss: 17035.5215\n",
            "Epoch [27/100], Step [901/929], Loss: 18145.3242\n",
            "Epoch [28/100], Step [1/929], Loss: 16826.9727\n",
            "Epoch [28/100], Step [101/929], Loss: 17270.8906\n",
            "Epoch [28/100], Step [201/929], Loss: 17599.8574\n",
            "Epoch [28/100], Step [301/929], Loss: 16898.2520\n",
            "Epoch [28/100], Step [401/929], Loss: 17010.2344\n",
            "Epoch [28/100], Step [501/929], Loss: 17734.0234\n",
            "Epoch [28/100], Step [601/929], Loss: 17129.0977\n",
            "Epoch [28/100], Step [701/929], Loss: 16836.6191\n",
            "Epoch [28/100], Step [801/929], Loss: 17230.1699\n",
            "Epoch [28/100], Step [901/929], Loss: 17487.0332\n",
            "Epoch [29/100], Step [1/929], Loss: 17354.1973\n",
            "Epoch [29/100], Step [101/929], Loss: 16923.7344\n",
            "Epoch [29/100], Step [201/929], Loss: 16678.3164\n",
            "Epoch [29/100], Step [301/929], Loss: 17030.9375\n",
            "Epoch [29/100], Step [401/929], Loss: 17181.4219\n",
            "Epoch [29/100], Step [501/929], Loss: 17991.4434\n",
            "Epoch [29/100], Step [601/929], Loss: 17058.5273\n",
            "Epoch [29/100], Step [701/929], Loss: 17268.7988\n",
            "Epoch [29/100], Step [801/929], Loss: 16999.1641\n",
            "Epoch [29/100], Step [901/929], Loss: 18447.1230\n",
            "Epoch [30/100], Step [1/929], Loss: 17261.2715\n",
            "Epoch [30/100], Step [101/929], Loss: 17390.0898\n",
            "Epoch [30/100], Step [201/929], Loss: 17405.3594\n",
            "Epoch [30/100], Step [301/929], Loss: 16458.9062\n",
            "Epoch [30/100], Step [401/929], Loss: 17409.4219\n",
            "Epoch [30/100], Step [501/929], Loss: 16981.6582\n",
            "Epoch [30/100], Step [601/929], Loss: 16501.2832\n",
            "Epoch [30/100], Step [701/929], Loss: 17049.4766\n",
            "Epoch [30/100], Step [801/929], Loss: 17412.3086\n",
            "Epoch [30/100], Step [901/929], Loss: 17237.4199\n",
            "Epoch [31/100], Step [1/929], Loss: 17381.1230\n",
            "Epoch [31/100], Step [101/929], Loss: 16135.0068\n",
            "Epoch [31/100], Step [201/929], Loss: 17416.5938\n",
            "Epoch [31/100], Step [301/929], Loss: 16585.7773\n",
            "Epoch [31/100], Step [401/929], Loss: 16908.4629\n",
            "Epoch [31/100], Step [501/929], Loss: 16575.2637\n",
            "Epoch [31/100], Step [601/929], Loss: 17719.0469\n",
            "Epoch [31/100], Step [701/929], Loss: 17142.3438\n",
            "Epoch [31/100], Step [801/929], Loss: 17279.7812\n",
            "Epoch [31/100], Step [901/929], Loss: 17931.4062\n",
            "Epoch [32/100], Step [1/929], Loss: 17348.3574\n",
            "Epoch [32/100], Step [101/929], Loss: 17702.8770\n",
            "Epoch [32/100], Step [201/929], Loss: 17048.7070\n",
            "Epoch [32/100], Step [301/929], Loss: 17090.0488\n",
            "Epoch [32/100], Step [401/929], Loss: 16313.1064\n",
            "Epoch [32/100], Step [501/929], Loss: 16105.1250\n",
            "Epoch [32/100], Step [601/929], Loss: 15871.6221\n",
            "Epoch [32/100], Step [701/929], Loss: 16619.5430\n",
            "Epoch [32/100], Step [801/929], Loss: 16850.8359\n",
            "Epoch [32/100], Step [901/929], Loss: 16647.4805\n",
            "Epoch [33/100], Step [1/929], Loss: 17202.4531\n",
            "Epoch [33/100], Step [101/929], Loss: 17097.3008\n",
            "Epoch [33/100], Step [201/929], Loss: 16736.1777\n",
            "Epoch [33/100], Step [301/929], Loss: 16545.4590\n",
            "Epoch [33/100], Step [401/929], Loss: 16392.7188\n",
            "Epoch [33/100], Step [501/929], Loss: 17171.7383\n",
            "Epoch [33/100], Step [601/929], Loss: 16710.0137\n",
            "Epoch [33/100], Step [701/929], Loss: 17263.9629\n",
            "Epoch [33/100], Step [801/929], Loss: 17196.7578\n",
            "Epoch [33/100], Step [901/929], Loss: 16115.0684\n",
            "Epoch [34/100], Step [1/929], Loss: 17996.6387\n",
            "Epoch [34/100], Step [101/929], Loss: 16462.9512\n",
            "Epoch [34/100], Step [201/929], Loss: 16911.4805\n",
            "Epoch [34/100], Step [301/929], Loss: 16468.2734\n",
            "Epoch [34/100], Step [401/929], Loss: 17641.8789\n",
            "Epoch [34/100], Step [501/929], Loss: 17345.6992\n",
            "Epoch [34/100], Step [601/929], Loss: 17610.7129\n",
            "Epoch [34/100], Step [701/929], Loss: 16977.7363\n",
            "Epoch [34/100], Step [801/929], Loss: 18221.3320\n",
            "Epoch [34/100], Step [901/929], Loss: 15976.0430\n",
            "Epoch [35/100], Step [1/929], Loss: 17723.2480\n",
            "Epoch [35/100], Step [101/929], Loss: 16965.8887\n",
            "Epoch [35/100], Step [201/929], Loss: 17383.4102\n",
            "Epoch [35/100], Step [301/929], Loss: 17403.8848\n",
            "Epoch [35/100], Step [401/929], Loss: 17277.9570\n",
            "Epoch [35/100], Step [501/929], Loss: 16768.2129\n",
            "Epoch [35/100], Step [601/929], Loss: 17559.5664\n",
            "Epoch [35/100], Step [701/929], Loss: 16934.7617\n",
            "Epoch [35/100], Step [801/929], Loss: 17826.3594\n",
            "Epoch [35/100], Step [901/929], Loss: 16513.4199\n",
            "Epoch [36/100], Step [1/929], Loss: 17320.0996\n",
            "Epoch [36/100], Step [101/929], Loss: 16220.9570\n",
            "Epoch [36/100], Step [201/929], Loss: 17554.5293\n",
            "Epoch [36/100], Step [301/929], Loss: 17229.9023\n",
            "Epoch [36/100], Step [401/929], Loss: 16481.5137\n",
            "Epoch [36/100], Step [501/929], Loss: 16504.6641\n",
            "Epoch [36/100], Step [601/929], Loss: 17606.9922\n",
            "Epoch [36/100], Step [701/929], Loss: 16741.9336\n",
            "Epoch [36/100], Step [801/929], Loss: 16442.6133\n",
            "Epoch [36/100], Step [901/929], Loss: 16369.2949\n",
            "Epoch [37/100], Step [1/929], Loss: 17367.1641\n",
            "Epoch [37/100], Step [101/929], Loss: 16699.2695\n",
            "Epoch [37/100], Step [201/929], Loss: 17657.5918\n",
            "Epoch [37/100], Step [301/929], Loss: 17189.1309\n",
            "Epoch [37/100], Step [401/929], Loss: 17330.8164\n",
            "Epoch [37/100], Step [501/929], Loss: 17603.2754\n",
            "Epoch [37/100], Step [601/929], Loss: 16935.6250\n",
            "Epoch [37/100], Step [701/929], Loss: 16938.9023\n",
            "Epoch [37/100], Step [801/929], Loss: 17328.4590\n",
            "Epoch [37/100], Step [901/929], Loss: 16589.0801\n",
            "Epoch [38/100], Step [1/929], Loss: 17996.6094\n",
            "Epoch [38/100], Step [101/929], Loss: 17634.0918\n",
            "Epoch [38/100], Step [201/929], Loss: 18028.6523\n",
            "Epoch [38/100], Step [301/929], Loss: 17020.8535\n",
            "Epoch [38/100], Step [401/929], Loss: 15170.2070\n",
            "Epoch [38/100], Step [501/929], Loss: 18052.7285\n",
            "Epoch [38/100], Step [601/929], Loss: 17950.2188\n",
            "Epoch [38/100], Step [701/929], Loss: 17365.0449\n",
            "Epoch [38/100], Step [801/929], Loss: 17748.9922\n",
            "Epoch [38/100], Step [901/929], Loss: 18382.8984\n",
            "Epoch [39/100], Step [1/929], Loss: 17078.1172\n",
            "Epoch [39/100], Step [101/929], Loss: 18739.1270\n",
            "Epoch [39/100], Step [201/929], Loss: 16493.9277\n",
            "Epoch [39/100], Step [301/929], Loss: 17628.0254\n",
            "Epoch [39/100], Step [401/929], Loss: 17116.6895\n",
            "Epoch [39/100], Step [501/929], Loss: 16583.9707\n",
            "Epoch [39/100], Step [601/929], Loss: 16666.4043\n",
            "Epoch [39/100], Step [701/929], Loss: 18594.8008\n",
            "Epoch [39/100], Step [801/929], Loss: 16378.0215\n",
            "Epoch [39/100], Step [901/929], Loss: 16448.6172\n",
            "Epoch [40/100], Step [1/929], Loss: 16375.7891\n",
            "Epoch [40/100], Step [101/929], Loss: 16555.3359\n",
            "Epoch [40/100], Step [201/929], Loss: 16213.7676\n",
            "Epoch [40/100], Step [301/929], Loss: 17710.5957\n",
            "Epoch [40/100], Step [401/929], Loss: 17689.2559\n",
            "Epoch [40/100], Step [501/929], Loss: 17626.0293\n",
            "Epoch [40/100], Step [601/929], Loss: 17242.8203\n",
            "Epoch [40/100], Step [701/929], Loss: 17524.2012\n",
            "Epoch [40/100], Step [801/929], Loss: 16819.7969\n",
            "Epoch [40/100], Step [901/929], Loss: 17441.0957\n",
            "Epoch [41/100], Step [1/929], Loss: 17560.6953\n",
            "Epoch [41/100], Step [101/929], Loss: 17782.2832\n",
            "Epoch [41/100], Step [201/929], Loss: 17247.2949\n",
            "Epoch [41/100], Step [301/929], Loss: 17122.9199\n",
            "Epoch [41/100], Step [401/929], Loss: 16092.6816\n",
            "Epoch [41/100], Step [501/929], Loss: 17353.5723\n",
            "Epoch [41/100], Step [601/929], Loss: 17280.3906\n",
            "Epoch [41/100], Step [701/929], Loss: 16186.2490\n",
            "Epoch [41/100], Step [801/929], Loss: 16127.4199\n",
            "Epoch [41/100], Step [901/929], Loss: 18238.0742\n",
            "Epoch [42/100], Step [1/929], Loss: 17650.6172\n",
            "Epoch [42/100], Step [101/929], Loss: 17338.4883\n",
            "Epoch [42/100], Step [201/929], Loss: 16635.5020\n",
            "Epoch [42/100], Step [301/929], Loss: 16439.7852\n",
            "Epoch [42/100], Step [401/929], Loss: 17678.0176\n",
            "Epoch [42/100], Step [501/929], Loss: 17228.3027\n",
            "Epoch [42/100], Step [601/929], Loss: 17157.5859\n",
            "Epoch [42/100], Step [701/929], Loss: 18055.5820\n",
            "Epoch [42/100], Step [801/929], Loss: 17578.6270\n",
            "Epoch [42/100], Step [901/929], Loss: 17154.3457\n",
            "Epoch [43/100], Step [1/929], Loss: 17437.2031\n",
            "Epoch [43/100], Step [101/929], Loss: 17078.4668\n",
            "Epoch [43/100], Step [201/929], Loss: 16913.2227\n",
            "Epoch [43/100], Step [301/929], Loss: 16862.1035\n",
            "Epoch [43/100], Step [401/929], Loss: 16512.7188\n",
            "Epoch [43/100], Step [501/929], Loss: 17690.7578\n",
            "Epoch [43/100], Step [601/929], Loss: 17258.4785\n",
            "Epoch [43/100], Step [701/929], Loss: 17081.6914\n",
            "Epoch [43/100], Step [801/929], Loss: 16487.9473\n",
            "Epoch [43/100], Step [901/929], Loss: 16797.6406\n",
            "Epoch [44/100], Step [1/929], Loss: 17367.9844\n",
            "Epoch [44/100], Step [101/929], Loss: 17156.9062\n",
            "Epoch [44/100], Step [201/929], Loss: 17810.0508\n",
            "Epoch [44/100], Step [301/929], Loss: 16592.4395\n",
            "Epoch [44/100], Step [401/929], Loss: 17460.4492\n",
            "Epoch [44/100], Step [501/929], Loss: 17754.3164\n",
            "Epoch [44/100], Step [601/929], Loss: 17281.6660\n",
            "Epoch [44/100], Step [701/929], Loss: 16444.3203\n",
            "Epoch [44/100], Step [801/929], Loss: 17565.1582\n",
            "Epoch [44/100], Step [901/929], Loss: 16837.0078\n",
            "Epoch [45/100], Step [1/929], Loss: 16285.1729\n",
            "Epoch [45/100], Step [101/929], Loss: 16330.5674\n",
            "Epoch [45/100], Step [201/929], Loss: 18329.9941\n",
            "Epoch [45/100], Step [301/929], Loss: 16569.3594\n",
            "Epoch [45/100], Step [401/929], Loss: 16570.7422\n",
            "Epoch [45/100], Step [501/929], Loss: 16713.9570\n",
            "Epoch [45/100], Step [601/929], Loss: 17291.1035\n",
            "Epoch [45/100], Step [701/929], Loss: 16609.4395\n",
            "Epoch [45/100], Step [801/929], Loss: 17268.2461\n",
            "Epoch [45/100], Step [901/929], Loss: 17327.5605\n",
            "Epoch [46/100], Step [1/929], Loss: 17786.3379\n",
            "Epoch [46/100], Step [101/929], Loss: 17118.6504\n",
            "Epoch [46/100], Step [201/929], Loss: 15993.2354\n",
            "Epoch [46/100], Step [301/929], Loss: 16043.1025\n",
            "Epoch [46/100], Step [401/929], Loss: 17411.7852\n",
            "Epoch [46/100], Step [501/929], Loss: 17310.0605\n",
            "Epoch [46/100], Step [601/929], Loss: 17530.0762\n",
            "Epoch [46/100], Step [701/929], Loss: 17322.2656\n",
            "Epoch [46/100], Step [801/929], Loss: 16543.7461\n",
            "Epoch [46/100], Step [901/929], Loss: 17334.7598\n",
            "Epoch [47/100], Step [1/929], Loss: 17424.0430\n",
            "Epoch [47/100], Step [101/929], Loss: 16499.3203\n",
            "Epoch [47/100], Step [201/929], Loss: 17364.8906\n",
            "Epoch [47/100], Step [301/929], Loss: 17278.8574\n",
            "Epoch [47/100], Step [401/929], Loss: 16230.3213\n",
            "Epoch [47/100], Step [501/929], Loss: 16686.2051\n",
            "Epoch [47/100], Step [601/929], Loss: 17002.2676\n",
            "Epoch [47/100], Step [701/929], Loss: 16790.5859\n",
            "Epoch [47/100], Step [801/929], Loss: 16681.3125\n",
            "Epoch [47/100], Step [901/929], Loss: 17563.4219\n",
            "Epoch [48/100], Step [1/929], Loss: 16695.5957\n",
            "Epoch [48/100], Step [101/929], Loss: 16762.7012\n",
            "Epoch [48/100], Step [201/929], Loss: 16857.8574\n",
            "Epoch [48/100], Step [301/929], Loss: 16708.7539\n",
            "Epoch [48/100], Step [401/929], Loss: 17720.5527\n",
            "Epoch [48/100], Step [501/929], Loss: 17634.7031\n",
            "Epoch [48/100], Step [601/929], Loss: 16521.2402\n",
            "Epoch [48/100], Step [701/929], Loss: 16882.4395\n",
            "Epoch [48/100], Step [801/929], Loss: 17521.0234\n",
            "Epoch [48/100], Step [901/929], Loss: 16355.9883\n",
            "Epoch [49/100], Step [1/929], Loss: 16230.2334\n",
            "Epoch [49/100], Step [101/929], Loss: 17231.4863\n",
            "Epoch [49/100], Step [201/929], Loss: 16024.1348\n",
            "Epoch [49/100], Step [301/929], Loss: 17636.5977\n",
            "Epoch [49/100], Step [401/929], Loss: 16851.6738\n",
            "Epoch [49/100], Step [501/929], Loss: 17451.2676\n",
            "Epoch [49/100], Step [601/929], Loss: 16653.2930\n",
            "Epoch [49/100], Step [701/929], Loss: 17958.2051\n",
            "Epoch [49/100], Step [801/929], Loss: 16418.4121\n",
            "Epoch [49/100], Step [901/929], Loss: 17411.5000\n",
            "Epoch [50/100], Step [1/929], Loss: 16774.5508\n",
            "Epoch [50/100], Step [101/929], Loss: 16961.5645\n",
            "Epoch [50/100], Step [201/929], Loss: 18237.7832\n",
            "Epoch [50/100], Step [301/929], Loss: 17605.1250\n",
            "Epoch [50/100], Step [401/929], Loss: 17903.6211\n",
            "Epoch [50/100], Step [501/929], Loss: 17561.2363\n",
            "Epoch [50/100], Step [601/929], Loss: 18517.2051\n",
            "Epoch [50/100], Step [701/929], Loss: 17393.5078\n",
            "Epoch [50/100], Step [801/929], Loss: 15863.9102\n",
            "Epoch [50/100], Step [901/929], Loss: 18177.8262\n",
            "Epoch [51/100], Step [1/929], Loss: 16724.6191\n",
            "Epoch [51/100], Step [101/929], Loss: 16928.0527\n",
            "Epoch [51/100], Step [201/929], Loss: 16668.2656\n",
            "Epoch [51/100], Step [301/929], Loss: 16864.2148\n",
            "Epoch [51/100], Step [401/929], Loss: 16236.5518\n",
            "Epoch [51/100], Step [501/929], Loss: 17055.7969\n",
            "Epoch [51/100], Step [601/929], Loss: 17013.9297\n",
            "Epoch [51/100], Step [701/929], Loss: 16784.2637\n",
            "Epoch [51/100], Step [801/929], Loss: 17858.8730\n",
            "Epoch [51/100], Step [901/929], Loss: 15931.0840\n",
            "Epoch [52/100], Step [1/929], Loss: 17536.8066\n",
            "Epoch [52/100], Step [101/929], Loss: 17052.7402\n",
            "Epoch [52/100], Step [201/929], Loss: 16502.2539\n",
            "Epoch [52/100], Step [301/929], Loss: 17362.4844\n",
            "Epoch [52/100], Step [401/929], Loss: 16778.1973\n",
            "Epoch [52/100], Step [501/929], Loss: 16838.8809\n",
            "Epoch [52/100], Step [601/929], Loss: 16702.5664\n",
            "Epoch [52/100], Step [701/929], Loss: 16700.8770\n",
            "Epoch [52/100], Step [801/929], Loss: 16240.9766\n",
            "Epoch [52/100], Step [901/929], Loss: 16345.8721\n",
            "Epoch [53/100], Step [1/929], Loss: 16931.9570\n",
            "Epoch [53/100], Step [101/929], Loss: 16994.8887\n",
            "Epoch [53/100], Step [201/929], Loss: 16645.8555\n",
            "Epoch [53/100], Step [301/929], Loss: 17600.4844\n",
            "Epoch [53/100], Step [401/929], Loss: 17206.6465\n",
            "Epoch [53/100], Step [501/929], Loss: 17426.2656\n",
            "Epoch [53/100], Step [601/929], Loss: 16551.2285\n",
            "Epoch [53/100], Step [701/929], Loss: 17917.5977\n",
            "Epoch [53/100], Step [801/929], Loss: 16280.7109\n",
            "Epoch [53/100], Step [901/929], Loss: 17599.7168\n",
            "Epoch [54/100], Step [1/929], Loss: 17827.8750\n",
            "Epoch [54/100], Step [101/929], Loss: 16649.7168\n",
            "Epoch [54/100], Step [201/929], Loss: 17057.3008\n",
            "Epoch [54/100], Step [301/929], Loss: 16726.6367\n",
            "Epoch [54/100], Step [401/929], Loss: 17182.5879\n",
            "Epoch [54/100], Step [501/929], Loss: 16875.9434\n",
            "Epoch [54/100], Step [601/929], Loss: 17401.0137\n",
            "Epoch [54/100], Step [701/929], Loss: 17816.1250\n",
            "Epoch [54/100], Step [801/929], Loss: 17316.0039\n",
            "Epoch [54/100], Step [901/929], Loss: 16310.1523\n",
            "Epoch [55/100], Step [1/929], Loss: 15615.0684\n",
            "Epoch [55/100], Step [101/929], Loss: 16639.9102\n",
            "Epoch [55/100], Step [201/929], Loss: 17236.2305\n",
            "Epoch [55/100], Step [301/929], Loss: 17135.0137\n",
            "Epoch [55/100], Step [401/929], Loss: 17049.8203\n",
            "Epoch [55/100], Step [501/929], Loss: 16757.7363\n",
            "Epoch [55/100], Step [601/929], Loss: 17372.2402\n",
            "Epoch [55/100], Step [701/929], Loss: 16869.0957\n",
            "Epoch [55/100], Step [801/929], Loss: 16166.1641\n",
            "Epoch [55/100], Step [901/929], Loss: 16629.7637\n",
            "Epoch [56/100], Step [1/929], Loss: 16714.5410\n",
            "Epoch [56/100], Step [101/929], Loss: 16208.6436\n",
            "Epoch [56/100], Step [201/929], Loss: 16819.6914\n",
            "Epoch [56/100], Step [301/929], Loss: 15405.1104\n",
            "Epoch [56/100], Step [401/929], Loss: 17283.1738\n",
            "Epoch [56/100], Step [501/929], Loss: 16291.6973\n",
            "Epoch [56/100], Step [601/929], Loss: 16657.9844\n",
            "Epoch [56/100], Step [701/929], Loss: 18047.8203\n",
            "Epoch [56/100], Step [801/929], Loss: 16617.4609\n",
            "Epoch [56/100], Step [901/929], Loss: 18098.2715\n",
            "Epoch [57/100], Step [1/929], Loss: 17457.5879\n",
            "Epoch [57/100], Step [101/929], Loss: 16116.1660\n",
            "Epoch [57/100], Step [201/929], Loss: 16720.4844\n",
            "Epoch [57/100], Step [301/929], Loss: 16221.2812\n",
            "Epoch [57/100], Step [401/929], Loss: 16437.7422\n",
            "Epoch [57/100], Step [501/929], Loss: 15754.7012\n",
            "Epoch [57/100], Step [601/929], Loss: 17515.4297\n",
            "Epoch [57/100], Step [701/929], Loss: 17884.5508\n",
            "Epoch [57/100], Step [801/929], Loss: 16269.9766\n",
            "Epoch [57/100], Step [901/929], Loss: 16520.1680\n",
            "Epoch [58/100], Step [1/929], Loss: 17447.8340\n",
            "Epoch [58/100], Step [101/929], Loss: 16900.2129\n",
            "Epoch [58/100], Step [201/929], Loss: 17927.7207\n",
            "Epoch [58/100], Step [301/929], Loss: 17126.0000\n",
            "Epoch [58/100], Step [401/929], Loss: 15975.6621\n",
            "Epoch [58/100], Step [501/929], Loss: 16608.1641\n",
            "Epoch [58/100], Step [601/929], Loss: 17779.7285\n",
            "Epoch [58/100], Step [701/929], Loss: 17856.5586\n",
            "Epoch [58/100], Step [801/929], Loss: 17230.8184\n",
            "Epoch [58/100], Step [901/929], Loss: 17190.0781\n",
            "Epoch [59/100], Step [1/929], Loss: 18432.9258\n",
            "Epoch [59/100], Step [101/929], Loss: 16610.6680\n",
            "Epoch [59/100], Step [201/929], Loss: 17132.1621\n",
            "Epoch [59/100], Step [301/929], Loss: 16988.4023\n",
            "Epoch [59/100], Step [401/929], Loss: 18757.2402\n",
            "Epoch [59/100], Step [501/929], Loss: 16776.8320\n",
            "Epoch [59/100], Step [601/929], Loss: 17432.1211\n",
            "Epoch [59/100], Step [701/929], Loss: 17295.2578\n",
            "Epoch [59/100], Step [801/929], Loss: 17502.4062\n",
            "Epoch [59/100], Step [901/929], Loss: 16368.5879\n",
            "Epoch [60/100], Step [1/929], Loss: 17636.6582\n",
            "Epoch [60/100], Step [101/929], Loss: 16784.8789\n",
            "Epoch [60/100], Step [201/929], Loss: 17905.4609\n",
            "Epoch [60/100], Step [301/929], Loss: 17082.8594\n",
            "Epoch [60/100], Step [401/929], Loss: 17777.7559\n",
            "Epoch [60/100], Step [501/929], Loss: 18122.4570\n",
            "Epoch [60/100], Step [601/929], Loss: 16147.2197\n",
            "Epoch [60/100], Step [701/929], Loss: 17090.0918\n",
            "Epoch [60/100], Step [801/929], Loss: 17525.9180\n",
            "Epoch [60/100], Step [901/929], Loss: 17356.8887\n",
            "Epoch [61/100], Step [1/929], Loss: 16891.1055\n",
            "Epoch [61/100], Step [101/929], Loss: 17575.4316\n",
            "Epoch [61/100], Step [201/929], Loss: 16245.9570\n",
            "Epoch [61/100], Step [301/929], Loss: 16713.9355\n",
            "Epoch [61/100], Step [401/929], Loss: 16501.1758\n",
            "Epoch [61/100], Step [501/929], Loss: 16413.9902\n",
            "Epoch [61/100], Step [601/929], Loss: 16978.9395\n",
            "Epoch [61/100], Step [701/929], Loss: 17380.2988\n",
            "Epoch [61/100], Step [801/929], Loss: 15996.4014\n",
            "Epoch [61/100], Step [901/929], Loss: 15618.0996\n",
            "Epoch [62/100], Step [1/929], Loss: 16916.7695\n",
            "Epoch [62/100], Step [101/929], Loss: 17250.3086\n",
            "Epoch [62/100], Step [201/929], Loss: 16539.8770\n",
            "Epoch [62/100], Step [301/929], Loss: 16948.8867\n",
            "Epoch [62/100], Step [401/929], Loss: 16374.1104\n",
            "Epoch [62/100], Step [501/929], Loss: 17324.3789\n",
            "Epoch [62/100], Step [601/929], Loss: 16838.3926\n",
            "Epoch [62/100], Step [701/929], Loss: 16696.8789\n",
            "Epoch [62/100], Step [801/929], Loss: 17535.1367\n",
            "Epoch [62/100], Step [901/929], Loss: 17228.5469\n",
            "Epoch [63/100], Step [1/929], Loss: 16955.5371\n",
            "Epoch [63/100], Step [101/929], Loss: 17583.7129\n",
            "Epoch [63/100], Step [201/929], Loss: 16558.1934\n",
            "Epoch [63/100], Step [301/929], Loss: 17008.3594\n",
            "Epoch [63/100], Step [401/929], Loss: 17993.3066\n",
            "Epoch [63/100], Step [501/929], Loss: 16896.5801\n",
            "Epoch [63/100], Step [601/929], Loss: 17369.2422\n",
            "Epoch [63/100], Step [701/929], Loss: 17148.7266\n",
            "Epoch [63/100], Step [801/929], Loss: 16945.5430\n",
            "Epoch [63/100], Step [901/929], Loss: 17505.2168\n",
            "Epoch [64/100], Step [1/929], Loss: 17240.1016\n",
            "Epoch [64/100], Step [101/929], Loss: 16325.7324\n",
            "Epoch [64/100], Step [201/929], Loss: 19364.7676\n",
            "Epoch [64/100], Step [301/929], Loss: 16541.4961\n",
            "Epoch [64/100], Step [401/929], Loss: 16258.8486\n",
            "Epoch [64/100], Step [501/929], Loss: 17173.5234\n",
            "Epoch [64/100], Step [601/929], Loss: 17012.6836\n",
            "Epoch [64/100], Step [701/929], Loss: 17269.7949\n",
            "Epoch [64/100], Step [801/929], Loss: 17802.2520\n",
            "Epoch [64/100], Step [901/929], Loss: 16644.5703\n",
            "Epoch [65/100], Step [1/929], Loss: 16710.2207\n",
            "Epoch [65/100], Step [101/929], Loss: 17186.1660\n",
            "Epoch [65/100], Step [201/929], Loss: 17110.3281\n",
            "Epoch [65/100], Step [301/929], Loss: 17164.7461\n",
            "Epoch [65/100], Step [401/929], Loss: 17366.3535\n",
            "Epoch [65/100], Step [501/929], Loss: 16995.7188\n",
            "Epoch [65/100], Step [601/929], Loss: 16762.5410\n",
            "Epoch [65/100], Step [701/929], Loss: 17294.8789\n",
            "Epoch [65/100], Step [801/929], Loss: 17708.5508\n",
            "Epoch [65/100], Step [901/929], Loss: 17736.1953\n",
            "Epoch [66/100], Step [1/929], Loss: 17459.9297\n",
            "Epoch [66/100], Step [101/929], Loss: 17521.3184\n",
            "Epoch [66/100], Step [201/929], Loss: 16626.3828\n",
            "Epoch [66/100], Step [301/929], Loss: 16311.5410\n",
            "Epoch [66/100], Step [401/929], Loss: 17031.0059\n",
            "Epoch [66/100], Step [501/929], Loss: 17257.2910\n",
            "Epoch [66/100], Step [601/929], Loss: 16922.9961\n",
            "Epoch [66/100], Step [701/929], Loss: 17659.7363\n",
            "Epoch [66/100], Step [801/929], Loss: 16689.0312\n",
            "Epoch [66/100], Step [901/929], Loss: 17424.8730\n",
            "Epoch [67/100], Step [1/929], Loss: 17428.1934\n",
            "Epoch [67/100], Step [101/929], Loss: 16706.8320\n",
            "Epoch [67/100], Step [201/929], Loss: 17609.6191\n",
            "Epoch [67/100], Step [301/929], Loss: 17043.2969\n",
            "Epoch [67/100], Step [401/929], Loss: 16401.6152\n",
            "Epoch [67/100], Step [501/929], Loss: 17280.0703\n",
            "Epoch [67/100], Step [601/929], Loss: 17328.5742\n",
            "Epoch [67/100], Step [701/929], Loss: 16806.1348\n",
            "Epoch [67/100], Step [801/929], Loss: 16401.3477\n",
            "Epoch [67/100], Step [901/929], Loss: 15522.3994\n",
            "Epoch [68/100], Step [1/929], Loss: 17226.9004\n",
            "Epoch [68/100], Step [101/929], Loss: 15695.3262\n",
            "Epoch [68/100], Step [201/929], Loss: 17790.3555\n",
            "Epoch [68/100], Step [301/929], Loss: 17852.3848\n",
            "Epoch [68/100], Step [401/929], Loss: 17184.2578\n",
            "Epoch [68/100], Step [501/929], Loss: 17071.1016\n",
            "Epoch [68/100], Step [601/929], Loss: 17049.6934\n",
            "Epoch [68/100], Step [701/929], Loss: 17127.5410\n",
            "Epoch [68/100], Step [801/929], Loss: 17877.3145\n",
            "Epoch [68/100], Step [901/929], Loss: 16386.4531\n",
            "Epoch [69/100], Step [1/929], Loss: 16237.7100\n",
            "Epoch [69/100], Step [101/929], Loss: 17194.7578\n",
            "Epoch [69/100], Step [201/929], Loss: 16894.4004\n",
            "Epoch [69/100], Step [301/929], Loss: 16684.0820\n",
            "Epoch [69/100], Step [401/929], Loss: 16695.5137\n",
            "Epoch [69/100], Step [501/929], Loss: 16991.9805\n",
            "Epoch [69/100], Step [601/929], Loss: 17599.6172\n",
            "Epoch [69/100], Step [701/929], Loss: 17242.1191\n",
            "Epoch [69/100], Step [801/929], Loss: 17781.1152\n",
            "Epoch [69/100], Step [901/929], Loss: 17210.6172\n",
            "Epoch [70/100], Step [1/929], Loss: 16877.3711\n",
            "Epoch [70/100], Step [101/929], Loss: 16123.9746\n",
            "Epoch [70/100], Step [201/929], Loss: 15707.3125\n",
            "Epoch [70/100], Step [301/929], Loss: 18323.4590\n",
            "Epoch [70/100], Step [401/929], Loss: 15377.4004\n",
            "Epoch [70/100], Step [501/929], Loss: 16217.1309\n",
            "Epoch [70/100], Step [601/929], Loss: 17112.8496\n",
            "Epoch [70/100], Step [701/929], Loss: 16951.1621\n",
            "Epoch [70/100], Step [801/929], Loss: 16564.4609\n",
            "Epoch [70/100], Step [901/929], Loss: 18223.2500\n",
            "Epoch [71/100], Step [1/929], Loss: 16589.8320\n",
            "Epoch [71/100], Step [101/929], Loss: 16355.3906\n",
            "Epoch [71/100], Step [201/929], Loss: 16983.0273\n",
            "Epoch [71/100], Step [301/929], Loss: 16957.4609\n",
            "Epoch [71/100], Step [401/929], Loss: 15947.1543\n",
            "Epoch [71/100], Step [501/929], Loss: 17206.2441\n",
            "Epoch [71/100], Step [601/929], Loss: 16904.9688\n",
            "Epoch [71/100], Step [701/929], Loss: 17892.3828\n",
            "Epoch [71/100], Step [801/929], Loss: 16415.8691\n",
            "Epoch [71/100], Step [901/929], Loss: 17269.4277\n",
            "Epoch [72/100], Step [1/929], Loss: 17367.5938\n",
            "Epoch [72/100], Step [101/929], Loss: 17334.9824\n",
            "Epoch [72/100], Step [201/929], Loss: 16363.1191\n",
            "Epoch [72/100], Step [301/929], Loss: 17897.6250\n",
            "Epoch [72/100], Step [401/929], Loss: 16750.2461\n",
            "Epoch [72/100], Step [501/929], Loss: 16671.8203\n",
            "Epoch [72/100], Step [601/929], Loss: 17083.0039\n",
            "Epoch [72/100], Step [701/929], Loss: 17470.5645\n",
            "Epoch [72/100], Step [801/929], Loss: 16791.0957\n",
            "Epoch [72/100], Step [901/929], Loss: 15901.8350\n",
            "Epoch [73/100], Step [1/929], Loss: 16045.9678\n",
            "Epoch [73/100], Step [101/929], Loss: 17080.0742\n",
            "Epoch [73/100], Step [201/929], Loss: 17080.3906\n",
            "Epoch [73/100], Step [301/929], Loss: 17134.9609\n",
            "Epoch [73/100], Step [401/929], Loss: 15795.8496\n",
            "Epoch [73/100], Step [501/929], Loss: 17326.9141\n",
            "Epoch [73/100], Step [601/929], Loss: 16996.8105\n",
            "Epoch [73/100], Step [701/929], Loss: 17216.7031\n",
            "Epoch [73/100], Step [801/929], Loss: 16413.0801\n",
            "Epoch [73/100], Step [901/929], Loss: 16595.3730\n",
            "Epoch [74/100], Step [1/929], Loss: 18118.3184\n",
            "Epoch [74/100], Step [101/929], Loss: 17304.7773\n",
            "Epoch [74/100], Step [201/929], Loss: 17115.2617\n",
            "Epoch [74/100], Step [301/929], Loss: 17902.7910\n",
            "Epoch [74/100], Step [401/929], Loss: 16092.3281\n",
            "Epoch [74/100], Step [501/929], Loss: 16931.1836\n",
            "Epoch [74/100], Step [601/929], Loss: 16621.6484\n",
            "Epoch [74/100], Step [701/929], Loss: 15974.6484\n",
            "Epoch [74/100], Step [801/929], Loss: 16098.0811\n",
            "Epoch [74/100], Step [901/929], Loss: 16656.1738\n",
            "Epoch [75/100], Step [1/929], Loss: 17210.5449\n",
            "Epoch [75/100], Step [101/929], Loss: 16813.3320\n",
            "Epoch [75/100], Step [201/929], Loss: 16865.6895\n",
            "Epoch [75/100], Step [301/929], Loss: 16370.6738\n",
            "Epoch [75/100], Step [401/929], Loss: 17279.1855\n",
            "Epoch [75/100], Step [501/929], Loss: 16478.2988\n",
            "Epoch [75/100], Step [601/929], Loss: 18160.9844\n",
            "Epoch [75/100], Step [701/929], Loss: 17071.5078\n",
            "Epoch [75/100], Step [801/929], Loss: 16669.3105\n",
            "Epoch [75/100], Step [901/929], Loss: 16039.2188\n",
            "Epoch [76/100], Step [1/929], Loss: 17065.0312\n",
            "Epoch [76/100], Step [101/929], Loss: 16776.8242\n",
            "Epoch [76/100], Step [201/929], Loss: 16921.0039\n",
            "Epoch [76/100], Step [301/929], Loss: 16201.9688\n",
            "Epoch [76/100], Step [401/929], Loss: 17842.7793\n",
            "Epoch [76/100], Step [501/929], Loss: 17118.1250\n",
            "Epoch [76/100], Step [601/929], Loss: 16878.7812\n",
            "Epoch [76/100], Step [701/929], Loss: 17915.9746\n",
            "Epoch [76/100], Step [801/929], Loss: 18080.0234\n",
            "Epoch [76/100], Step [901/929], Loss: 17571.6621\n",
            "Epoch [77/100], Step [1/929], Loss: 16774.3496\n",
            "Epoch [77/100], Step [101/929], Loss: 17269.4668\n",
            "Epoch [77/100], Step [201/929], Loss: 15948.6689\n",
            "Epoch [77/100], Step [301/929], Loss: 17153.2227\n",
            "Epoch [77/100], Step [401/929], Loss: 16845.4551\n",
            "Epoch [77/100], Step [501/929], Loss: 17325.8711\n",
            "Epoch [77/100], Step [601/929], Loss: 17167.6797\n",
            "Epoch [77/100], Step [701/929], Loss: 16061.8057\n",
            "Epoch [77/100], Step [801/929], Loss: 16762.8652\n",
            "Epoch [77/100], Step [901/929], Loss: 17934.0762\n",
            "Epoch [78/100], Step [1/929], Loss: 17751.3008\n",
            "Epoch [78/100], Step [101/929], Loss: 16243.8916\n",
            "Epoch [78/100], Step [201/929], Loss: 16889.3066\n",
            "Epoch [78/100], Step [301/929], Loss: 17452.1973\n",
            "Epoch [78/100], Step [401/929], Loss: 17045.8828\n",
            "Epoch [78/100], Step [501/929], Loss: 17366.7578\n",
            "Epoch [78/100], Step [601/929], Loss: 17392.6973\n",
            "Epoch [78/100], Step [701/929], Loss: 17062.9219\n",
            "Epoch [78/100], Step [801/929], Loss: 16157.5508\n",
            "Epoch [78/100], Step [901/929], Loss: 16752.1582\n",
            "Epoch [79/100], Step [1/929], Loss: 16767.2461\n",
            "Epoch [79/100], Step [101/929], Loss: 16723.8398\n",
            "Epoch [79/100], Step [201/929], Loss: 17934.7012\n",
            "Epoch [79/100], Step [301/929], Loss: 16764.3574\n",
            "Epoch [79/100], Step [401/929], Loss: 17331.7949\n",
            "Epoch [79/100], Step [501/929], Loss: 17714.9043\n",
            "Epoch [79/100], Step [601/929], Loss: 16291.6758\n",
            "Epoch [79/100], Step [701/929], Loss: 17453.6211\n",
            "Epoch [79/100], Step [801/929], Loss: 15896.2881\n",
            "Epoch [79/100], Step [901/929], Loss: 17481.5625\n",
            "Epoch [80/100], Step [1/929], Loss: 17185.0137\n",
            "Epoch [80/100], Step [101/929], Loss: 16769.8105\n",
            "Epoch [80/100], Step [201/929], Loss: 17182.6348\n",
            "Epoch [80/100], Step [301/929], Loss: 17557.0625\n",
            "Epoch [80/100], Step [401/929], Loss: 15196.5381\n",
            "Epoch [80/100], Step [501/929], Loss: 17442.7051\n",
            "Epoch [80/100], Step [601/929], Loss: 17023.8457\n",
            "Epoch [80/100], Step [701/929], Loss: 18205.2715\n",
            "Epoch [80/100], Step [801/929], Loss: 15640.5371\n",
            "Epoch [80/100], Step [901/929], Loss: 16873.6602\n",
            "Epoch [81/100], Step [1/929], Loss: 17144.3438\n",
            "Epoch [81/100], Step [101/929], Loss: 16654.2559\n",
            "Epoch [81/100], Step [201/929], Loss: 16879.0898\n",
            "Epoch [81/100], Step [301/929], Loss: 16616.2793\n",
            "Epoch [81/100], Step [401/929], Loss: 17412.9746\n",
            "Epoch [81/100], Step [501/929], Loss: 18134.3965\n",
            "Epoch [81/100], Step [601/929], Loss: 16905.6367\n",
            "Epoch [81/100], Step [701/929], Loss: 17838.7891\n",
            "Epoch [81/100], Step [801/929], Loss: 18082.0137\n",
            "Epoch [81/100], Step [901/929], Loss: 16978.9980\n",
            "Epoch [82/100], Step [1/929], Loss: 17822.1016\n",
            "Epoch [82/100], Step [101/929], Loss: 17723.4355\n",
            "Epoch [82/100], Step [201/929], Loss: 17358.9492\n",
            "Epoch [82/100], Step [301/929], Loss: 16135.0664\n",
            "Epoch [82/100], Step [401/929], Loss: 17314.4141\n",
            "Epoch [82/100], Step [501/929], Loss: 17265.2324\n",
            "Epoch [82/100], Step [601/929], Loss: 15971.2061\n",
            "Epoch [82/100], Step [701/929], Loss: 16756.2578\n",
            "Epoch [82/100], Step [801/929], Loss: 17412.7520\n",
            "Epoch [82/100], Step [901/929], Loss: 16277.0742\n",
            "Epoch [83/100], Step [1/929], Loss: 16651.9980\n",
            "Epoch [83/100], Step [101/929], Loss: 18047.4277\n",
            "Epoch [83/100], Step [201/929], Loss: 16855.1387\n",
            "Epoch [83/100], Step [301/929], Loss: 16548.2949\n",
            "Epoch [83/100], Step [401/929], Loss: 17974.2031\n",
            "Epoch [83/100], Step [501/929], Loss: 17261.4258\n",
            "Epoch [83/100], Step [601/929], Loss: 16597.2832\n",
            "Epoch [83/100], Step [701/929], Loss: 16255.1279\n",
            "Epoch [83/100], Step [801/929], Loss: 17647.4766\n",
            "Epoch [83/100], Step [901/929], Loss: 16218.5078\n",
            "Epoch [84/100], Step [1/929], Loss: 15750.3848\n",
            "Epoch [84/100], Step [101/929], Loss: 16620.9277\n",
            "Epoch [84/100], Step [201/929], Loss: 16238.9277\n",
            "Epoch [84/100], Step [301/929], Loss: 16617.9941\n",
            "Epoch [84/100], Step [401/929], Loss: 16635.0059\n",
            "Epoch [84/100], Step [501/929], Loss: 16405.4570\n",
            "Epoch [84/100], Step [601/929], Loss: 18241.3262\n",
            "Epoch [84/100], Step [701/929], Loss: 16763.5977\n",
            "Epoch [84/100], Step [801/929], Loss: 16669.1914\n",
            "Epoch [84/100], Step [901/929], Loss: 16113.6191\n",
            "Epoch [85/100], Step [1/929], Loss: 17071.9043\n",
            "Epoch [85/100], Step [101/929], Loss: 15803.1016\n",
            "Epoch [85/100], Step [201/929], Loss: 17060.3105\n",
            "Epoch [85/100], Step [301/929], Loss: 16892.3145\n",
            "Epoch [85/100], Step [401/929], Loss: 17836.9551\n",
            "Epoch [85/100], Step [501/929], Loss: 17416.5430\n",
            "Epoch [85/100], Step [601/929], Loss: 17755.4355\n",
            "Epoch [85/100], Step [701/929], Loss: 16950.2266\n",
            "Epoch [85/100], Step [801/929], Loss: 16198.9590\n",
            "Epoch [85/100], Step [901/929], Loss: 17033.5547\n",
            "Epoch [86/100], Step [1/929], Loss: 16355.8311\n",
            "Epoch [86/100], Step [101/929], Loss: 17181.2598\n",
            "Epoch [86/100], Step [201/929], Loss: 17393.6777\n",
            "Epoch [86/100], Step [301/929], Loss: 17064.9355\n",
            "Epoch [86/100], Step [401/929], Loss: 17884.6875\n",
            "Epoch [86/100], Step [501/929], Loss: 16966.9375\n",
            "Epoch [86/100], Step [601/929], Loss: 17503.1367\n",
            "Epoch [86/100], Step [701/929], Loss: 17363.3535\n",
            "Epoch [86/100], Step [801/929], Loss: 16808.8613\n",
            "Epoch [86/100], Step [901/929], Loss: 15724.9512\n",
            "Epoch [87/100], Step [1/929], Loss: 16149.3193\n",
            "Epoch [87/100], Step [101/929], Loss: 17458.5430\n",
            "Epoch [87/100], Step [201/929], Loss: 17870.3516\n",
            "Epoch [87/100], Step [301/929], Loss: 17933.4551\n",
            "Epoch [87/100], Step [401/929], Loss: 17300.1582\n",
            "Epoch [87/100], Step [501/929], Loss: 16784.6602\n",
            "Epoch [87/100], Step [601/929], Loss: 16998.3223\n",
            "Epoch [87/100], Step [701/929], Loss: 17633.1953\n",
            "Epoch [87/100], Step [801/929], Loss: 16419.6641\n",
            "Epoch [87/100], Step [901/929], Loss: 17270.0430\n",
            "Epoch [88/100], Step [1/929], Loss: 16876.4395\n",
            "Epoch [88/100], Step [101/929], Loss: 17674.5918\n",
            "Epoch [88/100], Step [201/929], Loss: 17091.4590\n",
            "Epoch [88/100], Step [301/929], Loss: 17641.5625\n",
            "Epoch [88/100], Step [401/929], Loss: 17431.4062\n",
            "Epoch [88/100], Step [501/929], Loss: 16901.2949\n",
            "Epoch [88/100], Step [601/929], Loss: 16685.9473\n",
            "Epoch [88/100], Step [701/929], Loss: 17333.2246\n",
            "Epoch [88/100], Step [801/929], Loss: 17216.0508\n",
            "Epoch [88/100], Step [901/929], Loss: 16723.1758\n",
            "Epoch [89/100], Step [1/929], Loss: 17119.1895\n",
            "Epoch [89/100], Step [101/929], Loss: 17717.1406\n",
            "Epoch [89/100], Step [201/929], Loss: 17173.9961\n",
            "Epoch [89/100], Step [301/929], Loss: 17242.5938\n",
            "Epoch [89/100], Step [401/929], Loss: 17433.9355\n",
            "Epoch [89/100], Step [501/929], Loss: 16312.1836\n",
            "Epoch [89/100], Step [601/929], Loss: 16575.2715\n",
            "Epoch [89/100], Step [701/929], Loss: 16204.6680\n",
            "Epoch [89/100], Step [801/929], Loss: 15562.7627\n",
            "Epoch [89/100], Step [901/929], Loss: 17786.0781\n",
            "Epoch [90/100], Step [1/929], Loss: 16811.5371\n",
            "Epoch [90/100], Step [101/929], Loss: 16846.1680\n",
            "Epoch [90/100], Step [201/929], Loss: 16496.1484\n",
            "Epoch [90/100], Step [301/929], Loss: 16967.4316\n",
            "Epoch [90/100], Step [401/929], Loss: 18191.2227\n",
            "Epoch [90/100], Step [501/929], Loss: 16767.7324\n",
            "Epoch [90/100], Step [601/929], Loss: 17315.4648\n",
            "Epoch [90/100], Step [701/929], Loss: 16648.7168\n",
            "Epoch [90/100], Step [801/929], Loss: 17664.2852\n",
            "Epoch [90/100], Step [901/929], Loss: 16252.7969\n",
            "Epoch [91/100], Step [1/929], Loss: 16726.6875\n",
            "Epoch [91/100], Step [101/929], Loss: 17284.3438\n",
            "Epoch [91/100], Step [201/929], Loss: 17508.1895\n",
            "Epoch [91/100], Step [301/929], Loss: 17619.4863\n",
            "Epoch [91/100], Step [401/929], Loss: 16123.6123\n",
            "Epoch [91/100], Step [501/929], Loss: 16267.0967\n",
            "Epoch [91/100], Step [601/929], Loss: 16609.0527\n",
            "Epoch [91/100], Step [701/929], Loss: 16994.3711\n",
            "Epoch [91/100], Step [801/929], Loss: 17505.8750\n",
            "Epoch [91/100], Step [901/929], Loss: 16878.2578\n",
            "Epoch [92/100], Step [1/929], Loss: 16811.9805\n",
            "Epoch [92/100], Step [101/929], Loss: 16168.9736\n",
            "Epoch [92/100], Step [201/929], Loss: 16916.8711\n",
            "Epoch [92/100], Step [301/929], Loss: 18343.4395\n",
            "Epoch [92/100], Step [401/929], Loss: 18065.7031\n",
            "Epoch [92/100], Step [501/929], Loss: 17030.5938\n",
            "Epoch [92/100], Step [601/929], Loss: 17131.0332\n",
            "Epoch [92/100], Step [701/929], Loss: 17520.8418\n",
            "Epoch [92/100], Step [801/929], Loss: 16186.5430\n",
            "Epoch [92/100], Step [901/929], Loss: 16443.7207\n",
            "Epoch [93/100], Step [1/929], Loss: 16073.9961\n",
            "Epoch [93/100], Step [101/929], Loss: 17727.8438\n",
            "Epoch [93/100], Step [201/929], Loss: 17572.7871\n",
            "Epoch [93/100], Step [301/929], Loss: 17557.6621\n",
            "Epoch [93/100], Step [401/929], Loss: 17597.5625\n",
            "Epoch [93/100], Step [501/929], Loss: 16655.0586\n",
            "Epoch [93/100], Step [601/929], Loss: 16108.0420\n",
            "Epoch [93/100], Step [701/929], Loss: 17318.6113\n",
            "Epoch [93/100], Step [801/929], Loss: 16272.8076\n",
            "Epoch [93/100], Step [901/929], Loss: 16184.6318\n",
            "Epoch [94/100], Step [1/929], Loss: 17427.9941\n",
            "Epoch [94/100], Step [101/929], Loss: 17567.7871\n",
            "Epoch [94/100], Step [201/929], Loss: 17714.9980\n",
            "Epoch [94/100], Step [301/929], Loss: 16614.9941\n",
            "Epoch [94/100], Step [401/929], Loss: 16317.5830\n",
            "Epoch [94/100], Step [501/929], Loss: 17038.8379\n",
            "Epoch [94/100], Step [601/929], Loss: 16759.1406\n",
            "Epoch [94/100], Step [701/929], Loss: 17585.2168\n",
            "Epoch [94/100], Step [801/929], Loss: 18933.6250\n",
            "Epoch [94/100], Step [901/929], Loss: 17146.8066\n",
            "Epoch [95/100], Step [1/929], Loss: 16300.7070\n",
            "Epoch [95/100], Step [101/929], Loss: 17619.7246\n",
            "Epoch [95/100], Step [201/929], Loss: 17907.7500\n",
            "Epoch [95/100], Step [301/929], Loss: 17817.6250\n",
            "Epoch [95/100], Step [401/929], Loss: 15703.7852\n",
            "Epoch [95/100], Step [501/929], Loss: 17065.5645\n",
            "Epoch [95/100], Step [601/929], Loss: 16427.4102\n",
            "Epoch [95/100], Step [701/929], Loss: 16756.5312\n",
            "Epoch [95/100], Step [801/929], Loss: 16696.1523\n",
            "Epoch [95/100], Step [901/929], Loss: 16955.1426\n",
            "Epoch [96/100], Step [1/929], Loss: 16037.1104\n",
            "Epoch [96/100], Step [101/929], Loss: 18196.5996\n",
            "Epoch [96/100], Step [201/929], Loss: 16899.3223\n",
            "Epoch [96/100], Step [301/929], Loss: 17061.5273\n",
            "Epoch [96/100], Step [401/929], Loss: 16933.0625\n",
            "Epoch [96/100], Step [501/929], Loss: 16711.8984\n",
            "Epoch [96/100], Step [601/929], Loss: 16592.9668\n",
            "Epoch [96/100], Step [701/929], Loss: 17596.1562\n",
            "Epoch [96/100], Step [801/929], Loss: 17228.9746\n",
            "Epoch [96/100], Step [901/929], Loss: 16434.1113\n",
            "Epoch [97/100], Step [1/929], Loss: 17880.2305\n",
            "Epoch [97/100], Step [101/929], Loss: 17140.2715\n",
            "Epoch [97/100], Step [201/929], Loss: 17107.7188\n",
            "Epoch [97/100], Step [301/929], Loss: 17510.9746\n",
            "Epoch [97/100], Step [401/929], Loss: 16456.8730\n",
            "Epoch [97/100], Step [501/929], Loss: 17337.5879\n",
            "Epoch [97/100], Step [601/929], Loss: 16476.9492\n",
            "Epoch [97/100], Step [701/929], Loss: 17205.6230\n",
            "Epoch [97/100], Step [801/929], Loss: 18077.5410\n",
            "Epoch [97/100], Step [901/929], Loss: 16018.1973\n",
            "Epoch [98/100], Step [1/929], Loss: 16812.8164\n",
            "Epoch [98/100], Step [101/929], Loss: 16950.5859\n",
            "Epoch [98/100], Step [201/929], Loss: 16662.6777\n",
            "Epoch [98/100], Step [301/929], Loss: 16474.5918\n",
            "Epoch [98/100], Step [401/929], Loss: 17521.3359\n",
            "Epoch [98/100], Step [501/929], Loss: 17756.9395\n",
            "Epoch [98/100], Step [601/929], Loss: 16085.1992\n",
            "Epoch [98/100], Step [701/929], Loss: 17250.7520\n",
            "Epoch [98/100], Step [801/929], Loss: 16492.9805\n",
            "Epoch [98/100], Step [901/929], Loss: 16819.0645\n",
            "Epoch [99/100], Step [1/929], Loss: 17293.3340\n",
            "Epoch [99/100], Step [101/929], Loss: 16793.0586\n",
            "Epoch [99/100], Step [201/929], Loss: 17413.4766\n",
            "Epoch [99/100], Step [301/929], Loss: 17803.0176\n",
            "Epoch [99/100], Step [401/929], Loss: 17306.4355\n",
            "Epoch [99/100], Step [501/929], Loss: 16697.9355\n",
            "Epoch [99/100], Step [601/929], Loss: 16761.7617\n",
            "Epoch [99/100], Step [701/929], Loss: 17089.7695\n",
            "Epoch [99/100], Step [801/929], Loss: 15714.7764\n",
            "Epoch [99/100], Step [901/929], Loss: 17200.5293\n",
            "Epoch [100/100], Step [1/929], Loss: 16445.2383\n",
            "Epoch [100/100], Step [101/929], Loss: 16320.5840\n",
            "Epoch [100/100], Step [201/929], Loss: 15222.0059\n",
            "Epoch [100/100], Step [301/929], Loss: 17662.9512\n",
            "Epoch [100/100], Step [401/929], Loss: 17040.2012\n",
            "Epoch [100/100], Step [501/929], Loss: 17250.4629\n",
            "Epoch [100/100], Step [601/929], Loss: 17258.3418\n",
            "Epoch [100/100], Step [701/929], Loss: 17421.4531\n",
            "Epoch [100/100], Step [801/929], Loss: 17070.4277\n",
            "Epoch [100/100], Step [901/929], Loss: 17168.8809\n",
            "Accuracy of the SVM on the test images with 600 labeled images: 77.79%\n",
            "Epoch [1/100], Step [1/922], Loss: 37973.5859\n",
            "Epoch [1/100], Step [101/922], Loss: 20981.8301\n",
            "Epoch [1/100], Step [201/922], Loss: 20041.6250\n",
            "Epoch [1/100], Step [301/922], Loss: 21428.5391\n",
            "Epoch [1/100], Step [401/922], Loss: 19459.3770\n",
            "Epoch [1/100], Step [501/922], Loss: 18810.1934\n",
            "Epoch [1/100], Step [601/922], Loss: 18259.2812\n",
            "Epoch [1/100], Step [701/922], Loss: 19027.3613\n",
            "Epoch [1/100], Step [801/922], Loss: 19148.8086\n",
            "Epoch [1/100], Step [901/922], Loss: 20018.6094\n",
            "Epoch [2/100], Step [1/922], Loss: 19201.9961\n",
            "Epoch [2/100], Step [101/922], Loss: 18803.6289\n",
            "Epoch [2/100], Step [201/922], Loss: 17922.4727\n",
            "Epoch [2/100], Step [301/922], Loss: 18814.6875\n",
            "Epoch [2/100], Step [401/922], Loss: 17685.5684\n",
            "Epoch [2/100], Step [501/922], Loss: 19352.1309\n",
            "Epoch [2/100], Step [601/922], Loss: 17495.2617\n",
            "Epoch [2/100], Step [701/922], Loss: 18099.2109\n",
            "Epoch [2/100], Step [801/922], Loss: 18935.9062\n",
            "Epoch [2/100], Step [901/922], Loss: 18158.0449\n",
            "Epoch [3/100], Step [1/922], Loss: 19284.4062\n",
            "Epoch [3/100], Step [101/922], Loss: 17872.5527\n",
            "Epoch [3/100], Step [201/922], Loss: 18575.3613\n",
            "Epoch [3/100], Step [301/922], Loss: 18996.4277\n",
            "Epoch [3/100], Step [401/922], Loss: 18715.2754\n",
            "Epoch [3/100], Step [501/922], Loss: 18979.0938\n",
            "Epoch [3/100], Step [601/922], Loss: 17844.9551\n",
            "Epoch [3/100], Step [701/922], Loss: 18356.8691\n",
            "Epoch [3/100], Step [801/922], Loss: 18476.4277\n",
            "Epoch [3/100], Step [901/922], Loss: 17950.2617\n",
            "Epoch [4/100], Step [1/922], Loss: 17168.7266\n",
            "Epoch [4/100], Step [101/922], Loss: 18320.8164\n",
            "Epoch [4/100], Step [201/922], Loss: 18431.3984\n",
            "Epoch [4/100], Step [301/922], Loss: 18505.6504\n",
            "Epoch [4/100], Step [401/922], Loss: 18764.4297\n",
            "Epoch [4/100], Step [501/922], Loss: 17620.3086\n",
            "Epoch [4/100], Step [601/922], Loss: 18410.2695\n",
            "Epoch [4/100], Step [701/922], Loss: 18249.8320\n",
            "Epoch [4/100], Step [801/922], Loss: 18723.8477\n",
            "Epoch [4/100], Step [901/922], Loss: 17068.7832\n",
            "Epoch [5/100], Step [1/922], Loss: 17605.1055\n",
            "Epoch [5/100], Step [101/922], Loss: 17969.8711\n",
            "Epoch [5/100], Step [201/922], Loss: 17195.1895\n",
            "Epoch [5/100], Step [301/922], Loss: 17510.3164\n",
            "Epoch [5/100], Step [401/922], Loss: 17892.7578\n",
            "Epoch [5/100], Step [501/922], Loss: 17628.2754\n",
            "Epoch [5/100], Step [601/922], Loss: 17688.5469\n",
            "Epoch [5/100], Step [701/922], Loss: 18647.6465\n",
            "Epoch [5/100], Step [801/922], Loss: 17253.7422\n",
            "Epoch [5/100], Step [901/922], Loss: 17228.4316\n",
            "Epoch [6/100], Step [1/922], Loss: 18197.7891\n",
            "Epoch [6/100], Step [101/922], Loss: 18025.1992\n",
            "Epoch [6/100], Step [201/922], Loss: 17296.9609\n",
            "Epoch [6/100], Step [301/922], Loss: 17688.7227\n",
            "Epoch [6/100], Step [401/922], Loss: 18799.5801\n",
            "Epoch [6/100], Step [501/922], Loss: 16960.7695\n",
            "Epoch [6/100], Step [601/922], Loss: 17967.3281\n",
            "Epoch [6/100], Step [701/922], Loss: 18465.2285\n",
            "Epoch [6/100], Step [801/922], Loss: 17685.6680\n",
            "Epoch [6/100], Step [901/922], Loss: 17718.6367\n",
            "Epoch [7/100], Step [1/922], Loss: 17743.6172\n",
            "Epoch [7/100], Step [101/922], Loss: 17804.6406\n",
            "Epoch [7/100], Step [201/922], Loss: 17929.1523\n",
            "Epoch [7/100], Step [301/922], Loss: 16152.9297\n",
            "Epoch [7/100], Step [401/922], Loss: 17556.8516\n",
            "Epoch [7/100], Step [501/922], Loss: 17654.1523\n",
            "Epoch [7/100], Step [601/922], Loss: 18460.8555\n",
            "Epoch [7/100], Step [701/922], Loss: 16583.5586\n",
            "Epoch [7/100], Step [801/922], Loss: 15920.2637\n",
            "Epoch [7/100], Step [901/922], Loss: 16805.9316\n",
            "Epoch [8/100], Step [1/922], Loss: 16871.6738\n",
            "Epoch [8/100], Step [101/922], Loss: 17900.0410\n",
            "Epoch [8/100], Step [201/922], Loss: 18460.2109\n",
            "Epoch [8/100], Step [301/922], Loss: 18532.1094\n",
            "Epoch [8/100], Step [401/922], Loss: 17272.7930\n",
            "Epoch [8/100], Step [501/922], Loss: 17080.9707\n",
            "Epoch [8/100], Step [601/922], Loss: 17484.5234\n",
            "Epoch [8/100], Step [701/922], Loss: 17591.6328\n",
            "Epoch [8/100], Step [801/922], Loss: 16969.7773\n",
            "Epoch [8/100], Step [901/922], Loss: 17300.5469\n",
            "Epoch [9/100], Step [1/922], Loss: 17565.5820\n",
            "Epoch [9/100], Step [101/922], Loss: 17706.6445\n",
            "Epoch [9/100], Step [201/922], Loss: 17091.9668\n",
            "Epoch [9/100], Step [301/922], Loss: 16866.9316\n",
            "Epoch [9/100], Step [401/922], Loss: 17019.2559\n",
            "Epoch [9/100], Step [501/922], Loss: 18069.8809\n",
            "Epoch [9/100], Step [601/922], Loss: 18869.8730\n",
            "Epoch [9/100], Step [701/922], Loss: 16583.2500\n",
            "Epoch [9/100], Step [801/922], Loss: 18420.6973\n",
            "Epoch [9/100], Step [901/922], Loss: 18509.1621\n",
            "Epoch [10/100], Step [1/922], Loss: 17692.6406\n",
            "Epoch [10/100], Step [101/922], Loss: 18430.0156\n",
            "Epoch [10/100], Step [201/922], Loss: 17316.0586\n",
            "Epoch [10/100], Step [301/922], Loss: 18145.1289\n",
            "Epoch [10/100], Step [401/922], Loss: 17573.1895\n",
            "Epoch [10/100], Step [501/922], Loss: 18830.6641\n",
            "Epoch [10/100], Step [601/922], Loss: 17587.1621\n",
            "Epoch [10/100], Step [701/922], Loss: 17272.7012\n",
            "Epoch [10/100], Step [801/922], Loss: 17749.3633\n",
            "Epoch [10/100], Step [901/922], Loss: 18313.5977\n",
            "Epoch [11/100], Step [1/922], Loss: 19186.8594\n",
            "Epoch [11/100], Step [101/922], Loss: 18049.3457\n",
            "Epoch [11/100], Step [201/922], Loss: 16929.6973\n",
            "Epoch [11/100], Step [301/922], Loss: 18174.6211\n",
            "Epoch [11/100], Step [401/922], Loss: 17429.1602\n",
            "Epoch [11/100], Step [501/922], Loss: 17118.0273\n",
            "Epoch [11/100], Step [601/922], Loss: 17910.2832\n",
            "Epoch [11/100], Step [701/922], Loss: 18295.0723\n",
            "Epoch [11/100], Step [801/922], Loss: 17423.2012\n",
            "Epoch [11/100], Step [901/922], Loss: 16346.6387\n",
            "Epoch [12/100], Step [1/922], Loss: 18482.7617\n",
            "Epoch [12/100], Step [101/922], Loss: 17467.4922\n",
            "Epoch [12/100], Step [201/922], Loss: 18275.3477\n",
            "Epoch [12/100], Step [301/922], Loss: 17464.0957\n",
            "Epoch [12/100], Step [401/922], Loss: 18263.8184\n",
            "Epoch [12/100], Step [501/922], Loss: 17555.5059\n",
            "Epoch [12/100], Step [601/922], Loss: 18234.9727\n",
            "Epoch [12/100], Step [701/922], Loss: 17997.4863\n",
            "Epoch [12/100], Step [801/922], Loss: 15821.1875\n",
            "Epoch [12/100], Step [901/922], Loss: 17727.0664\n",
            "Epoch [13/100], Step [1/922], Loss: 17469.7109\n",
            "Epoch [13/100], Step [101/922], Loss: 17133.9629\n",
            "Epoch [13/100], Step [201/922], Loss: 18433.8047\n",
            "Epoch [13/100], Step [301/922], Loss: 17482.6406\n",
            "Epoch [13/100], Step [401/922], Loss: 17704.2695\n",
            "Epoch [13/100], Step [501/922], Loss: 17062.1562\n",
            "Epoch [13/100], Step [601/922], Loss: 17885.3320\n",
            "Epoch [13/100], Step [701/922], Loss: 17720.2266\n",
            "Epoch [13/100], Step [801/922], Loss: 16821.5918\n",
            "Epoch [13/100], Step [901/922], Loss: 17542.9121\n",
            "Epoch [14/100], Step [1/922], Loss: 17363.8633\n",
            "Epoch [14/100], Step [101/922], Loss: 16490.1172\n",
            "Epoch [14/100], Step [201/922], Loss: 16718.3574\n",
            "Epoch [14/100], Step [301/922], Loss: 17170.1211\n",
            "Epoch [14/100], Step [401/922], Loss: 17593.8008\n",
            "Epoch [14/100], Step [501/922], Loss: 17194.3691\n",
            "Epoch [14/100], Step [601/922], Loss: 16786.6289\n",
            "Epoch [14/100], Step [701/922], Loss: 16964.3711\n",
            "Epoch [14/100], Step [801/922], Loss: 17549.8691\n",
            "Epoch [14/100], Step [901/922], Loss: 17649.2285\n",
            "Epoch [15/100], Step [1/922], Loss: 17980.9062\n",
            "Epoch [15/100], Step [101/922], Loss: 17133.6523\n",
            "Epoch [15/100], Step [201/922], Loss: 17827.7246\n",
            "Epoch [15/100], Step [301/922], Loss: 17908.8789\n",
            "Epoch [15/100], Step [401/922], Loss: 17666.9746\n",
            "Epoch [15/100], Step [501/922], Loss: 17234.5117\n",
            "Epoch [15/100], Step [601/922], Loss: 17253.2520\n",
            "Epoch [15/100], Step [701/922], Loss: 16377.7666\n",
            "Epoch [15/100], Step [801/922], Loss: 16579.6055\n",
            "Epoch [15/100], Step [901/922], Loss: 18439.0820\n",
            "Epoch [16/100], Step [1/922], Loss: 16821.9824\n",
            "Epoch [16/100], Step [101/922], Loss: 17413.8906\n",
            "Epoch [16/100], Step [201/922], Loss: 17058.2559\n",
            "Epoch [16/100], Step [301/922], Loss: 18146.9902\n",
            "Epoch [16/100], Step [401/922], Loss: 18357.0957\n",
            "Epoch [16/100], Step [501/922], Loss: 16876.5762\n",
            "Epoch [16/100], Step [601/922], Loss: 16637.4316\n",
            "Epoch [16/100], Step [701/922], Loss: 16803.4980\n",
            "Epoch [16/100], Step [801/922], Loss: 17302.4258\n",
            "Epoch [16/100], Step [901/922], Loss: 17700.0605\n",
            "Epoch [17/100], Step [1/922], Loss: 16271.3750\n",
            "Epoch [17/100], Step [101/922], Loss: 17476.9434\n",
            "Epoch [17/100], Step [201/922], Loss: 17106.9512\n",
            "Epoch [17/100], Step [301/922], Loss: 17489.0039\n",
            "Epoch [17/100], Step [401/922], Loss: 16916.0918\n",
            "Epoch [17/100], Step [501/922], Loss: 17934.7520\n",
            "Epoch [17/100], Step [601/922], Loss: 17887.1191\n",
            "Epoch [17/100], Step [701/922], Loss: 17525.3223\n",
            "Epoch [17/100], Step [801/922], Loss: 17523.9590\n",
            "Epoch [17/100], Step [901/922], Loss: 17162.4922\n",
            "Epoch [18/100], Step [1/922], Loss: 16291.3291\n",
            "Epoch [18/100], Step [101/922], Loss: 18165.8125\n",
            "Epoch [18/100], Step [201/922], Loss: 16652.4043\n",
            "Epoch [18/100], Step [301/922], Loss: 17730.5137\n",
            "Epoch [18/100], Step [401/922], Loss: 18113.6133\n",
            "Epoch [18/100], Step [501/922], Loss: 17197.4746\n",
            "Epoch [18/100], Step [601/922], Loss: 16977.1523\n",
            "Epoch [18/100], Step [701/922], Loss: 18164.3086\n",
            "Epoch [18/100], Step [801/922], Loss: 17058.9805\n",
            "Epoch [18/100], Step [901/922], Loss: 16961.9805\n",
            "Epoch [19/100], Step [1/922], Loss: 16073.8662\n",
            "Epoch [19/100], Step [101/922], Loss: 17595.7344\n",
            "Epoch [19/100], Step [201/922], Loss: 17576.6562\n",
            "Epoch [19/100], Step [301/922], Loss: 17304.4570\n",
            "Epoch [19/100], Step [401/922], Loss: 17755.5352\n",
            "Epoch [19/100], Step [501/922], Loss: 18326.6816\n",
            "Epoch [19/100], Step [601/922], Loss: 17551.4473\n",
            "Epoch [19/100], Step [701/922], Loss: 17588.6855\n",
            "Epoch [19/100], Step [801/922], Loss: 17364.7812\n",
            "Epoch [19/100], Step [901/922], Loss: 16784.2891\n",
            "Epoch [20/100], Step [1/922], Loss: 17750.5273\n",
            "Epoch [20/100], Step [101/922], Loss: 18911.4648\n",
            "Epoch [20/100], Step [201/922], Loss: 15773.3213\n",
            "Epoch [20/100], Step [301/922], Loss: 18027.4531\n",
            "Epoch [20/100], Step [401/922], Loss: 16140.5830\n",
            "Epoch [20/100], Step [501/922], Loss: 16977.1816\n",
            "Epoch [20/100], Step [601/922], Loss: 17232.5098\n",
            "Epoch [20/100], Step [701/922], Loss: 18250.6348\n",
            "Epoch [20/100], Step [801/922], Loss: 16740.8848\n",
            "Epoch [20/100], Step [901/922], Loss: 16388.0059\n",
            "Epoch [21/100], Step [1/922], Loss: 16281.1992\n",
            "Epoch [21/100], Step [101/922], Loss: 16541.6074\n",
            "Epoch [21/100], Step [201/922], Loss: 17801.3691\n",
            "Epoch [21/100], Step [301/922], Loss: 17818.6699\n",
            "Epoch [21/100], Step [401/922], Loss: 17166.5156\n",
            "Epoch [21/100], Step [501/922], Loss: 17378.8242\n",
            "Epoch [21/100], Step [601/922], Loss: 17114.9277\n",
            "Epoch [21/100], Step [701/922], Loss: 16215.4043\n",
            "Epoch [21/100], Step [801/922], Loss: 17435.4355\n",
            "Epoch [21/100], Step [901/922], Loss: 16748.0801\n",
            "Epoch [22/100], Step [1/922], Loss: 17046.6113\n",
            "Epoch [22/100], Step [101/922], Loss: 16937.0352\n",
            "Epoch [22/100], Step [201/922], Loss: 18316.3262\n",
            "Epoch [22/100], Step [301/922], Loss: 18328.2930\n",
            "Epoch [22/100], Step [401/922], Loss: 17947.9297\n",
            "Epoch [22/100], Step [501/922], Loss: 17905.4277\n",
            "Epoch [22/100], Step [601/922], Loss: 18287.4844\n",
            "Epoch [22/100], Step [701/922], Loss: 16598.6074\n",
            "Epoch [22/100], Step [801/922], Loss: 15839.8066\n",
            "Epoch [22/100], Step [901/922], Loss: 16350.8828\n",
            "Epoch [23/100], Step [1/922], Loss: 17748.5273\n",
            "Epoch [23/100], Step [101/922], Loss: 17325.9180\n",
            "Epoch [23/100], Step [201/922], Loss: 16625.1406\n",
            "Epoch [23/100], Step [301/922], Loss: 17453.4707\n",
            "Epoch [23/100], Step [401/922], Loss: 16189.5068\n",
            "Epoch [23/100], Step [501/922], Loss: 18128.1934\n",
            "Epoch [23/100], Step [601/922], Loss: 17145.7754\n",
            "Epoch [23/100], Step [701/922], Loss: 16970.2441\n",
            "Epoch [23/100], Step [801/922], Loss: 18257.9336\n",
            "Epoch [23/100], Step [901/922], Loss: 17067.6875\n",
            "Epoch [24/100], Step [1/922], Loss: 16564.4961\n",
            "Epoch [24/100], Step [101/922], Loss: 17448.9551\n",
            "Epoch [24/100], Step [201/922], Loss: 16738.5078\n",
            "Epoch [24/100], Step [301/922], Loss: 16850.5723\n",
            "Epoch [24/100], Step [401/922], Loss: 16989.0625\n",
            "Epoch [24/100], Step [501/922], Loss: 18038.1289\n",
            "Epoch [24/100], Step [601/922], Loss: 18300.8203\n",
            "Epoch [24/100], Step [701/922], Loss: 17205.6934\n",
            "Epoch [24/100], Step [801/922], Loss: 18488.3672\n",
            "Epoch [24/100], Step [901/922], Loss: 17376.4785\n",
            "Epoch [25/100], Step [1/922], Loss: 17545.3711\n",
            "Epoch [25/100], Step [101/922], Loss: 17661.6621\n",
            "Epoch [25/100], Step [201/922], Loss: 16572.7832\n",
            "Epoch [25/100], Step [301/922], Loss: 17410.2637\n",
            "Epoch [25/100], Step [401/922], Loss: 17104.6914\n",
            "Epoch [25/100], Step [501/922], Loss: 16458.2637\n",
            "Epoch [25/100], Step [601/922], Loss: 17482.0312\n",
            "Epoch [25/100], Step [701/922], Loss: 17183.4395\n",
            "Epoch [25/100], Step [801/922], Loss: 17089.4102\n",
            "Epoch [25/100], Step [901/922], Loss: 17345.5820\n",
            "Epoch [26/100], Step [1/922], Loss: 15765.0088\n",
            "Epoch [26/100], Step [101/922], Loss: 17099.9922\n",
            "Epoch [26/100], Step [201/922], Loss: 17979.9141\n",
            "Epoch [26/100], Step [301/922], Loss: 16744.9238\n",
            "Epoch [26/100], Step [401/922], Loss: 18069.7832\n",
            "Epoch [26/100], Step [501/922], Loss: 17803.0801\n",
            "Epoch [26/100], Step [601/922], Loss: 17525.7773\n",
            "Epoch [26/100], Step [701/922], Loss: 16147.2070\n",
            "Epoch [26/100], Step [801/922], Loss: 16990.1445\n",
            "Epoch [26/100], Step [901/922], Loss: 17321.8477\n",
            "Epoch [27/100], Step [1/922], Loss: 16698.6875\n",
            "Epoch [27/100], Step [101/922], Loss: 16319.1748\n",
            "Epoch [27/100], Step [201/922], Loss: 17479.0020\n",
            "Epoch [27/100], Step [301/922], Loss: 16684.3262\n",
            "Epoch [27/100], Step [401/922], Loss: 18484.3164\n",
            "Epoch [27/100], Step [501/922], Loss: 17837.3828\n",
            "Epoch [27/100], Step [601/922], Loss: 17428.9922\n",
            "Epoch [27/100], Step [701/922], Loss: 17034.5957\n",
            "Epoch [27/100], Step [801/922], Loss: 17596.2324\n",
            "Epoch [27/100], Step [901/922], Loss: 16511.8867\n",
            "Epoch [28/100], Step [1/922], Loss: 19140.5156\n",
            "Epoch [28/100], Step [101/922], Loss: 16539.1719\n",
            "Epoch [28/100], Step [201/922], Loss: 16605.2754\n",
            "Epoch [28/100], Step [301/922], Loss: 16872.5176\n",
            "Epoch [28/100], Step [401/922], Loss: 17646.4102\n",
            "Epoch [28/100], Step [501/922], Loss: 17711.4863\n",
            "Epoch [28/100], Step [601/922], Loss: 17094.0508\n",
            "Epoch [28/100], Step [701/922], Loss: 16845.3672\n",
            "Epoch [28/100], Step [801/922], Loss: 16547.2031\n",
            "Epoch [28/100], Step [901/922], Loss: 17834.4180\n",
            "Epoch [29/100], Step [1/922], Loss: 16807.0020\n",
            "Epoch [29/100], Step [101/922], Loss: 16951.9609\n",
            "Epoch [29/100], Step [201/922], Loss: 16763.0254\n",
            "Epoch [29/100], Step [301/922], Loss: 17449.1367\n",
            "Epoch [29/100], Step [401/922], Loss: 17749.9980\n",
            "Epoch [29/100], Step [501/922], Loss: 17019.3672\n",
            "Epoch [29/100], Step [601/922], Loss: 16627.4707\n",
            "Epoch [29/100], Step [701/922], Loss: 16020.9678\n",
            "Epoch [29/100], Step [801/922], Loss: 17849.0664\n",
            "Epoch [29/100], Step [901/922], Loss: 16689.3594\n",
            "Epoch [30/100], Step [1/922], Loss: 17090.2422\n",
            "Epoch [30/100], Step [101/922], Loss: 17371.5527\n",
            "Epoch [30/100], Step [201/922], Loss: 16665.6152\n",
            "Epoch [30/100], Step [301/922], Loss: 16154.8613\n",
            "Epoch [30/100], Step [401/922], Loss: 17165.0742\n",
            "Epoch [30/100], Step [501/922], Loss: 16657.4844\n",
            "Epoch [30/100], Step [601/922], Loss: 17926.9258\n",
            "Epoch [30/100], Step [701/922], Loss: 17493.2969\n",
            "Epoch [30/100], Step [801/922], Loss: 17405.1270\n",
            "Epoch [30/100], Step [901/922], Loss: 17686.3301\n",
            "Epoch [31/100], Step [1/922], Loss: 17322.9688\n",
            "Epoch [31/100], Step [101/922], Loss: 16673.0527\n",
            "Epoch [31/100], Step [201/922], Loss: 15880.5703\n",
            "Epoch [31/100], Step [301/922], Loss: 17399.9648\n",
            "Epoch [31/100], Step [401/922], Loss: 17682.3848\n",
            "Epoch [31/100], Step [501/922], Loss: 16654.8652\n",
            "Epoch [31/100], Step [601/922], Loss: 17186.2773\n",
            "Epoch [31/100], Step [701/922], Loss: 17183.6895\n",
            "Epoch [31/100], Step [801/922], Loss: 16789.4102\n",
            "Epoch [31/100], Step [901/922], Loss: 17688.9082\n",
            "Epoch [32/100], Step [1/922], Loss: 17648.4492\n",
            "Epoch [32/100], Step [101/922], Loss: 17005.4883\n",
            "Epoch [32/100], Step [201/922], Loss: 17752.9766\n",
            "Epoch [32/100], Step [301/922], Loss: 17224.0938\n",
            "Epoch [32/100], Step [401/922], Loss: 17366.5371\n",
            "Epoch [32/100], Step [501/922], Loss: 17284.3398\n",
            "Epoch [32/100], Step [601/922], Loss: 17544.2734\n",
            "Epoch [32/100], Step [701/922], Loss: 15951.9326\n",
            "Epoch [32/100], Step [801/922], Loss: 17363.2500\n",
            "Epoch [32/100], Step [901/922], Loss: 16778.2578\n",
            "Epoch [33/100], Step [1/922], Loss: 17285.1582\n",
            "Epoch [33/100], Step [101/922], Loss: 15603.1025\n",
            "Epoch [33/100], Step [201/922], Loss: 16076.5234\n",
            "Epoch [33/100], Step [301/922], Loss: 16759.2422\n",
            "Epoch [33/100], Step [401/922], Loss: 16099.0381\n",
            "Epoch [33/100], Step [501/922], Loss: 17001.2461\n",
            "Epoch [33/100], Step [601/922], Loss: 16863.0020\n",
            "Epoch [33/100], Step [701/922], Loss: 16785.4922\n",
            "Epoch [33/100], Step [801/922], Loss: 16334.3330\n",
            "Epoch [33/100], Step [901/922], Loss: 17910.2793\n",
            "Epoch [34/100], Step [1/922], Loss: 17442.7715\n",
            "Epoch [34/100], Step [101/922], Loss: 16692.8555\n",
            "Epoch [34/100], Step [201/922], Loss: 17544.2402\n",
            "Epoch [34/100], Step [301/922], Loss: 16980.1816\n",
            "Epoch [34/100], Step [401/922], Loss: 17169.0059\n",
            "Epoch [34/100], Step [501/922], Loss: 17454.9082\n",
            "Epoch [34/100], Step [601/922], Loss: 17321.7422\n",
            "Epoch [34/100], Step [701/922], Loss: 16594.1855\n",
            "Epoch [34/100], Step [801/922], Loss: 17274.4258\n",
            "Epoch [34/100], Step [901/922], Loss: 18143.7656\n",
            "Epoch [35/100], Step [1/922], Loss: 17703.9023\n",
            "Epoch [35/100], Step [101/922], Loss: 17144.6914\n",
            "Epoch [35/100], Step [201/922], Loss: 17680.8066\n",
            "Epoch [35/100], Step [301/922], Loss: 16194.4844\n",
            "Epoch [35/100], Step [401/922], Loss: 17331.8887\n",
            "Epoch [35/100], Step [501/922], Loss: 18141.4492\n",
            "Epoch [35/100], Step [601/922], Loss: 17049.7324\n",
            "Epoch [35/100], Step [701/922], Loss: 17074.4219\n",
            "Epoch [35/100], Step [801/922], Loss: 16523.8086\n",
            "Epoch [35/100], Step [901/922], Loss: 17612.4609\n",
            "Epoch [36/100], Step [1/922], Loss: 16905.1152\n",
            "Epoch [36/100], Step [101/922], Loss: 16509.2109\n",
            "Epoch [36/100], Step [201/922], Loss: 16127.8477\n",
            "Epoch [36/100], Step [301/922], Loss: 16816.8008\n",
            "Epoch [36/100], Step [401/922], Loss: 17007.3516\n",
            "Epoch [36/100], Step [501/922], Loss: 17670.1152\n",
            "Epoch [36/100], Step [601/922], Loss: 16662.4922\n",
            "Epoch [36/100], Step [701/922], Loss: 17762.7363\n",
            "Epoch [36/100], Step [801/922], Loss: 17499.8926\n",
            "Epoch [36/100], Step [901/922], Loss: 16741.5254\n",
            "Epoch [37/100], Step [1/922], Loss: 17288.8906\n",
            "Epoch [37/100], Step [101/922], Loss: 17133.4082\n",
            "Epoch [37/100], Step [201/922], Loss: 17200.0840\n",
            "Epoch [37/100], Step [301/922], Loss: 17006.5215\n",
            "Epoch [37/100], Step [401/922], Loss: 18085.4180\n",
            "Epoch [37/100], Step [501/922], Loss: 16933.2285\n",
            "Epoch [37/100], Step [601/922], Loss: 16475.4473\n",
            "Epoch [37/100], Step [701/922], Loss: 17552.2656\n",
            "Epoch [37/100], Step [801/922], Loss: 16732.0059\n",
            "Epoch [37/100], Step [901/922], Loss: 16715.5273\n",
            "Epoch [38/100], Step [1/922], Loss: 16093.9678\n",
            "Epoch [38/100], Step [101/922], Loss: 16953.7969\n",
            "Epoch [38/100], Step [201/922], Loss: 17035.3535\n",
            "Epoch [38/100], Step [301/922], Loss: 17148.8711\n",
            "Epoch [38/100], Step [401/922], Loss: 17001.5410\n",
            "Epoch [38/100], Step [501/922], Loss: 16491.7383\n",
            "Epoch [38/100], Step [601/922], Loss: 18093.0527\n",
            "Epoch [38/100], Step [701/922], Loss: 17273.8281\n",
            "Epoch [38/100], Step [801/922], Loss: 16923.6816\n",
            "Epoch [38/100], Step [901/922], Loss: 17851.7363\n",
            "Epoch [39/100], Step [1/922], Loss: 17341.8496\n",
            "Epoch [39/100], Step [101/922], Loss: 17740.7695\n",
            "Epoch [39/100], Step [201/922], Loss: 16941.1855\n",
            "Epoch [39/100], Step [301/922], Loss: 17002.8750\n",
            "Epoch [39/100], Step [401/922], Loss: 16055.0928\n",
            "Epoch [39/100], Step [501/922], Loss: 16748.2988\n",
            "Epoch [39/100], Step [601/922], Loss: 17415.9043\n",
            "Epoch [39/100], Step [701/922], Loss: 17221.2324\n",
            "Epoch [39/100], Step [801/922], Loss: 16710.9082\n",
            "Epoch [39/100], Step [901/922], Loss: 16910.4219\n",
            "Epoch [40/100], Step [1/922], Loss: 17678.3633\n",
            "Epoch [40/100], Step [101/922], Loss: 16737.1855\n",
            "Epoch [40/100], Step [201/922], Loss: 17619.0996\n",
            "Epoch [40/100], Step [301/922], Loss: 17034.2285\n",
            "Epoch [40/100], Step [401/922], Loss: 16656.7227\n",
            "Epoch [40/100], Step [501/922], Loss: 16555.6191\n",
            "Epoch [40/100], Step [601/922], Loss: 16851.3809\n",
            "Epoch [40/100], Step [701/922], Loss: 16410.7500\n",
            "Epoch [40/100], Step [801/922], Loss: 18208.6250\n",
            "Epoch [40/100], Step [901/922], Loss: 17916.2910\n",
            "Epoch [41/100], Step [1/922], Loss: 17233.2852\n",
            "Epoch [41/100], Step [101/922], Loss: 16839.5762\n",
            "Epoch [41/100], Step [201/922], Loss: 16551.0898\n",
            "Epoch [41/100], Step [301/922], Loss: 16067.6260\n",
            "Epoch [41/100], Step [401/922], Loss: 18307.4766\n",
            "Epoch [41/100], Step [501/922], Loss: 18951.7441\n",
            "Epoch [41/100], Step [601/922], Loss: 17575.9082\n",
            "Epoch [41/100], Step [701/922], Loss: 16978.2246\n",
            "Epoch [41/100], Step [801/922], Loss: 18259.5234\n",
            "Epoch [41/100], Step [901/922], Loss: 16267.2646\n",
            "Epoch [42/100], Step [1/922], Loss: 16848.7441\n",
            "Epoch [42/100], Step [101/922], Loss: 17563.2188\n",
            "Epoch [42/100], Step [201/922], Loss: 17932.5703\n",
            "Epoch [42/100], Step [301/922], Loss: 16993.0488\n",
            "Epoch [42/100], Step [401/922], Loss: 17417.5762\n",
            "Epoch [42/100], Step [501/922], Loss: 15826.4072\n",
            "Epoch [42/100], Step [601/922], Loss: 16832.4121\n",
            "Epoch [42/100], Step [701/922], Loss: 17750.7012\n",
            "Epoch [42/100], Step [801/922], Loss: 18159.2285\n",
            "Epoch [42/100], Step [901/922], Loss: 16782.1562\n",
            "Epoch [43/100], Step [1/922], Loss: 16422.9609\n",
            "Epoch [43/100], Step [101/922], Loss: 17016.3398\n",
            "Epoch [43/100], Step [201/922], Loss: 16737.8105\n",
            "Epoch [43/100], Step [301/922], Loss: 16754.1406\n",
            "Epoch [43/100], Step [401/922], Loss: 16465.4062\n",
            "Epoch [43/100], Step [501/922], Loss: 17239.7852\n",
            "Epoch [43/100], Step [601/922], Loss: 17304.6191\n",
            "Epoch [43/100], Step [701/922], Loss: 17095.5039\n",
            "Epoch [43/100], Step [801/922], Loss: 17964.7227\n",
            "Epoch [43/100], Step [901/922], Loss: 17478.6543\n",
            "Epoch [44/100], Step [1/922], Loss: 17262.0527\n",
            "Epoch [44/100], Step [101/922], Loss: 15661.8975\n",
            "Epoch [44/100], Step [201/922], Loss: 16089.6582\n",
            "Epoch [44/100], Step [301/922], Loss: 16478.2891\n",
            "Epoch [44/100], Step [401/922], Loss: 18145.5762\n",
            "Epoch [44/100], Step [501/922], Loss: 17918.7617\n",
            "Epoch [44/100], Step [601/922], Loss: 16333.6729\n",
            "Epoch [44/100], Step [701/922], Loss: 16233.6523\n",
            "Epoch [44/100], Step [801/922], Loss: 17450.4902\n",
            "Epoch [44/100], Step [901/922], Loss: 17622.7793\n",
            "Epoch [45/100], Step [1/922], Loss: 15917.7744\n",
            "Epoch [45/100], Step [101/922], Loss: 16281.9004\n",
            "Epoch [45/100], Step [201/922], Loss: 17142.9922\n",
            "Epoch [45/100], Step [301/922], Loss: 16368.1250\n",
            "Epoch [45/100], Step [401/922], Loss: 17023.6211\n",
            "Epoch [45/100], Step [501/922], Loss: 16354.5557\n",
            "Epoch [45/100], Step [601/922], Loss: 17030.1152\n",
            "Epoch [45/100], Step [701/922], Loss: 17118.5273\n",
            "Epoch [45/100], Step [801/922], Loss: 16694.6934\n",
            "Epoch [45/100], Step [901/922], Loss: 16783.5547\n",
            "Epoch [46/100], Step [1/922], Loss: 17722.1328\n",
            "Epoch [46/100], Step [101/922], Loss: 17468.2168\n",
            "Epoch [46/100], Step [201/922], Loss: 17584.4512\n",
            "Epoch [46/100], Step [301/922], Loss: 17279.7871\n",
            "Epoch [46/100], Step [401/922], Loss: 16339.6328\n",
            "Epoch [46/100], Step [501/922], Loss: 17189.1934\n",
            "Epoch [46/100], Step [601/922], Loss: 17148.5645\n",
            "Epoch [46/100], Step [701/922], Loss: 17141.8652\n",
            "Epoch [46/100], Step [801/922], Loss: 16672.5859\n",
            "Epoch [46/100], Step [901/922], Loss: 17167.1797\n",
            "Epoch [47/100], Step [1/922], Loss: 17041.1387\n",
            "Epoch [47/100], Step [101/922], Loss: 16927.4473\n",
            "Epoch [47/100], Step [201/922], Loss: 16210.8555\n",
            "Epoch [47/100], Step [301/922], Loss: 16396.2871\n",
            "Epoch [47/100], Step [401/922], Loss: 17192.6152\n",
            "Epoch [47/100], Step [501/922], Loss: 16746.7871\n",
            "Epoch [47/100], Step [601/922], Loss: 17022.5410\n",
            "Epoch [47/100], Step [701/922], Loss: 17795.0371\n",
            "Epoch [47/100], Step [801/922], Loss: 16964.6914\n",
            "Epoch [47/100], Step [901/922], Loss: 17764.2773\n",
            "Epoch [48/100], Step [1/922], Loss: 17168.6523\n",
            "Epoch [48/100], Step [101/922], Loss: 16413.6367\n",
            "Epoch [48/100], Step [201/922], Loss: 17007.1289\n",
            "Epoch [48/100], Step [301/922], Loss: 15501.3516\n",
            "Epoch [48/100], Step [401/922], Loss: 15864.7646\n",
            "Epoch [48/100], Step [501/922], Loss: 17251.8398\n",
            "Epoch [48/100], Step [601/922], Loss: 17178.3164\n",
            "Epoch [48/100], Step [701/922], Loss: 16876.7344\n",
            "Epoch [48/100], Step [801/922], Loss: 17103.8379\n",
            "Epoch [48/100], Step [901/922], Loss: 17366.7500\n",
            "Epoch [49/100], Step [1/922], Loss: 17850.6641\n",
            "Epoch [49/100], Step [101/922], Loss: 16665.5703\n",
            "Epoch [49/100], Step [201/922], Loss: 16432.7188\n",
            "Epoch [49/100], Step [301/922], Loss: 17041.8164\n",
            "Epoch [49/100], Step [401/922], Loss: 17565.9102\n",
            "Epoch [49/100], Step [501/922], Loss: 17279.0234\n",
            "Epoch [49/100], Step [601/922], Loss: 17912.0742\n",
            "Epoch [49/100], Step [701/922], Loss: 16764.4453\n",
            "Epoch [49/100], Step [801/922], Loss: 16961.4863\n",
            "Epoch [49/100], Step [901/922], Loss: 17436.5410\n",
            "Epoch [50/100], Step [1/922], Loss: 17022.4648\n",
            "Epoch [50/100], Step [101/922], Loss: 16342.1162\n",
            "Epoch [50/100], Step [201/922], Loss: 17739.5176\n",
            "Epoch [50/100], Step [301/922], Loss: 16825.2148\n",
            "Epoch [50/100], Step [401/922], Loss: 17356.9707\n",
            "Epoch [50/100], Step [501/922], Loss: 16851.7773\n",
            "Epoch [50/100], Step [601/922], Loss: 16622.7676\n",
            "Epoch [50/100], Step [701/922], Loss: 16795.8184\n",
            "Epoch [50/100], Step [801/922], Loss: 17739.2930\n",
            "Epoch [50/100], Step [901/922], Loss: 16884.9922\n",
            "Epoch [51/100], Step [1/922], Loss: 16531.0273\n",
            "Epoch [51/100], Step [101/922], Loss: 15832.8662\n",
            "Epoch [51/100], Step [201/922], Loss: 16851.9180\n",
            "Epoch [51/100], Step [301/922], Loss: 16882.5703\n",
            "Epoch [51/100], Step [401/922], Loss: 16302.6445\n",
            "Epoch [51/100], Step [501/922], Loss: 16213.8486\n",
            "Epoch [51/100], Step [601/922], Loss: 16675.1172\n",
            "Epoch [51/100], Step [701/922], Loss: 16540.3984\n",
            "Epoch [51/100], Step [801/922], Loss: 17236.5840\n",
            "Epoch [51/100], Step [901/922], Loss: 16869.9668\n",
            "Epoch [52/100], Step [1/922], Loss: 16971.0312\n",
            "Epoch [52/100], Step [101/922], Loss: 16211.7920\n",
            "Epoch [52/100], Step [201/922], Loss: 18435.5293\n",
            "Epoch [52/100], Step [301/922], Loss: 17230.8184\n",
            "Epoch [52/100], Step [401/922], Loss: 16625.2988\n",
            "Epoch [52/100], Step [501/922], Loss: 17362.6953\n",
            "Epoch [52/100], Step [601/922], Loss: 17151.0977\n",
            "Epoch [52/100], Step [701/922], Loss: 17862.1855\n",
            "Epoch [52/100], Step [801/922], Loss: 18118.5879\n",
            "Epoch [52/100], Step [901/922], Loss: 16732.3730\n",
            "Epoch [53/100], Step [1/922], Loss: 17871.7969\n",
            "Epoch [53/100], Step [101/922], Loss: 17664.6504\n",
            "Epoch [53/100], Step [201/922], Loss: 16461.4004\n",
            "Epoch [53/100], Step [301/922], Loss: 17372.3672\n",
            "Epoch [53/100], Step [401/922], Loss: 15513.5596\n",
            "Epoch [53/100], Step [501/922], Loss: 15931.4375\n",
            "Epoch [53/100], Step [601/922], Loss: 16790.0195\n",
            "Epoch [53/100], Step [701/922], Loss: 16928.8496\n",
            "Epoch [53/100], Step [801/922], Loss: 17497.8203\n",
            "Epoch [53/100], Step [901/922], Loss: 15649.4238\n",
            "Epoch [54/100], Step [1/922], Loss: 17191.4316\n",
            "Epoch [54/100], Step [101/922], Loss: 17817.5938\n",
            "Epoch [54/100], Step [201/922], Loss: 17429.8984\n",
            "Epoch [54/100], Step [301/922], Loss: 17092.8125\n",
            "Epoch [54/100], Step [401/922], Loss: 16941.3945\n",
            "Epoch [54/100], Step [501/922], Loss: 17396.6152\n",
            "Epoch [54/100], Step [601/922], Loss: 16659.2148\n",
            "Epoch [54/100], Step [701/922], Loss: 17807.2207\n",
            "Epoch [54/100], Step [801/922], Loss: 17373.1211\n",
            "Epoch [54/100], Step [901/922], Loss: 16987.8008\n",
            "Epoch [55/100], Step [1/922], Loss: 17110.5215\n",
            "Epoch [55/100], Step [101/922], Loss: 16174.8945\n",
            "Epoch [55/100], Step [201/922], Loss: 16751.3457\n",
            "Epoch [55/100], Step [301/922], Loss: 17879.0371\n",
            "Epoch [55/100], Step [401/922], Loss: 17251.9863\n",
            "Epoch [55/100], Step [501/922], Loss: 17323.0215\n",
            "Epoch [55/100], Step [601/922], Loss: 16966.4941\n",
            "Epoch [55/100], Step [701/922], Loss: 17313.8750\n",
            "Epoch [55/100], Step [801/922], Loss: 16659.8262\n",
            "Epoch [55/100], Step [901/922], Loss: 17622.3496\n",
            "Epoch [56/100], Step [1/922], Loss: 16681.4746\n",
            "Epoch [56/100], Step [101/922], Loss: 15539.7832\n",
            "Epoch [56/100], Step [201/922], Loss: 16988.6875\n",
            "Epoch [56/100], Step [301/922], Loss: 18053.9258\n",
            "Epoch [56/100], Step [401/922], Loss: 17061.0977\n",
            "Epoch [56/100], Step [501/922], Loss: 17580.7012\n",
            "Epoch [56/100], Step [601/922], Loss: 17505.6172\n",
            "Epoch [56/100], Step [701/922], Loss: 16787.3984\n",
            "Epoch [56/100], Step [801/922], Loss: 17215.2773\n",
            "Epoch [56/100], Step [901/922], Loss: 17025.0859\n",
            "Epoch [57/100], Step [1/922], Loss: 16673.2695\n",
            "Epoch [57/100], Step [101/922], Loss: 16386.3125\n",
            "Epoch [57/100], Step [201/922], Loss: 16186.4824\n",
            "Epoch [57/100], Step [301/922], Loss: 18594.3008\n",
            "Epoch [57/100], Step [401/922], Loss: 16748.9551\n",
            "Epoch [57/100], Step [501/922], Loss: 17466.4180\n",
            "Epoch [57/100], Step [601/922], Loss: 16902.6621\n",
            "Epoch [57/100], Step [701/922], Loss: 15958.0801\n",
            "Epoch [57/100], Step [801/922], Loss: 17326.5957\n",
            "Epoch [57/100], Step [901/922], Loss: 17023.9434\n",
            "Epoch [58/100], Step [1/922], Loss: 17318.3516\n",
            "Epoch [58/100], Step [101/922], Loss: 17228.8867\n",
            "Epoch [58/100], Step [201/922], Loss: 17788.8203\n",
            "Epoch [58/100], Step [301/922], Loss: 17582.4785\n",
            "Epoch [58/100], Step [401/922], Loss: 15412.1348\n",
            "Epoch [58/100], Step [501/922], Loss: 17894.7383\n",
            "Epoch [58/100], Step [601/922], Loss: 17169.9473\n",
            "Epoch [58/100], Step [701/922], Loss: 16995.2402\n",
            "Epoch [58/100], Step [801/922], Loss: 16951.1250\n",
            "Epoch [58/100], Step [901/922], Loss: 17036.6406\n",
            "Epoch [59/100], Step [1/922], Loss: 17927.3340\n",
            "Epoch [59/100], Step [101/922], Loss: 17242.2891\n",
            "Epoch [59/100], Step [201/922], Loss: 17276.6328\n",
            "Epoch [59/100], Step [301/922], Loss: 16913.7305\n",
            "Epoch [59/100], Step [401/922], Loss: 17358.5430\n",
            "Epoch [59/100], Step [501/922], Loss: 17097.7207\n",
            "Epoch [59/100], Step [601/922], Loss: 17718.8809\n",
            "Epoch [59/100], Step [701/922], Loss: 16731.6289\n",
            "Epoch [59/100], Step [801/922], Loss: 17019.2988\n",
            "Epoch [59/100], Step [901/922], Loss: 15915.2500\n",
            "Epoch [60/100], Step [1/922], Loss: 17135.5039\n",
            "Epoch [60/100], Step [101/922], Loss: 17134.6191\n",
            "Epoch [60/100], Step [201/922], Loss: 15928.6982\n",
            "Epoch [60/100], Step [301/922], Loss: 17533.8223\n",
            "Epoch [60/100], Step [401/922], Loss: 17336.8281\n",
            "Epoch [60/100], Step [501/922], Loss: 17461.5234\n",
            "Epoch [60/100], Step [601/922], Loss: 18152.5254\n",
            "Epoch [60/100], Step [701/922], Loss: 17424.5156\n",
            "Epoch [60/100], Step [801/922], Loss: 17008.4062\n",
            "Epoch [60/100], Step [901/922], Loss: 16956.2305\n",
            "Epoch [61/100], Step [1/922], Loss: 17724.5039\n",
            "Epoch [61/100], Step [101/922], Loss: 17537.9941\n",
            "Epoch [61/100], Step [201/922], Loss: 18768.7363\n",
            "Epoch [61/100], Step [301/922], Loss: 17076.4434\n",
            "Epoch [61/100], Step [401/922], Loss: 17077.9238\n",
            "Epoch [61/100], Step [501/922], Loss: 17521.9668\n",
            "Epoch [61/100], Step [601/922], Loss: 16393.2559\n",
            "Epoch [61/100], Step [701/922], Loss: 17497.4453\n",
            "Epoch [61/100], Step [801/922], Loss: 17057.8008\n",
            "Epoch [61/100], Step [901/922], Loss: 16672.2754\n",
            "Epoch [62/100], Step [1/922], Loss: 16953.3086\n",
            "Epoch [62/100], Step [101/922], Loss: 15870.6406\n",
            "Epoch [62/100], Step [201/922], Loss: 17660.4609\n",
            "Epoch [62/100], Step [301/922], Loss: 17932.4336\n",
            "Epoch [62/100], Step [401/922], Loss: 16676.5547\n",
            "Epoch [62/100], Step [501/922], Loss: 16753.7949\n",
            "Epoch [62/100], Step [601/922], Loss: 17854.2383\n",
            "Epoch [62/100], Step [701/922], Loss: 16175.2090\n",
            "Epoch [62/100], Step [801/922], Loss: 17349.1055\n",
            "Epoch [62/100], Step [901/922], Loss: 17496.7539\n",
            "Epoch [63/100], Step [1/922], Loss: 17318.9062\n",
            "Epoch [63/100], Step [101/922], Loss: 17742.6543\n",
            "Epoch [63/100], Step [201/922], Loss: 17613.8945\n",
            "Epoch [63/100], Step [301/922], Loss: 16700.5020\n",
            "Epoch [63/100], Step [401/922], Loss: 17388.4141\n",
            "Epoch [63/100], Step [501/922], Loss: 16742.3027\n",
            "Epoch [63/100], Step [601/922], Loss: 18198.9551\n",
            "Epoch [63/100], Step [701/922], Loss: 16626.6816\n",
            "Epoch [63/100], Step [801/922], Loss: 18558.2070\n",
            "Epoch [63/100], Step [901/922], Loss: 16835.6016\n",
            "Epoch [64/100], Step [1/922], Loss: 17402.2656\n",
            "Epoch [64/100], Step [101/922], Loss: 16798.5625\n",
            "Epoch [64/100], Step [201/922], Loss: 17064.5098\n",
            "Epoch [64/100], Step [301/922], Loss: 16351.0537\n",
            "Epoch [64/100], Step [401/922], Loss: 16667.5254\n",
            "Epoch [64/100], Step [501/922], Loss: 15518.3594\n",
            "Epoch [64/100], Step [601/922], Loss: 17434.3359\n",
            "Epoch [64/100], Step [701/922], Loss: 17025.9805\n",
            "Epoch [64/100], Step [801/922], Loss: 16711.3613\n",
            "Epoch [64/100], Step [901/922], Loss: 16793.7012\n",
            "Epoch [65/100], Step [1/922], Loss: 16813.7773\n",
            "Epoch [65/100], Step [101/922], Loss: 16958.9121\n",
            "Epoch [65/100], Step [201/922], Loss: 16984.9824\n",
            "Epoch [65/100], Step [301/922], Loss: 16616.2891\n",
            "Epoch [65/100], Step [401/922], Loss: 16681.2891\n",
            "Epoch [65/100], Step [501/922], Loss: 17567.8320\n",
            "Epoch [65/100], Step [601/922], Loss: 17392.9297\n",
            "Epoch [65/100], Step [701/922], Loss: 17656.6914\n",
            "Epoch [65/100], Step [801/922], Loss: 17348.0859\n",
            "Epoch [65/100], Step [901/922], Loss: 17579.4883\n",
            "Epoch [66/100], Step [1/922], Loss: 17118.5195\n",
            "Epoch [66/100], Step [101/922], Loss: 17548.9961\n",
            "Epoch [66/100], Step [201/922], Loss: 16579.2891\n",
            "Epoch [66/100], Step [301/922], Loss: 18101.2422\n",
            "Epoch [66/100], Step [401/922], Loss: 17809.8457\n",
            "Epoch [66/100], Step [501/922], Loss: 17725.6172\n",
            "Epoch [66/100], Step [601/922], Loss: 17486.3633\n",
            "Epoch [66/100], Step [701/922], Loss: 17307.8555\n",
            "Epoch [66/100], Step [801/922], Loss: 15518.8477\n",
            "Epoch [66/100], Step [901/922], Loss: 16975.3398\n",
            "Epoch [67/100], Step [1/922], Loss: 18096.1523\n",
            "Epoch [67/100], Step [101/922], Loss: 16849.4785\n",
            "Epoch [67/100], Step [201/922], Loss: 18202.1367\n",
            "Epoch [67/100], Step [301/922], Loss: 16859.5449\n",
            "Epoch [67/100], Step [401/922], Loss: 17468.9629\n",
            "Epoch [67/100], Step [501/922], Loss: 16864.8906\n",
            "Epoch [67/100], Step [601/922], Loss: 18758.2891\n",
            "Epoch [67/100], Step [701/922], Loss: 17677.6484\n",
            "Epoch [67/100], Step [801/922], Loss: 17908.0527\n",
            "Epoch [67/100], Step [901/922], Loss: 17155.0391\n",
            "Epoch [68/100], Step [1/922], Loss: 17237.3359\n",
            "Epoch [68/100], Step [101/922], Loss: 16467.2773\n",
            "Epoch [68/100], Step [201/922], Loss: 16372.7168\n",
            "Epoch [68/100], Step [301/922], Loss: 17947.1543\n",
            "Epoch [68/100], Step [401/922], Loss: 17215.6855\n",
            "Epoch [68/100], Step [501/922], Loss: 17589.3535\n",
            "Epoch [68/100], Step [601/922], Loss: 17727.0742\n",
            "Epoch [68/100], Step [701/922], Loss: 16057.9160\n",
            "Epoch [68/100], Step [801/922], Loss: 16592.7500\n",
            "Epoch [68/100], Step [901/922], Loss: 16398.7930\n",
            "Epoch [69/100], Step [1/922], Loss: 17091.8438\n",
            "Epoch [69/100], Step [101/922], Loss: 16417.4746\n",
            "Epoch [69/100], Step [201/922], Loss: 17168.3652\n",
            "Epoch [69/100], Step [301/922], Loss: 16982.1777\n",
            "Epoch [69/100], Step [401/922], Loss: 16968.3672\n",
            "Epoch [69/100], Step [501/922], Loss: 16820.8262\n",
            "Epoch [69/100], Step [601/922], Loss: 17338.9160\n",
            "Epoch [69/100], Step [701/922], Loss: 17539.2070\n",
            "Epoch [69/100], Step [801/922], Loss: 17616.3945\n",
            "Epoch [69/100], Step [901/922], Loss: 17243.5840\n",
            "Epoch [70/100], Step [1/922], Loss: 17026.0312\n",
            "Epoch [70/100], Step [101/922], Loss: 17869.4258\n",
            "Epoch [70/100], Step [201/922], Loss: 16424.2031\n",
            "Epoch [70/100], Step [301/922], Loss: 16297.2520\n",
            "Epoch [70/100], Step [401/922], Loss: 17623.2305\n",
            "Epoch [70/100], Step [501/922], Loss: 17900.8398\n",
            "Epoch [70/100], Step [601/922], Loss: 15357.2178\n",
            "Epoch [70/100], Step [701/922], Loss: 16808.6523\n",
            "Epoch [70/100], Step [801/922], Loss: 18226.7129\n",
            "Epoch [70/100], Step [901/922], Loss: 17083.4238\n",
            "Epoch [71/100], Step [1/922], Loss: 17431.8828\n",
            "Epoch [71/100], Step [101/922], Loss: 18124.1836\n",
            "Epoch [71/100], Step [201/922], Loss: 18298.9746\n",
            "Epoch [71/100], Step [301/922], Loss: 16019.3916\n",
            "Epoch [71/100], Step [401/922], Loss: 17606.0195\n",
            "Epoch [71/100], Step [501/922], Loss: 17370.7559\n",
            "Epoch [71/100], Step [601/922], Loss: 17016.1992\n",
            "Epoch [71/100], Step [701/922], Loss: 17039.1113\n",
            "Epoch [71/100], Step [801/922], Loss: 16234.7705\n",
            "Epoch [71/100], Step [901/922], Loss: 18004.7520\n",
            "Epoch [72/100], Step [1/922], Loss: 15950.1211\n",
            "Epoch [72/100], Step [101/922], Loss: 16769.4199\n",
            "Epoch [72/100], Step [201/922], Loss: 17399.6855\n",
            "Epoch [72/100], Step [301/922], Loss: 17138.1230\n",
            "Epoch [72/100], Step [401/922], Loss: 16446.9473\n",
            "Epoch [72/100], Step [501/922], Loss: 16448.2324\n",
            "Epoch [72/100], Step [601/922], Loss: 16816.6406\n",
            "Epoch [72/100], Step [701/922], Loss: 16687.0488\n",
            "Epoch [72/100], Step [801/922], Loss: 15996.8770\n",
            "Epoch [72/100], Step [901/922], Loss: 16146.9941\n",
            "Epoch [73/100], Step [1/922], Loss: 17571.4219\n",
            "Epoch [73/100], Step [101/922], Loss: 17092.8613\n",
            "Epoch [73/100], Step [201/922], Loss: 17938.8750\n",
            "Epoch [73/100], Step [301/922], Loss: 17321.9219\n",
            "Epoch [73/100], Step [401/922], Loss: 16665.7676\n",
            "Epoch [73/100], Step [501/922], Loss: 16629.5918\n",
            "Epoch [73/100], Step [601/922], Loss: 16234.2100\n",
            "Epoch [73/100], Step [701/922], Loss: 17285.6328\n",
            "Epoch [73/100], Step [801/922], Loss: 16876.6797\n",
            "Epoch [73/100], Step [901/922], Loss: 16112.7451\n",
            "Epoch [74/100], Step [1/922], Loss: 16280.5088\n",
            "Epoch [74/100], Step [101/922], Loss: 16767.5234\n",
            "Epoch [74/100], Step [201/922], Loss: 17619.2871\n",
            "Epoch [74/100], Step [301/922], Loss: 16466.8711\n",
            "Epoch [74/100], Step [401/922], Loss: 16638.8145\n",
            "Epoch [74/100], Step [501/922], Loss: 18004.2109\n",
            "Epoch [74/100], Step [601/922], Loss: 17944.1680\n",
            "Epoch [74/100], Step [701/922], Loss: 16423.5625\n",
            "Epoch [74/100], Step [801/922], Loss: 16387.5176\n",
            "Epoch [74/100], Step [901/922], Loss: 15360.1445\n",
            "Epoch [75/100], Step [1/922], Loss: 17813.0312\n",
            "Epoch [75/100], Step [101/922], Loss: 15942.0283\n",
            "Epoch [75/100], Step [201/922], Loss: 15796.3193\n",
            "Epoch [75/100], Step [301/922], Loss: 16824.1660\n",
            "Epoch [75/100], Step [401/922], Loss: 16718.7910\n",
            "Epoch [75/100], Step [501/922], Loss: 17270.4922\n",
            "Epoch [75/100], Step [601/922], Loss: 17548.5137\n",
            "Epoch [75/100], Step [701/922], Loss: 17599.5039\n",
            "Epoch [75/100], Step [801/922], Loss: 16224.5332\n",
            "Epoch [75/100], Step [901/922], Loss: 17296.9297\n",
            "Epoch [76/100], Step [1/922], Loss: 16713.7109\n",
            "Epoch [76/100], Step [101/922], Loss: 17480.4531\n",
            "Epoch [76/100], Step [201/922], Loss: 17036.1230\n",
            "Epoch [76/100], Step [301/922], Loss: 16414.2422\n",
            "Epoch [76/100], Step [401/922], Loss: 17430.5117\n",
            "Epoch [76/100], Step [501/922], Loss: 16465.4512\n",
            "Epoch [76/100], Step [601/922], Loss: 16464.9941\n",
            "Epoch [76/100], Step [701/922], Loss: 17378.0684\n",
            "Epoch [76/100], Step [801/922], Loss: 17205.6543\n",
            "Epoch [76/100], Step [901/922], Loss: 17557.5801\n",
            "Epoch [77/100], Step [1/922], Loss: 16868.8301\n",
            "Epoch [77/100], Step [101/922], Loss: 17553.8223\n",
            "Epoch [77/100], Step [201/922], Loss: 17504.3164\n",
            "Epoch [77/100], Step [301/922], Loss: 17476.7070\n",
            "Epoch [77/100], Step [401/922], Loss: 16745.7969\n",
            "Epoch [77/100], Step [501/922], Loss: 17229.4102\n",
            "Epoch [77/100], Step [601/922], Loss: 16253.8613\n",
            "Epoch [77/100], Step [701/922], Loss: 16602.8574\n",
            "Epoch [77/100], Step [801/922], Loss: 16783.9102\n",
            "Epoch [77/100], Step [901/922], Loss: 16543.0039\n",
            "Epoch [78/100], Step [1/922], Loss: 15882.5947\n",
            "Epoch [78/100], Step [101/922], Loss: 16548.7812\n",
            "Epoch [78/100], Step [201/922], Loss: 17393.6172\n",
            "Epoch [78/100], Step [301/922], Loss: 17328.3320\n",
            "Epoch [78/100], Step [401/922], Loss: 17647.5352\n",
            "Epoch [78/100], Step [501/922], Loss: 16675.4824\n",
            "Epoch [78/100], Step [601/922], Loss: 16783.2539\n",
            "Epoch [78/100], Step [701/922], Loss: 16068.8174\n",
            "Epoch [78/100], Step [801/922], Loss: 16444.7461\n",
            "Epoch [78/100], Step [901/922], Loss: 17921.6953\n",
            "Epoch [79/100], Step [1/922], Loss: 16803.7129\n",
            "Epoch [79/100], Step [101/922], Loss: 16638.1328\n",
            "Epoch [79/100], Step [201/922], Loss: 17036.2207\n",
            "Epoch [79/100], Step [301/922], Loss: 16335.0459\n",
            "Epoch [79/100], Step [401/922], Loss: 17674.8457\n",
            "Epoch [79/100], Step [501/922], Loss: 16763.7324\n",
            "Epoch [79/100], Step [601/922], Loss: 16779.7871\n",
            "Epoch [79/100], Step [701/922], Loss: 16745.5566\n",
            "Epoch [79/100], Step [801/922], Loss: 16417.4434\n",
            "Epoch [79/100], Step [901/922], Loss: 16170.5078\n",
            "Epoch [80/100], Step [1/922], Loss: 17673.0488\n",
            "Epoch [80/100], Step [101/922], Loss: 16936.2812\n",
            "Epoch [80/100], Step [201/922], Loss: 16526.4434\n",
            "Epoch [80/100], Step [301/922], Loss: 16071.3027\n",
            "Epoch [80/100], Step [401/922], Loss: 16930.8594\n",
            "Epoch [80/100], Step [501/922], Loss: 16143.3135\n",
            "Epoch [80/100], Step [601/922], Loss: 17873.8086\n",
            "Epoch [80/100], Step [701/922], Loss: 16810.7852\n",
            "Epoch [80/100], Step [801/922], Loss: 17214.7168\n",
            "Epoch [80/100], Step [901/922], Loss: 17959.8145\n",
            "Epoch [81/100], Step [1/922], Loss: 16477.0898\n",
            "Epoch [81/100], Step [101/922], Loss: 17329.2012\n",
            "Epoch [81/100], Step [201/922], Loss: 16306.9961\n",
            "Epoch [81/100], Step [301/922], Loss: 16902.3789\n",
            "Epoch [81/100], Step [401/922], Loss: 17052.6230\n",
            "Epoch [81/100], Step [501/922], Loss: 16393.5762\n",
            "Epoch [81/100], Step [601/922], Loss: 16339.0752\n",
            "Epoch [81/100], Step [701/922], Loss: 17461.6250\n",
            "Epoch [81/100], Step [801/922], Loss: 18341.3105\n",
            "Epoch [81/100], Step [901/922], Loss: 17537.4707\n",
            "Epoch [82/100], Step [1/922], Loss: 16243.7031\n",
            "Epoch [82/100], Step [101/922], Loss: 18660.4297\n",
            "Epoch [82/100], Step [201/922], Loss: 16463.9414\n",
            "Epoch [82/100], Step [301/922], Loss: 16505.2500\n",
            "Epoch [82/100], Step [401/922], Loss: 17167.5469\n",
            "Epoch [82/100], Step [501/922], Loss: 15740.7979\n",
            "Epoch [82/100], Step [601/922], Loss: 16940.4414\n",
            "Epoch [82/100], Step [701/922], Loss: 16489.6543\n",
            "Epoch [82/100], Step [801/922], Loss: 17608.8730\n",
            "Epoch [82/100], Step [901/922], Loss: 17192.3867\n",
            "Epoch [83/100], Step [1/922], Loss: 16359.0059\n",
            "Epoch [83/100], Step [101/922], Loss: 17367.2422\n",
            "Epoch [83/100], Step [201/922], Loss: 16690.1465\n",
            "Epoch [83/100], Step [301/922], Loss: 15876.8789\n",
            "Epoch [83/100], Step [401/922], Loss: 16524.6816\n",
            "Epoch [83/100], Step [501/922], Loss: 18347.1641\n",
            "Epoch [83/100], Step [601/922], Loss: 16426.6562\n",
            "Epoch [83/100], Step [701/922], Loss: 17126.3770\n",
            "Epoch [83/100], Step [801/922], Loss: 18304.8770\n",
            "Epoch [83/100], Step [901/922], Loss: 17452.1680\n",
            "Epoch [84/100], Step [1/922], Loss: 15794.8486\n",
            "Epoch [84/100], Step [101/922], Loss: 16691.2422\n",
            "Epoch [84/100], Step [201/922], Loss: 17168.8008\n",
            "Epoch [84/100], Step [301/922], Loss: 16798.1582\n",
            "Epoch [84/100], Step [401/922], Loss: 16421.4277\n",
            "Epoch [84/100], Step [501/922], Loss: 18000.1523\n",
            "Epoch [84/100], Step [601/922], Loss: 16822.3750\n",
            "Epoch [84/100], Step [701/922], Loss: 15974.8555\n",
            "Epoch [84/100], Step [801/922], Loss: 17122.9043\n",
            "Epoch [84/100], Step [901/922], Loss: 17907.4141\n",
            "Epoch [85/100], Step [1/922], Loss: 16532.6211\n",
            "Epoch [85/100], Step [101/922], Loss: 16287.9082\n",
            "Epoch [85/100], Step [201/922], Loss: 16874.5156\n",
            "Epoch [85/100], Step [301/922], Loss: 15860.4229\n",
            "Epoch [85/100], Step [401/922], Loss: 17537.8809\n",
            "Epoch [85/100], Step [501/922], Loss: 16720.8789\n",
            "Epoch [85/100], Step [601/922], Loss: 16037.2383\n",
            "Epoch [85/100], Step [701/922], Loss: 17069.6289\n",
            "Epoch [85/100], Step [801/922], Loss: 17055.8203\n",
            "Epoch [85/100], Step [901/922], Loss: 17763.7070\n",
            "Epoch [86/100], Step [1/922], Loss: 16892.7227\n",
            "Epoch [86/100], Step [101/922], Loss: 17240.7188\n",
            "Epoch [86/100], Step [201/922], Loss: 15955.2607\n",
            "Epoch [86/100], Step [301/922], Loss: 17085.2324\n",
            "Epoch [86/100], Step [401/922], Loss: 16813.8867\n",
            "Epoch [86/100], Step [501/922], Loss: 16120.1377\n",
            "Epoch [86/100], Step [601/922], Loss: 16698.9492\n",
            "Epoch [86/100], Step [701/922], Loss: 16581.2539\n",
            "Epoch [86/100], Step [801/922], Loss: 17742.9121\n",
            "Epoch [86/100], Step [901/922], Loss: 17639.3320\n",
            "Epoch [87/100], Step [1/922], Loss: 17100.5996\n",
            "Epoch [87/100], Step [101/922], Loss: 16328.1055\n",
            "Epoch [87/100], Step [201/922], Loss: 16862.1816\n",
            "Epoch [87/100], Step [301/922], Loss: 16185.3203\n",
            "Epoch [87/100], Step [401/922], Loss: 16739.9199\n",
            "Epoch [87/100], Step [501/922], Loss: 15882.6650\n",
            "Epoch [87/100], Step [601/922], Loss: 16095.8438\n",
            "Epoch [87/100], Step [701/922], Loss: 16880.6758\n",
            "Epoch [87/100], Step [801/922], Loss: 15914.1846\n",
            "Epoch [87/100], Step [901/922], Loss: 17465.8848\n",
            "Epoch [88/100], Step [1/922], Loss: 17798.4824\n",
            "Epoch [88/100], Step [101/922], Loss: 16762.6230\n",
            "Epoch [88/100], Step [201/922], Loss: 16775.3164\n",
            "Epoch [88/100], Step [301/922], Loss: 16946.6973\n",
            "Epoch [88/100], Step [401/922], Loss: 16048.5342\n",
            "Epoch [88/100], Step [501/922], Loss: 16716.5781\n",
            "Epoch [88/100], Step [601/922], Loss: 16917.7500\n",
            "Epoch [88/100], Step [701/922], Loss: 17819.7148\n",
            "Epoch [88/100], Step [801/922], Loss: 16309.9707\n",
            "Epoch [88/100], Step [901/922], Loss: 16011.0254\n",
            "Epoch [89/100], Step [1/922], Loss: 16940.9316\n",
            "Epoch [89/100], Step [101/922], Loss: 17670.9316\n",
            "Epoch [89/100], Step [201/922], Loss: 17177.4805\n",
            "Epoch [89/100], Step [301/922], Loss: 17390.5547\n",
            "Epoch [89/100], Step [401/922], Loss: 15402.7695\n",
            "Epoch [89/100], Step [501/922], Loss: 16929.9941\n",
            "Epoch [89/100], Step [601/922], Loss: 17285.4512\n",
            "Epoch [89/100], Step [701/922], Loss: 16810.6406\n",
            "Epoch [89/100], Step [801/922], Loss: 17735.5137\n",
            "Epoch [89/100], Step [901/922], Loss: 17107.6055\n",
            "Epoch [90/100], Step [1/922], Loss: 16773.4922\n",
            "Epoch [90/100], Step [101/922], Loss: 16903.6680\n",
            "Epoch [90/100], Step [201/922], Loss: 17197.5039\n",
            "Epoch [90/100], Step [301/922], Loss: 17095.0410\n",
            "Epoch [90/100], Step [401/922], Loss: 16807.0996\n",
            "Epoch [90/100], Step [501/922], Loss: 16586.6875\n",
            "Epoch [90/100], Step [601/922], Loss: 17230.4316\n",
            "Epoch [90/100], Step [701/922], Loss: 16384.8867\n",
            "Epoch [90/100], Step [801/922], Loss: 16143.7773\n",
            "Epoch [90/100], Step [901/922], Loss: 16017.2744\n",
            "Epoch [91/100], Step [1/922], Loss: 17203.8945\n",
            "Epoch [91/100], Step [101/922], Loss: 16427.2617\n",
            "Epoch [91/100], Step [201/922], Loss: 17664.3457\n",
            "Epoch [91/100], Step [301/922], Loss: 17614.1465\n",
            "Epoch [91/100], Step [401/922], Loss: 16890.1562\n",
            "Epoch [91/100], Step [501/922], Loss: 16939.4980\n",
            "Epoch [91/100], Step [601/922], Loss: 17211.0195\n",
            "Epoch [91/100], Step [701/922], Loss: 17482.9141\n",
            "Epoch [91/100], Step [801/922], Loss: 16424.5781\n",
            "Epoch [91/100], Step [901/922], Loss: 17684.1328\n",
            "Epoch [92/100], Step [1/922], Loss: 16621.0020\n",
            "Epoch [92/100], Step [101/922], Loss: 16638.0762\n",
            "Epoch [92/100], Step [201/922], Loss: 16233.6611\n",
            "Epoch [92/100], Step [301/922], Loss: 16712.8809\n",
            "Epoch [92/100], Step [401/922], Loss: 16842.7734\n",
            "Epoch [92/100], Step [501/922], Loss: 16150.2412\n",
            "Epoch [92/100], Step [601/922], Loss: 17117.0195\n",
            "Epoch [92/100], Step [701/922], Loss: 17409.0410\n",
            "Epoch [92/100], Step [801/922], Loss: 16461.9492\n",
            "Epoch [92/100], Step [901/922], Loss: 16515.9609\n",
            "Epoch [93/100], Step [1/922], Loss: 15250.7451\n",
            "Epoch [93/100], Step [101/922], Loss: 16580.6387\n",
            "Epoch [93/100], Step [201/922], Loss: 17564.5176\n",
            "Epoch [93/100], Step [301/922], Loss: 17523.1953\n",
            "Epoch [93/100], Step [401/922], Loss: 16378.7969\n",
            "Epoch [93/100], Step [501/922], Loss: 17028.5059\n",
            "Epoch [93/100], Step [601/922], Loss: 16820.2402\n",
            "Epoch [93/100], Step [701/922], Loss: 17776.4727\n",
            "Epoch [93/100], Step [801/922], Loss: 16854.4551\n",
            "Epoch [93/100], Step [901/922], Loss: 17015.1641\n",
            "Epoch [94/100], Step [1/922], Loss: 16749.0957\n",
            "Epoch [94/100], Step [101/922], Loss: 16461.0938\n",
            "Epoch [94/100], Step [201/922], Loss: 17175.0781\n",
            "Epoch [94/100], Step [301/922], Loss: 16653.8574\n",
            "Epoch [94/100], Step [401/922], Loss: 17373.2852\n",
            "Epoch [94/100], Step [501/922], Loss: 17071.1230\n",
            "Epoch [94/100], Step [601/922], Loss: 16656.1172\n",
            "Epoch [94/100], Step [701/922], Loss: 16599.9395\n",
            "Epoch [94/100], Step [801/922], Loss: 17322.9336\n",
            "Epoch [94/100], Step [901/922], Loss: 15308.0352\n",
            "Epoch [95/100], Step [1/922], Loss: 17266.3457\n",
            "Epoch [95/100], Step [101/922], Loss: 17465.3105\n",
            "Epoch [95/100], Step [201/922], Loss: 16189.5215\n",
            "Epoch [95/100], Step [301/922], Loss: 16035.6514\n",
            "Epoch [95/100], Step [401/922], Loss: 16723.5488\n",
            "Epoch [95/100], Step [501/922], Loss: 16603.2812\n",
            "Epoch [95/100], Step [601/922], Loss: 17175.2051\n",
            "Epoch [95/100], Step [701/922], Loss: 17180.3516\n",
            "Epoch [95/100], Step [801/922], Loss: 16123.8340\n",
            "Epoch [95/100], Step [901/922], Loss: 16580.0039\n",
            "Epoch [96/100], Step [1/922], Loss: 16791.0918\n",
            "Epoch [96/100], Step [101/922], Loss: 15626.0488\n",
            "Epoch [96/100], Step [201/922], Loss: 16267.3398\n",
            "Epoch [96/100], Step [301/922], Loss: 17207.0215\n",
            "Epoch [96/100], Step [401/922], Loss: 15575.9307\n",
            "Epoch [96/100], Step [501/922], Loss: 17130.3086\n",
            "Epoch [96/100], Step [601/922], Loss: 16418.1289\n",
            "Epoch [96/100], Step [701/922], Loss: 17183.4199\n",
            "Epoch [96/100], Step [801/922], Loss: 16430.8438\n",
            "Epoch [96/100], Step [901/922], Loss: 17800.8398\n",
            "Epoch [97/100], Step [1/922], Loss: 15821.0830\n",
            "Epoch [97/100], Step [101/922], Loss: 17059.1191\n",
            "Epoch [97/100], Step [201/922], Loss: 16225.1758\n",
            "Epoch [97/100], Step [301/922], Loss: 17214.1777\n",
            "Epoch [97/100], Step [401/922], Loss: 16708.1582\n",
            "Epoch [97/100], Step [501/922], Loss: 17236.3438\n",
            "Epoch [97/100], Step [601/922], Loss: 17206.7812\n",
            "Epoch [97/100], Step [701/922], Loss: 16643.9316\n",
            "Epoch [97/100], Step [801/922], Loss: 16860.1680\n",
            "Epoch [97/100], Step [901/922], Loss: 16049.5684\n",
            "Epoch [98/100], Step [1/922], Loss: 16542.4551\n",
            "Epoch [98/100], Step [101/922], Loss: 17035.7480\n",
            "Epoch [98/100], Step [201/922], Loss: 17928.6113\n",
            "Epoch [98/100], Step [301/922], Loss: 16383.5791\n",
            "Epoch [98/100], Step [401/922], Loss: 17180.0879\n",
            "Epoch [98/100], Step [501/922], Loss: 16027.5537\n",
            "Epoch [98/100], Step [601/922], Loss: 16676.3574\n",
            "Epoch [98/100], Step [701/922], Loss: 17614.5840\n",
            "Epoch [98/100], Step [801/922], Loss: 15785.9180\n",
            "Epoch [98/100], Step [901/922], Loss: 17063.2832\n",
            "Epoch [99/100], Step [1/922], Loss: 17157.6816\n",
            "Epoch [99/100], Step [101/922], Loss: 17093.1230\n",
            "Epoch [99/100], Step [201/922], Loss: 16820.5371\n",
            "Epoch [99/100], Step [301/922], Loss: 16623.1426\n",
            "Epoch [99/100], Step [401/922], Loss: 16919.6289\n",
            "Epoch [99/100], Step [501/922], Loss: 18345.5664\n",
            "Epoch [99/100], Step [601/922], Loss: 16201.7441\n",
            "Epoch [99/100], Step [701/922], Loss: 17099.5781\n",
            "Epoch [99/100], Step [801/922], Loss: 17327.9590\n",
            "Epoch [99/100], Step [901/922], Loss: 16469.5215\n",
            "Epoch [100/100], Step [1/922], Loss: 16894.4355\n",
            "Epoch [100/100], Step [101/922], Loss: 16281.0303\n",
            "Epoch [100/100], Step [201/922], Loss: 16886.0332\n",
            "Epoch [100/100], Step [301/922], Loss: 15998.0156\n",
            "Epoch [100/100], Step [401/922], Loss: 16618.2168\n",
            "Epoch [100/100], Step [501/922], Loss: 16113.5322\n",
            "Epoch [100/100], Step [601/922], Loss: 16926.3652\n",
            "Epoch [100/100], Step [701/922], Loss: 16391.9375\n",
            "Epoch [100/100], Step [801/922], Loss: 16938.1660\n",
            "Epoch [100/100], Step [901/922], Loss: 16903.9922\n",
            "Accuracy of the SVM on the test images with 1000 labeled images: 78.93%\n",
            "Epoch [1/100], Step [1/891], Loss: 38258.9531\n",
            "Epoch [1/100], Step [101/891], Loss: 21324.2734\n",
            "Epoch [1/100], Step [201/891], Loss: 19783.1133\n",
            "Epoch [1/100], Step [301/891], Loss: 20430.6465\n",
            "Epoch [1/100], Step [401/891], Loss: 18992.7305\n",
            "Epoch [1/100], Step [501/891], Loss: 19326.5352\n",
            "Epoch [1/100], Step [601/891], Loss: 19735.3887\n",
            "Epoch [1/100], Step [701/891], Loss: 18287.8418\n",
            "Epoch [1/100], Step [801/891], Loss: 19252.0508\n",
            "Epoch [2/100], Step [1/891], Loss: 18925.6660\n",
            "Epoch [2/100], Step [101/891], Loss: 19430.5723\n",
            "Epoch [2/100], Step [201/891], Loss: 17138.7734\n",
            "Epoch [2/100], Step [301/891], Loss: 19067.1699\n",
            "Epoch [2/100], Step [401/891], Loss: 18502.3926\n",
            "Epoch [2/100], Step [501/891], Loss: 18257.4980\n",
            "Epoch [2/100], Step [601/891], Loss: 18323.6309\n",
            "Epoch [2/100], Step [701/891], Loss: 17897.9902\n",
            "Epoch [2/100], Step [801/891], Loss: 17419.8438\n",
            "Epoch [3/100], Step [1/891], Loss: 18215.1699\n",
            "Epoch [3/100], Step [101/891], Loss: 19857.4336\n",
            "Epoch [3/100], Step [201/891], Loss: 18502.8594\n",
            "Epoch [3/100], Step [301/891], Loss: 18061.9395\n",
            "Epoch [3/100], Step [401/891], Loss: 18931.7988\n",
            "Epoch [3/100], Step [501/891], Loss: 17260.9609\n",
            "Epoch [3/100], Step [601/891], Loss: 17978.7715\n",
            "Epoch [3/100], Step [701/891], Loss: 18509.8301\n",
            "Epoch [3/100], Step [801/891], Loss: 17575.1523\n",
            "Epoch [4/100], Step [1/891], Loss: 18213.8711\n",
            "Epoch [4/100], Step [101/891], Loss: 18090.7031\n",
            "Epoch [4/100], Step [201/891], Loss: 18620.6523\n",
            "Epoch [4/100], Step [301/891], Loss: 17659.2637\n",
            "Epoch [4/100], Step [401/891], Loss: 17809.5781\n",
            "Epoch [4/100], Step [501/891], Loss: 18463.9434\n",
            "Epoch [4/100], Step [601/891], Loss: 17875.5215\n",
            "Epoch [4/100], Step [701/891], Loss: 17617.5820\n",
            "Epoch [4/100], Step [801/891], Loss: 17650.5352\n",
            "Epoch [5/100], Step [1/891], Loss: 18454.6348\n",
            "Epoch [5/100], Step [101/891], Loss: 18568.2754\n",
            "Epoch [5/100], Step [201/891], Loss: 17673.2051\n",
            "Epoch [5/100], Step [301/891], Loss: 17708.2637\n",
            "Epoch [5/100], Step [401/891], Loss: 17348.5137\n",
            "Epoch [5/100], Step [501/891], Loss: 17079.2969\n",
            "Epoch [5/100], Step [601/891], Loss: 16784.9336\n",
            "Epoch [5/100], Step [701/891], Loss: 18476.2051\n",
            "Epoch [5/100], Step [801/891], Loss: 17659.8867\n",
            "Epoch [6/100], Step [1/891], Loss: 17604.6016\n",
            "Epoch [6/100], Step [101/891], Loss: 18449.9805\n",
            "Epoch [6/100], Step [201/891], Loss: 17338.7852\n",
            "Epoch [6/100], Step [301/891], Loss: 17672.1699\n",
            "Epoch [6/100], Step [401/891], Loss: 17681.9082\n",
            "Epoch [6/100], Step [501/891], Loss: 18146.0293\n",
            "Epoch [6/100], Step [601/891], Loss: 18101.7598\n",
            "Epoch [6/100], Step [701/891], Loss: 17555.8848\n",
            "Epoch [6/100], Step [801/891], Loss: 18393.1035\n",
            "Epoch [7/100], Step [1/891], Loss: 18450.7383\n",
            "Epoch [7/100], Step [101/891], Loss: 17595.6738\n",
            "Epoch [7/100], Step [201/891], Loss: 18705.2930\n",
            "Epoch [7/100], Step [301/891], Loss: 17439.7266\n",
            "Epoch [7/100], Step [401/891], Loss: 16283.2637\n",
            "Epoch [7/100], Step [501/891], Loss: 18462.5117\n",
            "Epoch [7/100], Step [601/891], Loss: 17790.0234\n",
            "Epoch [7/100], Step [701/891], Loss: 18067.7559\n",
            "Epoch [7/100], Step [801/891], Loss: 18301.2891\n",
            "Epoch [8/100], Step [1/891], Loss: 17472.0762\n",
            "Epoch [8/100], Step [101/891], Loss: 17233.9062\n",
            "Epoch [8/100], Step [201/891], Loss: 18366.7852\n",
            "Epoch [8/100], Step [301/891], Loss: 18384.2578\n",
            "Epoch [8/100], Step [401/891], Loss: 18377.4062\n",
            "Epoch [8/100], Step [501/891], Loss: 17357.1738\n",
            "Epoch [8/100], Step [601/891], Loss: 18527.8711\n",
            "Epoch [8/100], Step [701/891], Loss: 18367.3555\n",
            "Epoch [8/100], Step [801/891], Loss: 18539.4023\n",
            "Epoch [9/100], Step [1/891], Loss: 17399.7832\n",
            "Epoch [9/100], Step [101/891], Loss: 17613.9961\n",
            "Epoch [9/100], Step [201/891], Loss: 17341.3594\n",
            "Epoch [9/100], Step [301/891], Loss: 17333.8320\n",
            "Epoch [9/100], Step [401/891], Loss: 19114.7266\n",
            "Epoch [9/100], Step [501/891], Loss: 17762.9297\n",
            "Epoch [9/100], Step [601/891], Loss: 18524.9863\n",
            "Epoch [9/100], Step [701/891], Loss: 17896.4805\n",
            "Epoch [9/100], Step [801/891], Loss: 17091.5488\n",
            "Epoch [10/100], Step [1/891], Loss: 17977.9297\n",
            "Epoch [10/100], Step [101/891], Loss: 16990.6445\n",
            "Epoch [10/100], Step [201/891], Loss: 17423.7422\n",
            "Epoch [10/100], Step [301/891], Loss: 17408.8574\n",
            "Epoch [10/100], Step [401/891], Loss: 17716.4512\n",
            "Epoch [10/100], Step [501/891], Loss: 19668.8945\n",
            "Epoch [10/100], Step [601/891], Loss: 16661.1191\n",
            "Epoch [10/100], Step [701/891], Loss: 17616.4941\n",
            "Epoch [10/100], Step [801/891], Loss: 17709.0742\n",
            "Epoch [11/100], Step [1/891], Loss: 17794.6426\n",
            "Epoch [11/100], Step [101/891], Loss: 16624.1914\n",
            "Epoch [11/100], Step [201/891], Loss: 18030.7422\n",
            "Epoch [11/100], Step [301/891], Loss: 17462.3145\n",
            "Epoch [11/100], Step [401/891], Loss: 17181.2266\n",
            "Epoch [11/100], Step [501/891], Loss: 17490.4023\n",
            "Epoch [11/100], Step [601/891], Loss: 18255.2676\n",
            "Epoch [11/100], Step [701/891], Loss: 17706.3047\n",
            "Epoch [11/100], Step [801/891], Loss: 18197.6309\n",
            "Epoch [12/100], Step [1/891], Loss: 17728.2949\n",
            "Epoch [12/100], Step [101/891], Loss: 17690.6758\n",
            "Epoch [12/100], Step [201/891], Loss: 17973.9414\n",
            "Epoch [12/100], Step [301/891], Loss: 17553.2422\n",
            "Epoch [12/100], Step [401/891], Loss: 17042.8613\n",
            "Epoch [12/100], Step [501/891], Loss: 17412.2051\n",
            "Epoch [12/100], Step [601/891], Loss: 17940.3086\n",
            "Epoch [12/100], Step [701/891], Loss: 16693.0586\n",
            "Epoch [12/100], Step [801/891], Loss: 17169.5859\n",
            "Epoch [13/100], Step [1/891], Loss: 16830.9688\n",
            "Epoch [13/100], Step [101/891], Loss: 16071.4004\n",
            "Epoch [13/100], Step [201/891], Loss: 18123.0703\n",
            "Epoch [13/100], Step [301/891], Loss: 17258.8711\n",
            "Epoch [13/100], Step [401/891], Loss: 16953.5723\n",
            "Epoch [13/100], Step [501/891], Loss: 18325.2109\n",
            "Epoch [13/100], Step [601/891], Loss: 17310.1562\n",
            "Epoch [13/100], Step [701/891], Loss: 17715.1465\n",
            "Epoch [13/100], Step [801/891], Loss: 17993.4883\n",
            "Epoch [14/100], Step [1/891], Loss: 17922.5938\n",
            "Epoch [14/100], Step [101/891], Loss: 17955.5039\n",
            "Epoch [14/100], Step [201/891], Loss: 17723.5273\n",
            "Epoch [14/100], Step [301/891], Loss: 18129.1133\n",
            "Epoch [14/100], Step [401/891], Loss: 17960.4512\n",
            "Epoch [14/100], Step [501/891], Loss: 16960.6914\n",
            "Epoch [14/100], Step [601/891], Loss: 17745.2832\n",
            "Epoch [14/100], Step [701/891], Loss: 18407.2109\n",
            "Epoch [14/100], Step [801/891], Loss: 18050.7578\n",
            "Epoch [15/100], Step [1/891], Loss: 18464.2539\n",
            "Epoch [15/100], Step [101/891], Loss: 17717.4277\n",
            "Epoch [15/100], Step [201/891], Loss: 17239.3145\n",
            "Epoch [15/100], Step [301/891], Loss: 17265.3066\n",
            "Epoch [15/100], Step [401/891], Loss: 17251.7227\n",
            "Epoch [15/100], Step [501/891], Loss: 16870.0781\n",
            "Epoch [15/100], Step [601/891], Loss: 18252.6484\n",
            "Epoch [15/100], Step [701/891], Loss: 18104.1016\n",
            "Epoch [15/100], Step [801/891], Loss: 16623.9062\n",
            "Epoch [16/100], Step [1/891], Loss: 15834.9199\n",
            "Epoch [16/100], Step [101/891], Loss: 16906.1973\n",
            "Epoch [16/100], Step [201/891], Loss: 16884.5254\n",
            "Epoch [16/100], Step [301/891], Loss: 18839.5352\n",
            "Epoch [16/100], Step [401/891], Loss: 16920.9941\n",
            "Epoch [16/100], Step [501/891], Loss: 18397.8652\n",
            "Epoch [16/100], Step [601/891], Loss: 17527.4707\n",
            "Epoch [16/100], Step [701/891], Loss: 17260.6387\n",
            "Epoch [16/100], Step [801/891], Loss: 17442.7070\n",
            "Epoch [17/100], Step [1/891], Loss: 17451.8633\n",
            "Epoch [17/100], Step [101/891], Loss: 17406.4902\n",
            "Epoch [17/100], Step [201/891], Loss: 18237.8691\n",
            "Epoch [17/100], Step [301/891], Loss: 17077.2207\n",
            "Epoch [17/100], Step [401/891], Loss: 17809.1211\n",
            "Epoch [17/100], Step [501/891], Loss: 18051.1523\n",
            "Epoch [17/100], Step [601/891], Loss: 17831.1035\n",
            "Epoch [17/100], Step [701/891], Loss: 17370.9336\n",
            "Epoch [17/100], Step [801/891], Loss: 16940.4199\n",
            "Epoch [18/100], Step [1/891], Loss: 17734.1133\n",
            "Epoch [18/100], Step [101/891], Loss: 17506.8457\n",
            "Epoch [18/100], Step [201/891], Loss: 17103.5547\n",
            "Epoch [18/100], Step [301/891], Loss: 17853.0527\n",
            "Epoch [18/100], Step [401/891], Loss: 17576.5742\n",
            "Epoch [18/100], Step [501/891], Loss: 17595.3379\n",
            "Epoch [18/100], Step [601/891], Loss: 17624.1602\n",
            "Epoch [18/100], Step [701/891], Loss: 17331.9453\n",
            "Epoch [18/100], Step [801/891], Loss: 19505.6621\n",
            "Epoch [19/100], Step [1/891], Loss: 18669.2422\n",
            "Epoch [19/100], Step [101/891], Loss: 17525.4531\n",
            "Epoch [19/100], Step [201/891], Loss: 18612.0078\n",
            "Epoch [19/100], Step [301/891], Loss: 17003.9062\n",
            "Epoch [19/100], Step [401/891], Loss: 18196.3848\n",
            "Epoch [19/100], Step [501/891], Loss: 16360.8623\n",
            "Epoch [19/100], Step [601/891], Loss: 17291.7500\n",
            "Epoch [19/100], Step [701/891], Loss: 17870.6934\n",
            "Epoch [19/100], Step [801/891], Loss: 16359.9629\n",
            "Epoch [20/100], Step [1/891], Loss: 18033.6309\n",
            "Epoch [20/100], Step [101/891], Loss: 17829.7598\n",
            "Epoch [20/100], Step [201/891], Loss: 16174.6963\n",
            "Epoch [20/100], Step [301/891], Loss: 16888.8262\n",
            "Epoch [20/100], Step [401/891], Loss: 17177.1250\n",
            "Epoch [20/100], Step [501/891], Loss: 15937.5312\n",
            "Epoch [20/100], Step [601/891], Loss: 16992.6230\n",
            "Epoch [20/100], Step [701/891], Loss: 18223.5879\n",
            "Epoch [20/100], Step [801/891], Loss: 16507.5059\n",
            "Epoch [21/100], Step [1/891], Loss: 15961.9775\n",
            "Epoch [21/100], Step [101/891], Loss: 17933.1602\n",
            "Epoch [21/100], Step [201/891], Loss: 16854.6934\n",
            "Epoch [21/100], Step [301/891], Loss: 17324.1309\n",
            "Epoch [21/100], Step [401/891], Loss: 17039.2930\n",
            "Epoch [21/100], Step [501/891], Loss: 17940.6367\n",
            "Epoch [21/100], Step [601/891], Loss: 17961.8750\n",
            "Epoch [21/100], Step [701/891], Loss: 17085.1113\n",
            "Epoch [21/100], Step [801/891], Loss: 17439.8496\n",
            "Epoch [22/100], Step [1/891], Loss: 16669.2930\n",
            "Epoch [22/100], Step [101/891], Loss: 16752.7070\n",
            "Epoch [22/100], Step [201/891], Loss: 17469.8574\n",
            "Epoch [22/100], Step [301/891], Loss: 16368.5869\n",
            "Epoch [22/100], Step [401/891], Loss: 16893.8652\n",
            "Epoch [22/100], Step [501/891], Loss: 17463.3203\n",
            "Epoch [22/100], Step [601/891], Loss: 18992.6289\n",
            "Epoch [22/100], Step [701/891], Loss: 18355.1504\n",
            "Epoch [22/100], Step [801/891], Loss: 17933.5117\n",
            "Epoch [23/100], Step [1/891], Loss: 17504.6426\n",
            "Epoch [23/100], Step [101/891], Loss: 17195.5234\n",
            "Epoch [23/100], Step [201/891], Loss: 17967.4629\n",
            "Epoch [23/100], Step [301/891], Loss: 16123.5811\n",
            "Epoch [23/100], Step [401/891], Loss: 17069.7129\n",
            "Epoch [23/100], Step [501/891], Loss: 17365.4121\n",
            "Epoch [23/100], Step [601/891], Loss: 16762.5996\n",
            "Epoch [23/100], Step [701/891], Loss: 17627.4902\n",
            "Epoch [23/100], Step [801/891], Loss: 17204.3184\n",
            "Epoch [24/100], Step [1/891], Loss: 18223.1660\n",
            "Epoch [24/100], Step [101/891], Loss: 17336.2129\n",
            "Epoch [24/100], Step [201/891], Loss: 18638.8809\n",
            "Epoch [24/100], Step [301/891], Loss: 17385.6758\n",
            "Epoch [24/100], Step [401/891], Loss: 17275.5645\n",
            "Epoch [24/100], Step [501/891], Loss: 16438.6348\n",
            "Epoch [24/100], Step [601/891], Loss: 17281.8066\n",
            "Epoch [24/100], Step [701/891], Loss: 17502.0312\n",
            "Epoch [24/100], Step [801/891], Loss: 17261.5938\n",
            "Epoch [25/100], Step [1/891], Loss: 16879.5410\n",
            "Epoch [25/100], Step [101/891], Loss: 17575.4707\n",
            "Epoch [25/100], Step [201/891], Loss: 18246.8359\n",
            "Epoch [25/100], Step [301/891], Loss: 16987.3398\n",
            "Epoch [25/100], Step [401/891], Loss: 16886.1699\n",
            "Epoch [25/100], Step [501/891], Loss: 17923.6484\n",
            "Epoch [25/100], Step [601/891], Loss: 17691.0762\n",
            "Epoch [25/100], Step [701/891], Loss: 17995.9238\n",
            "Epoch [25/100], Step [801/891], Loss: 17251.6699\n",
            "Epoch [26/100], Step [1/891], Loss: 16075.1865\n",
            "Epoch [26/100], Step [101/891], Loss: 16923.5312\n",
            "Epoch [26/100], Step [201/891], Loss: 17424.5352\n",
            "Epoch [26/100], Step [301/891], Loss: 16361.2529\n",
            "Epoch [26/100], Step [401/891], Loss: 16916.5469\n",
            "Epoch [26/100], Step [501/891], Loss: 17061.0039\n",
            "Epoch [26/100], Step [601/891], Loss: 17328.6836\n",
            "Epoch [26/100], Step [701/891], Loss: 16489.9570\n",
            "Epoch [26/100], Step [801/891], Loss: 17425.5059\n",
            "Epoch [27/100], Step [1/891], Loss: 16838.9648\n",
            "Epoch [27/100], Step [101/891], Loss: 17865.8477\n",
            "Epoch [27/100], Step [201/891], Loss: 16513.2871\n",
            "Epoch [27/100], Step [301/891], Loss: 18447.3379\n",
            "Epoch [27/100], Step [401/891], Loss: 16377.3789\n",
            "Epoch [27/100], Step [501/891], Loss: 17171.5547\n",
            "Epoch [27/100], Step [601/891], Loss: 17469.7363\n",
            "Epoch [27/100], Step [701/891], Loss: 16835.9609\n",
            "Epoch [27/100], Step [801/891], Loss: 16689.1602\n",
            "Epoch [28/100], Step [1/891], Loss: 15572.8438\n",
            "Epoch [28/100], Step [101/891], Loss: 17540.9219\n",
            "Epoch [28/100], Step [201/891], Loss: 17311.3027\n",
            "Epoch [28/100], Step [301/891], Loss: 17863.2324\n",
            "Epoch [28/100], Step [401/891], Loss: 16504.5840\n",
            "Epoch [28/100], Step [501/891], Loss: 17750.0273\n",
            "Epoch [28/100], Step [601/891], Loss: 17923.3516\n",
            "Epoch [28/100], Step [701/891], Loss: 17827.9570\n",
            "Epoch [28/100], Step [801/891], Loss: 17654.7129\n",
            "Epoch [29/100], Step [1/891], Loss: 16780.6621\n",
            "Epoch [29/100], Step [101/891], Loss: 17843.9453\n",
            "Epoch [29/100], Step [201/891], Loss: 17025.3867\n",
            "Epoch [29/100], Step [301/891], Loss: 15702.7646\n",
            "Epoch [29/100], Step [401/891], Loss: 16195.4297\n",
            "Epoch [29/100], Step [501/891], Loss: 16785.7520\n",
            "Epoch [29/100], Step [601/891], Loss: 17479.8125\n",
            "Epoch [29/100], Step [701/891], Loss: 17232.4258\n",
            "Epoch [29/100], Step [801/891], Loss: 17106.0039\n",
            "Epoch [30/100], Step [1/891], Loss: 17457.1133\n",
            "Epoch [30/100], Step [101/891], Loss: 17874.6699\n",
            "Epoch [30/100], Step [201/891], Loss: 17684.9883\n",
            "Epoch [30/100], Step [301/891], Loss: 17151.7168\n",
            "Epoch [30/100], Step [401/891], Loss: 17017.0801\n",
            "Epoch [30/100], Step [501/891], Loss: 16859.5918\n",
            "Epoch [30/100], Step [601/891], Loss: 17423.4609\n",
            "Epoch [30/100], Step [701/891], Loss: 17399.3418\n",
            "Epoch [30/100], Step [801/891], Loss: 16327.1094\n",
            "Epoch [31/100], Step [1/891], Loss: 18056.4531\n",
            "Epoch [31/100], Step [101/891], Loss: 17767.8125\n",
            "Epoch [31/100], Step [201/891], Loss: 17874.1836\n",
            "Epoch [31/100], Step [301/891], Loss: 17837.1348\n",
            "Epoch [31/100], Step [401/891], Loss: 17397.2305\n",
            "Epoch [31/100], Step [501/891], Loss: 16396.2070\n",
            "Epoch [31/100], Step [601/891], Loss: 17053.8145\n",
            "Epoch [31/100], Step [701/891], Loss: 17090.7812\n",
            "Epoch [31/100], Step [801/891], Loss: 17428.3594\n",
            "Epoch [32/100], Step [1/891], Loss: 17477.0781\n",
            "Epoch [32/100], Step [101/891], Loss: 17325.4336\n",
            "Epoch [32/100], Step [201/891], Loss: 18264.1133\n",
            "Epoch [32/100], Step [301/891], Loss: 17156.8086\n",
            "Epoch [32/100], Step [401/891], Loss: 16535.2031\n",
            "Epoch [32/100], Step [501/891], Loss: 18207.9980\n",
            "Epoch [32/100], Step [601/891], Loss: 17500.8066\n",
            "Epoch [32/100], Step [701/891], Loss: 17353.8027\n",
            "Epoch [32/100], Step [801/891], Loss: 17809.5508\n",
            "Epoch [33/100], Step [1/891], Loss: 18227.8047\n",
            "Epoch [33/100], Step [101/891], Loss: 16965.2266\n",
            "Epoch [33/100], Step [201/891], Loss: 17254.7852\n",
            "Epoch [33/100], Step [301/891], Loss: 17035.3711\n",
            "Epoch [33/100], Step [401/891], Loss: 16649.1504\n",
            "Epoch [33/100], Step [501/891], Loss: 17604.5391\n",
            "Epoch [33/100], Step [601/891], Loss: 17963.5234\n",
            "Epoch [33/100], Step [701/891], Loss: 17221.5703\n",
            "Epoch [33/100], Step [801/891], Loss: 17070.6973\n",
            "Epoch [34/100], Step [1/891], Loss: 17404.7637\n",
            "Epoch [34/100], Step [101/891], Loss: 17234.7383\n",
            "Epoch [34/100], Step [201/891], Loss: 17420.3750\n",
            "Epoch [34/100], Step [301/891], Loss: 17395.6172\n",
            "Epoch [34/100], Step [401/891], Loss: 16289.0391\n",
            "Epoch [34/100], Step [501/891], Loss: 17367.4668\n",
            "Epoch [34/100], Step [601/891], Loss: 17950.4082\n",
            "Epoch [34/100], Step [701/891], Loss: 17404.0078\n",
            "Epoch [34/100], Step [801/891], Loss: 17323.1621\n",
            "Epoch [35/100], Step [1/891], Loss: 17150.0840\n",
            "Epoch [35/100], Step [101/891], Loss: 16888.1172\n",
            "Epoch [35/100], Step [201/891], Loss: 17306.7617\n",
            "Epoch [35/100], Step [301/891], Loss: 16976.6602\n",
            "Epoch [35/100], Step [401/891], Loss: 16528.5234\n",
            "Epoch [35/100], Step [501/891], Loss: 17117.4727\n",
            "Epoch [35/100], Step [601/891], Loss: 17851.2578\n",
            "Epoch [35/100], Step [701/891], Loss: 16369.0352\n",
            "Epoch [35/100], Step [801/891], Loss: 17205.7168\n",
            "Epoch [36/100], Step [1/891], Loss: 16362.8184\n",
            "Epoch [36/100], Step [101/891], Loss: 15493.7920\n",
            "Epoch [36/100], Step [201/891], Loss: 17169.8809\n",
            "Epoch [36/100], Step [301/891], Loss: 17266.1387\n",
            "Epoch [36/100], Step [401/891], Loss: 17659.0918\n",
            "Epoch [36/100], Step [501/891], Loss: 17911.8320\n",
            "Epoch [36/100], Step [601/891], Loss: 16245.3086\n",
            "Epoch [36/100], Step [701/891], Loss: 18067.0293\n",
            "Epoch [36/100], Step [801/891], Loss: 16978.8496\n",
            "Epoch [37/100], Step [1/891], Loss: 17344.7852\n",
            "Epoch [37/100], Step [101/891], Loss: 18812.2324\n",
            "Epoch [37/100], Step [201/891], Loss: 17676.1270\n",
            "Epoch [37/100], Step [301/891], Loss: 15924.1953\n",
            "Epoch [37/100], Step [401/891], Loss: 17054.5547\n",
            "Epoch [37/100], Step [501/891], Loss: 17332.1094\n",
            "Epoch [37/100], Step [601/891], Loss: 16993.7402\n",
            "Epoch [37/100], Step [701/891], Loss: 16948.7070\n",
            "Epoch [37/100], Step [801/891], Loss: 17231.9180\n",
            "Epoch [38/100], Step [1/891], Loss: 18413.9648\n",
            "Epoch [38/100], Step [101/891], Loss: 16833.9141\n",
            "Epoch [38/100], Step [201/891], Loss: 17186.6094\n",
            "Epoch [38/100], Step [301/891], Loss: 16514.0059\n",
            "Epoch [38/100], Step [401/891], Loss: 17682.0918\n",
            "Epoch [38/100], Step [501/891], Loss: 17840.4902\n",
            "Epoch [38/100], Step [601/891], Loss: 16042.7344\n",
            "Epoch [38/100], Step [701/891], Loss: 17567.1133\n",
            "Epoch [38/100], Step [801/891], Loss: 16801.9668\n",
            "Epoch [39/100], Step [1/891], Loss: 16277.7070\n",
            "Epoch [39/100], Step [101/891], Loss: 17600.9453\n",
            "Epoch [39/100], Step [201/891], Loss: 16350.3867\n",
            "Epoch [39/100], Step [301/891], Loss: 16425.7031\n",
            "Epoch [39/100], Step [401/891], Loss: 16895.9414\n",
            "Epoch [39/100], Step [501/891], Loss: 17467.7266\n",
            "Epoch [39/100], Step [601/891], Loss: 17588.4902\n",
            "Epoch [39/100], Step [701/891], Loss: 17179.9785\n",
            "Epoch [39/100], Step [801/891], Loss: 18133.8691\n",
            "Epoch [40/100], Step [1/891], Loss: 17575.7852\n",
            "Epoch [40/100], Step [101/891], Loss: 16521.8711\n",
            "Epoch [40/100], Step [201/891], Loss: 18317.8301\n",
            "Epoch [40/100], Step [301/891], Loss: 16317.3945\n",
            "Epoch [40/100], Step [401/891], Loss: 17263.0059\n",
            "Epoch [40/100], Step [501/891], Loss: 16644.7969\n",
            "Epoch [40/100], Step [601/891], Loss: 18114.8027\n",
            "Epoch [40/100], Step [701/891], Loss: 16847.3535\n",
            "Epoch [40/100], Step [801/891], Loss: 16196.5410\n",
            "Epoch [41/100], Step [1/891], Loss: 16949.3945\n",
            "Epoch [41/100], Step [101/891], Loss: 18069.2832\n",
            "Epoch [41/100], Step [201/891], Loss: 17395.2188\n",
            "Epoch [41/100], Step [301/891], Loss: 17015.8340\n",
            "Epoch [41/100], Step [401/891], Loss: 16756.4844\n",
            "Epoch [41/100], Step [501/891], Loss: 16598.4629\n",
            "Epoch [41/100], Step [601/891], Loss: 18134.5723\n",
            "Epoch [41/100], Step [701/891], Loss: 18073.7285\n",
            "Epoch [41/100], Step [801/891], Loss: 17967.4219\n",
            "Epoch [42/100], Step [1/891], Loss: 17711.7637\n",
            "Epoch [42/100], Step [101/891], Loss: 17354.7578\n",
            "Epoch [42/100], Step [201/891], Loss: 17070.8828\n",
            "Epoch [42/100], Step [301/891], Loss: 16746.0840\n",
            "Epoch [42/100], Step [401/891], Loss: 18254.9062\n",
            "Epoch [42/100], Step [501/891], Loss: 17759.3203\n",
            "Epoch [42/100], Step [601/891], Loss: 16679.7754\n",
            "Epoch [42/100], Step [701/891], Loss: 17084.0410\n",
            "Epoch [42/100], Step [801/891], Loss: 17622.3652\n",
            "Epoch [43/100], Step [1/891], Loss: 16341.3301\n",
            "Epoch [43/100], Step [101/891], Loss: 17317.1582\n",
            "Epoch [43/100], Step [201/891], Loss: 17125.5430\n",
            "Epoch [43/100], Step [301/891], Loss: 16068.0342\n",
            "Epoch [43/100], Step [401/891], Loss: 17338.8730\n",
            "Epoch [43/100], Step [501/891], Loss: 18272.1719\n",
            "Epoch [43/100], Step [601/891], Loss: 17092.5312\n",
            "Epoch [43/100], Step [701/891], Loss: 17286.2656\n",
            "Epoch [43/100], Step [801/891], Loss: 17340.5078\n",
            "Epoch [44/100], Step [1/891], Loss: 17864.9336\n",
            "Epoch [44/100], Step [101/891], Loss: 17777.6016\n",
            "Epoch [44/100], Step [201/891], Loss: 18154.7227\n",
            "Epoch [44/100], Step [301/891], Loss: 17595.2012\n",
            "Epoch [44/100], Step [401/891], Loss: 16963.6328\n",
            "Epoch [44/100], Step [501/891], Loss: 16984.5312\n",
            "Epoch [44/100], Step [601/891], Loss: 16744.7031\n",
            "Epoch [44/100], Step [701/891], Loss: 18886.7637\n",
            "Epoch [44/100], Step [801/891], Loss: 17163.9629\n",
            "Epoch [45/100], Step [1/891], Loss: 17434.3691\n",
            "Epoch [45/100], Step [101/891], Loss: 16086.9961\n",
            "Epoch [45/100], Step [201/891], Loss: 16001.3613\n",
            "Epoch [45/100], Step [301/891], Loss: 16562.1016\n",
            "Epoch [45/100], Step [401/891], Loss: 16883.0664\n",
            "Epoch [45/100], Step [501/891], Loss: 17658.5430\n",
            "Epoch [45/100], Step [601/891], Loss: 17407.9590\n",
            "Epoch [45/100], Step [701/891], Loss: 17849.2441\n",
            "Epoch [45/100], Step [801/891], Loss: 16463.2266\n",
            "Epoch [46/100], Step [1/891], Loss: 17141.1250\n",
            "Epoch [46/100], Step [101/891], Loss: 17174.8984\n",
            "Epoch [46/100], Step [201/891], Loss: 17135.3945\n",
            "Epoch [46/100], Step [301/891], Loss: 16844.5781\n",
            "Epoch [46/100], Step [401/891], Loss: 17645.8223\n",
            "Epoch [46/100], Step [501/891], Loss: 17232.4180\n",
            "Epoch [46/100], Step [601/891], Loss: 16550.9004\n",
            "Epoch [46/100], Step [701/891], Loss: 17786.8848\n",
            "Epoch [46/100], Step [801/891], Loss: 18919.0840\n",
            "Epoch [47/100], Step [1/891], Loss: 17987.2500\n",
            "Epoch [47/100], Step [101/891], Loss: 17000.9922\n",
            "Epoch [47/100], Step [201/891], Loss: 16056.9072\n",
            "Epoch [47/100], Step [301/891], Loss: 16731.1211\n",
            "Epoch [47/100], Step [401/891], Loss: 17229.6953\n",
            "Epoch [47/100], Step [501/891], Loss: 15543.5869\n",
            "Epoch [47/100], Step [601/891], Loss: 16468.6270\n",
            "Epoch [47/100], Step [701/891], Loss: 17293.5137\n",
            "Epoch [47/100], Step [801/891], Loss: 17734.9375\n",
            "Epoch [48/100], Step [1/891], Loss: 17750.4512\n",
            "Epoch [48/100], Step [101/891], Loss: 16829.0273\n",
            "Epoch [48/100], Step [201/891], Loss: 17919.2012\n",
            "Epoch [48/100], Step [301/891], Loss: 17581.4727\n",
            "Epoch [48/100], Step [401/891], Loss: 16843.2031\n",
            "Epoch [48/100], Step [501/891], Loss: 16165.9238\n",
            "Epoch [48/100], Step [601/891], Loss: 16999.1426\n",
            "Epoch [48/100], Step [701/891], Loss: 18241.6230\n",
            "Epoch [48/100], Step [801/891], Loss: 17455.0000\n",
            "Epoch [49/100], Step [1/891], Loss: 16278.9219\n",
            "Epoch [49/100], Step [101/891], Loss: 17251.2969\n",
            "Epoch [49/100], Step [201/891], Loss: 16647.5508\n",
            "Epoch [49/100], Step [301/891], Loss: 16992.8203\n",
            "Epoch [49/100], Step [401/891], Loss: 16493.1953\n",
            "Epoch [49/100], Step [501/891], Loss: 18014.4238\n",
            "Epoch [49/100], Step [601/891], Loss: 16992.5957\n",
            "Epoch [49/100], Step [701/891], Loss: 17132.1641\n",
            "Epoch [49/100], Step [801/891], Loss: 16922.9336\n",
            "Epoch [50/100], Step [1/891], Loss: 17451.3477\n",
            "Epoch [50/100], Step [101/891], Loss: 16976.3574\n",
            "Epoch [50/100], Step [201/891], Loss: 17647.1094\n",
            "Epoch [50/100], Step [301/891], Loss: 17672.3887\n",
            "Epoch [50/100], Step [401/891], Loss: 17379.3672\n",
            "Epoch [50/100], Step [501/891], Loss: 16534.9648\n",
            "Epoch [50/100], Step [601/891], Loss: 16881.7148\n",
            "Epoch [50/100], Step [701/891], Loss: 16584.9688\n",
            "Epoch [50/100], Step [801/891], Loss: 17634.7031\n",
            "Epoch [51/100], Step [1/891], Loss: 17002.1289\n",
            "Epoch [51/100], Step [101/891], Loss: 16583.6270\n",
            "Epoch [51/100], Step [201/891], Loss: 17449.3477\n",
            "Epoch [51/100], Step [301/891], Loss: 17243.5703\n",
            "Epoch [51/100], Step [401/891], Loss: 17980.9336\n",
            "Epoch [51/100], Step [501/891], Loss: 17027.1543\n",
            "Epoch [51/100], Step [601/891], Loss: 16356.2686\n",
            "Epoch [51/100], Step [701/891], Loss: 16682.0117\n",
            "Epoch [51/100], Step [801/891], Loss: 17055.3984\n",
            "Epoch [52/100], Step [1/891], Loss: 16580.8438\n",
            "Epoch [52/100], Step [101/891], Loss: 16658.7070\n",
            "Epoch [52/100], Step [201/891], Loss: 17349.7637\n",
            "Epoch [52/100], Step [301/891], Loss: 18431.4258\n",
            "Epoch [52/100], Step [401/891], Loss: 16346.8232\n",
            "Epoch [52/100], Step [501/891], Loss: 17054.0215\n",
            "Epoch [52/100], Step [601/891], Loss: 18028.2266\n",
            "Epoch [52/100], Step [701/891], Loss: 17083.0098\n",
            "Epoch [52/100], Step [801/891], Loss: 17698.6660\n",
            "Epoch [53/100], Step [1/891], Loss: 18154.6855\n",
            "Epoch [53/100], Step [101/891], Loss: 17254.5898\n",
            "Epoch [53/100], Step [201/891], Loss: 16782.2207\n",
            "Epoch [53/100], Step [301/891], Loss: 17684.1680\n",
            "Epoch [53/100], Step [401/891], Loss: 17355.7207\n",
            "Epoch [53/100], Step [501/891], Loss: 16944.9512\n",
            "Epoch [53/100], Step [601/891], Loss: 17517.2402\n",
            "Epoch [53/100], Step [701/891], Loss: 18506.4238\n",
            "Epoch [53/100], Step [801/891], Loss: 17446.0254\n",
            "Epoch [54/100], Step [1/891], Loss: 16950.1250\n",
            "Epoch [54/100], Step [101/891], Loss: 17818.8281\n",
            "Epoch [54/100], Step [201/891], Loss: 17326.4727\n",
            "Epoch [54/100], Step [301/891], Loss: 16520.7500\n",
            "Epoch [54/100], Step [401/891], Loss: 17281.5059\n",
            "Epoch [54/100], Step [501/891], Loss: 18219.8066\n",
            "Epoch [54/100], Step [601/891], Loss: 17433.8418\n",
            "Epoch [54/100], Step [701/891], Loss: 17478.2012\n",
            "Epoch [54/100], Step [801/891], Loss: 16719.3145\n",
            "Epoch [55/100], Step [1/891], Loss: 16697.3418\n",
            "Epoch [55/100], Step [101/891], Loss: 17024.7461\n",
            "Epoch [55/100], Step [201/891], Loss: 16698.9414\n",
            "Epoch [55/100], Step [301/891], Loss: 16886.7617\n",
            "Epoch [55/100], Step [401/891], Loss: 17157.2070\n",
            "Epoch [55/100], Step [501/891], Loss: 16680.6328\n",
            "Epoch [55/100], Step [601/891], Loss: 18150.9043\n",
            "Epoch [55/100], Step [701/891], Loss: 17256.0371\n",
            "Epoch [55/100], Step [801/891], Loss: 16571.1426\n",
            "Epoch [56/100], Step [1/891], Loss: 17145.7871\n",
            "Epoch [56/100], Step [101/891], Loss: 17197.1016\n",
            "Epoch [56/100], Step [201/891], Loss: 17579.8574\n",
            "Epoch [56/100], Step [301/891], Loss: 16594.0684\n",
            "Epoch [56/100], Step [401/891], Loss: 16662.9395\n",
            "Epoch [56/100], Step [501/891], Loss: 17049.8672\n",
            "Epoch [56/100], Step [601/891], Loss: 16510.1016\n",
            "Epoch [56/100], Step [701/891], Loss: 16689.3906\n",
            "Epoch [56/100], Step [801/891], Loss: 17332.8418\n",
            "Epoch [57/100], Step [1/891], Loss: 17243.9277\n",
            "Epoch [57/100], Step [101/891], Loss: 16845.4219\n",
            "Epoch [57/100], Step [201/891], Loss: 17509.6152\n",
            "Epoch [57/100], Step [301/891], Loss: 17497.7148\n",
            "Epoch [57/100], Step [401/891], Loss: 16735.8535\n",
            "Epoch [57/100], Step [501/891], Loss: 17099.8691\n",
            "Epoch [57/100], Step [601/891], Loss: 16680.5742\n",
            "Epoch [57/100], Step [701/891], Loss: 17487.1680\n",
            "Epoch [57/100], Step [801/891], Loss: 16341.9258\n",
            "Epoch [58/100], Step [1/891], Loss: 16833.3574\n",
            "Epoch [58/100], Step [101/891], Loss: 17281.0449\n",
            "Epoch [58/100], Step [201/891], Loss: 17491.1777\n",
            "Epoch [58/100], Step [301/891], Loss: 17164.4062\n",
            "Epoch [58/100], Step [401/891], Loss: 15990.1436\n",
            "Epoch [58/100], Step [501/891], Loss: 16933.2109\n",
            "Epoch [58/100], Step [601/891], Loss: 17355.1465\n",
            "Epoch [58/100], Step [701/891], Loss: 17421.2383\n",
            "Epoch [58/100], Step [801/891], Loss: 17453.4590\n",
            "Epoch [59/100], Step [1/891], Loss: 17436.7598\n",
            "Epoch [59/100], Step [101/891], Loss: 16771.0273\n",
            "Epoch [59/100], Step [201/891], Loss: 16911.0703\n",
            "Epoch [59/100], Step [301/891], Loss: 17747.7188\n",
            "Epoch [59/100], Step [401/891], Loss: 16892.2969\n",
            "Epoch [59/100], Step [501/891], Loss: 15940.1309\n",
            "Epoch [59/100], Step [601/891], Loss: 16501.3242\n",
            "Epoch [59/100], Step [701/891], Loss: 17190.8320\n",
            "Epoch [59/100], Step [801/891], Loss: 17573.7207\n",
            "Epoch [60/100], Step [1/891], Loss: 17128.5469\n",
            "Epoch [60/100], Step [101/891], Loss: 16838.5156\n",
            "Epoch [60/100], Step [201/891], Loss: 16812.1016\n",
            "Epoch [60/100], Step [301/891], Loss: 16595.4141\n",
            "Epoch [60/100], Step [401/891], Loss: 18444.2656\n",
            "Epoch [60/100], Step [501/891], Loss: 16335.2627\n",
            "Epoch [60/100], Step [601/891], Loss: 17028.7344\n",
            "Epoch [60/100], Step [701/891], Loss: 17282.7598\n",
            "Epoch [60/100], Step [801/891], Loss: 16715.0254\n",
            "Epoch [61/100], Step [1/891], Loss: 17980.5605\n",
            "Epoch [61/100], Step [101/891], Loss: 16971.2363\n",
            "Epoch [61/100], Step [201/891], Loss: 16674.1504\n",
            "Epoch [61/100], Step [301/891], Loss: 16474.6777\n",
            "Epoch [61/100], Step [401/891], Loss: 17778.6309\n",
            "Epoch [61/100], Step [501/891], Loss: 17822.3594\n",
            "Epoch [61/100], Step [601/891], Loss: 16978.1992\n",
            "Epoch [61/100], Step [701/891], Loss: 17144.4199\n",
            "Epoch [61/100], Step [801/891], Loss: 17342.3066\n",
            "Epoch [62/100], Step [1/891], Loss: 16504.6250\n",
            "Epoch [62/100], Step [101/891], Loss: 17550.1641\n",
            "Epoch [62/100], Step [201/891], Loss: 17422.7207\n",
            "Epoch [62/100], Step [301/891], Loss: 17029.0664\n",
            "Epoch [62/100], Step [401/891], Loss: 16716.7852\n",
            "Epoch [62/100], Step [501/891], Loss: 16732.4766\n",
            "Epoch [62/100], Step [601/891], Loss: 17282.4824\n",
            "Epoch [62/100], Step [701/891], Loss: 17268.3848\n",
            "Epoch [62/100], Step [801/891], Loss: 17397.0371\n",
            "Epoch [63/100], Step [1/891], Loss: 15537.2148\n",
            "Epoch [63/100], Step [101/891], Loss: 17685.9316\n",
            "Epoch [63/100], Step [201/891], Loss: 17868.0605\n",
            "Epoch [63/100], Step [301/891], Loss: 17085.9512\n",
            "Epoch [63/100], Step [401/891], Loss: 16631.4297\n",
            "Epoch [63/100], Step [501/891], Loss: 17673.3594\n",
            "Epoch [63/100], Step [601/891], Loss: 16437.2070\n",
            "Epoch [63/100], Step [701/891], Loss: 18421.5293\n",
            "Epoch [63/100], Step [801/891], Loss: 16718.2969\n",
            "Epoch [64/100], Step [1/891], Loss: 18503.5352\n",
            "Epoch [64/100], Step [101/891], Loss: 16292.1016\n",
            "Epoch [64/100], Step [201/891], Loss: 16012.5527\n",
            "Epoch [64/100], Step [301/891], Loss: 17149.0879\n",
            "Epoch [64/100], Step [401/891], Loss: 17549.4941\n",
            "Epoch [64/100], Step [501/891], Loss: 17596.0312\n",
            "Epoch [64/100], Step [601/891], Loss: 16534.4375\n",
            "Epoch [64/100], Step [701/891], Loss: 17837.1367\n",
            "Epoch [64/100], Step [801/891], Loss: 18287.1602\n",
            "Epoch [65/100], Step [1/891], Loss: 16255.5957\n",
            "Epoch [65/100], Step [101/891], Loss: 17402.7520\n",
            "Epoch [65/100], Step [201/891], Loss: 16513.0703\n",
            "Epoch [65/100], Step [301/891], Loss: 17000.4688\n",
            "Epoch [65/100], Step [401/891], Loss: 18184.9414\n",
            "Epoch [65/100], Step [501/891], Loss: 18244.2305\n",
            "Epoch [65/100], Step [601/891], Loss: 16796.9844\n",
            "Epoch [65/100], Step [701/891], Loss: 17180.6055\n",
            "Epoch [65/100], Step [801/891], Loss: 16745.0449\n",
            "Epoch [66/100], Step [1/891], Loss: 16793.4219\n",
            "Epoch [66/100], Step [101/891], Loss: 16993.5312\n",
            "Epoch [66/100], Step [201/891], Loss: 17499.2480\n",
            "Epoch [66/100], Step [301/891], Loss: 18049.0469\n",
            "Epoch [66/100], Step [401/891], Loss: 17297.4844\n",
            "Epoch [66/100], Step [501/891], Loss: 16460.3594\n",
            "Epoch [66/100], Step [601/891], Loss: 16854.3438\n",
            "Epoch [66/100], Step [701/891], Loss: 17030.4922\n",
            "Epoch [66/100], Step [801/891], Loss: 16783.7148\n",
            "Epoch [67/100], Step [1/891], Loss: 16227.7734\n",
            "Epoch [67/100], Step [101/891], Loss: 17308.6719\n",
            "Epoch [67/100], Step [201/891], Loss: 17555.2500\n",
            "Epoch [67/100], Step [301/891], Loss: 17451.2051\n",
            "Epoch [67/100], Step [401/891], Loss: 15923.9941\n",
            "Epoch [67/100], Step [501/891], Loss: 15748.3887\n",
            "Epoch [67/100], Step [601/891], Loss: 17136.0137\n",
            "Epoch [67/100], Step [701/891], Loss: 16327.5098\n",
            "Epoch [67/100], Step [801/891], Loss: 16587.2656\n",
            "Epoch [68/100], Step [1/891], Loss: 18720.3691\n",
            "Epoch [68/100], Step [101/891], Loss: 16733.2090\n",
            "Epoch [68/100], Step [201/891], Loss: 16569.8789\n",
            "Epoch [68/100], Step [301/891], Loss: 17099.9141\n",
            "Epoch [68/100], Step [401/891], Loss: 16816.2852\n",
            "Epoch [68/100], Step [501/891], Loss: 17983.0527\n",
            "Epoch [68/100], Step [601/891], Loss: 16209.4023\n",
            "Epoch [68/100], Step [701/891], Loss: 16886.3652\n",
            "Epoch [68/100], Step [801/891], Loss: 16815.4785\n",
            "Epoch [69/100], Step [1/891], Loss: 17316.4277\n",
            "Epoch [69/100], Step [101/891], Loss: 17369.4219\n",
            "Epoch [69/100], Step [201/891], Loss: 17581.6426\n",
            "Epoch [69/100], Step [301/891], Loss: 17917.0684\n",
            "Epoch [69/100], Step [401/891], Loss: 16948.4492\n",
            "Epoch [69/100], Step [501/891], Loss: 16375.6895\n",
            "Epoch [69/100], Step [601/891], Loss: 17747.6270\n",
            "Epoch [69/100], Step [701/891], Loss: 16680.9219\n",
            "Epoch [69/100], Step [801/891], Loss: 16432.8887\n",
            "Epoch [70/100], Step [1/891], Loss: 16686.7812\n",
            "Epoch [70/100], Step [101/891], Loss: 17187.5156\n",
            "Epoch [70/100], Step [201/891], Loss: 17542.6016\n",
            "Epoch [70/100], Step [301/891], Loss: 17447.8301\n",
            "Epoch [70/100], Step [401/891], Loss: 17850.9434\n",
            "Epoch [70/100], Step [501/891], Loss: 17755.6797\n",
            "Epoch [70/100], Step [601/891], Loss: 17239.6250\n",
            "Epoch [70/100], Step [701/891], Loss: 16262.7344\n",
            "Epoch [70/100], Step [801/891], Loss: 16992.7832\n",
            "Epoch [71/100], Step [1/891], Loss: 16654.3633\n",
            "Epoch [71/100], Step [101/891], Loss: 16998.2988\n",
            "Epoch [71/100], Step [201/891], Loss: 16741.7305\n",
            "Epoch [71/100], Step [301/891], Loss: 15710.8848\n",
            "Epoch [71/100], Step [401/891], Loss: 15830.5137\n",
            "Epoch [71/100], Step [501/891], Loss: 17588.2598\n",
            "Epoch [71/100], Step [601/891], Loss: 16099.1211\n",
            "Epoch [71/100], Step [701/891], Loss: 15925.0195\n",
            "Epoch [71/100], Step [801/891], Loss: 16904.0156\n",
            "Epoch [72/100], Step [1/891], Loss: 17720.2676\n",
            "Epoch [72/100], Step [101/891], Loss: 16306.0996\n",
            "Epoch [72/100], Step [201/891], Loss: 17047.0801\n",
            "Epoch [72/100], Step [301/891], Loss: 16822.9336\n",
            "Epoch [72/100], Step [401/891], Loss: 17424.4336\n",
            "Epoch [72/100], Step [501/891], Loss: 17831.7734\n",
            "Epoch [72/100], Step [601/891], Loss: 17702.9414\n",
            "Epoch [72/100], Step [701/891], Loss: 16427.1348\n",
            "Epoch [72/100], Step [801/891], Loss: 16812.6914\n",
            "Epoch [73/100], Step [1/891], Loss: 17778.4551\n",
            "Epoch [73/100], Step [101/891], Loss: 16376.2021\n",
            "Epoch [73/100], Step [201/891], Loss: 18048.1406\n",
            "Epoch [73/100], Step [301/891], Loss: 16553.6699\n",
            "Epoch [73/100], Step [401/891], Loss: 17174.9219\n",
            "Epoch [73/100], Step [501/891], Loss: 17014.7129\n",
            "Epoch [73/100], Step [601/891], Loss: 17592.5312\n",
            "Epoch [73/100], Step [701/891], Loss: 16084.1777\n",
            "Epoch [73/100], Step [801/891], Loss: 16713.1777\n",
            "Epoch [74/100], Step [1/891], Loss: 18124.4258\n",
            "Epoch [74/100], Step [101/891], Loss: 17715.2871\n",
            "Epoch [74/100], Step [201/891], Loss: 16717.6738\n",
            "Epoch [74/100], Step [301/891], Loss: 16698.4766\n",
            "Epoch [74/100], Step [401/891], Loss: 17398.0039\n",
            "Epoch [74/100], Step [501/891], Loss: 16222.3701\n",
            "Epoch [74/100], Step [601/891], Loss: 16428.5332\n",
            "Epoch [74/100], Step [701/891], Loss: 16328.0264\n",
            "Epoch [74/100], Step [801/891], Loss: 16455.0410\n",
            "Epoch [75/100], Step [1/891], Loss: 16601.1113\n",
            "Epoch [75/100], Step [101/891], Loss: 16852.1484\n",
            "Epoch [75/100], Step [201/891], Loss: 16557.6602\n",
            "Epoch [75/100], Step [301/891], Loss: 16985.8867\n",
            "Epoch [75/100], Step [401/891], Loss: 17246.8457\n",
            "Epoch [75/100], Step [501/891], Loss: 16588.5645\n",
            "Epoch [75/100], Step [601/891], Loss: 16859.3184\n",
            "Epoch [75/100], Step [701/891], Loss: 16757.1738\n",
            "Epoch [75/100], Step [801/891], Loss: 17018.9570\n",
            "Epoch [76/100], Step [1/891], Loss: 16810.3105\n",
            "Epoch [76/100], Step [101/891], Loss: 17905.4902\n",
            "Epoch [76/100], Step [201/891], Loss: 16992.9043\n",
            "Epoch [76/100], Step [301/891], Loss: 16762.5176\n",
            "Epoch [76/100], Step [401/891], Loss: 17350.9688\n",
            "Epoch [76/100], Step [501/891], Loss: 16606.0996\n",
            "Epoch [76/100], Step [601/891], Loss: 16378.5234\n",
            "Epoch [76/100], Step [701/891], Loss: 18760.0391\n",
            "Epoch [76/100], Step [801/891], Loss: 16710.4277\n",
            "Epoch [77/100], Step [1/891], Loss: 16691.4336\n",
            "Epoch [77/100], Step [101/891], Loss: 16336.5400\n",
            "Epoch [77/100], Step [201/891], Loss: 15727.1377\n",
            "Epoch [77/100], Step [301/891], Loss: 17592.7129\n",
            "Epoch [77/100], Step [401/891], Loss: 16213.5947\n",
            "Epoch [77/100], Step [501/891], Loss: 15917.3008\n",
            "Epoch [77/100], Step [601/891], Loss: 17193.1621\n",
            "Epoch [77/100], Step [701/891], Loss: 16400.8496\n",
            "Epoch [77/100], Step [801/891], Loss: 15914.6992\n",
            "Epoch [78/100], Step [1/891], Loss: 18034.4082\n",
            "Epoch [78/100], Step [101/891], Loss: 17410.0293\n",
            "Epoch [78/100], Step [201/891], Loss: 16535.7910\n",
            "Epoch [78/100], Step [301/891], Loss: 19005.9082\n",
            "Epoch [78/100], Step [401/891], Loss: 16868.3652\n",
            "Epoch [78/100], Step [501/891], Loss: 17802.1680\n",
            "Epoch [78/100], Step [601/891], Loss: 16005.2598\n",
            "Epoch [78/100], Step [701/891], Loss: 17424.5098\n",
            "Epoch [78/100], Step [801/891], Loss: 17106.2734\n",
            "Epoch [79/100], Step [1/891], Loss: 16657.9824\n",
            "Epoch [79/100], Step [101/891], Loss: 17531.5586\n",
            "Epoch [79/100], Step [201/891], Loss: 17829.1133\n",
            "Epoch [79/100], Step [301/891], Loss: 17272.4453\n",
            "Epoch [79/100], Step [401/891], Loss: 17260.4414\n",
            "Epoch [79/100], Step [501/891], Loss: 16683.1641\n",
            "Epoch [79/100], Step [601/891], Loss: 16323.8252\n",
            "Epoch [79/100], Step [701/891], Loss: 16933.2305\n",
            "Epoch [79/100], Step [801/891], Loss: 16715.5371\n",
            "Epoch [80/100], Step [1/891], Loss: 16401.9277\n",
            "Epoch [80/100], Step [101/891], Loss: 16977.7324\n",
            "Epoch [80/100], Step [201/891], Loss: 17012.8457\n",
            "Epoch [80/100], Step [301/891], Loss: 17246.5137\n",
            "Epoch [80/100], Step [401/891], Loss: 16937.1465\n",
            "Epoch [80/100], Step [501/891], Loss: 18058.5781\n",
            "Epoch [80/100], Step [601/891], Loss: 17904.7812\n",
            "Epoch [80/100], Step [701/891], Loss: 16961.7852\n",
            "Epoch [80/100], Step [801/891], Loss: 17566.7520\n",
            "Epoch [81/100], Step [1/891], Loss: 15798.1621\n",
            "Epoch [81/100], Step [101/891], Loss: 17589.7656\n",
            "Epoch [81/100], Step [201/891], Loss: 16190.3877\n",
            "Epoch [81/100], Step [301/891], Loss: 16628.6484\n",
            "Epoch [81/100], Step [401/891], Loss: 17502.1953\n",
            "Epoch [81/100], Step [501/891], Loss: 18223.3086\n",
            "Epoch [81/100], Step [601/891], Loss: 16792.1406\n",
            "Epoch [81/100], Step [701/891], Loss: 16950.6348\n",
            "Epoch [81/100], Step [801/891], Loss: 16608.6094\n",
            "Epoch [82/100], Step [1/891], Loss: 17212.9570\n",
            "Epoch [82/100], Step [101/891], Loss: 16530.1074\n",
            "Epoch [82/100], Step [201/891], Loss: 16601.9199\n",
            "Epoch [82/100], Step [301/891], Loss: 16801.7051\n",
            "Epoch [82/100], Step [401/891], Loss: 16793.1641\n",
            "Epoch [82/100], Step [501/891], Loss: 16162.1543\n",
            "Epoch [82/100], Step [601/891], Loss: 17418.6602\n",
            "Epoch [82/100], Step [701/891], Loss: 17025.2383\n",
            "Epoch [82/100], Step [801/891], Loss: 17422.9316\n",
            "Epoch [83/100], Step [1/891], Loss: 16417.4668\n",
            "Epoch [83/100], Step [101/891], Loss: 17339.7148\n",
            "Epoch [83/100], Step [201/891], Loss: 17428.0234\n",
            "Epoch [83/100], Step [301/891], Loss: 17143.4922\n",
            "Epoch [83/100], Step [401/891], Loss: 16837.4961\n",
            "Epoch [83/100], Step [501/891], Loss: 16633.5547\n",
            "Epoch [83/100], Step [601/891], Loss: 16444.9824\n",
            "Epoch [83/100], Step [701/891], Loss: 16794.2773\n",
            "Epoch [83/100], Step [801/891], Loss: 18105.8301\n",
            "Epoch [84/100], Step [1/891], Loss: 19064.7812\n",
            "Epoch [84/100], Step [101/891], Loss: 15396.5537\n",
            "Epoch [84/100], Step [201/891], Loss: 17213.6758\n",
            "Epoch [84/100], Step [301/891], Loss: 16007.8643\n",
            "Epoch [84/100], Step [401/891], Loss: 16374.5352\n",
            "Epoch [84/100], Step [501/891], Loss: 16872.9414\n",
            "Epoch [84/100], Step [601/891], Loss: 17814.9414\n",
            "Epoch [84/100], Step [701/891], Loss: 17599.8672\n",
            "Epoch [84/100], Step [801/891], Loss: 16343.0361\n",
            "Epoch [85/100], Step [1/891], Loss: 17358.2734\n",
            "Epoch [85/100], Step [101/891], Loss: 17166.1367\n",
            "Epoch [85/100], Step [201/891], Loss: 17012.0781\n",
            "Epoch [85/100], Step [301/891], Loss: 16555.8379\n",
            "Epoch [85/100], Step [401/891], Loss: 16532.7461\n",
            "Epoch [85/100], Step [501/891], Loss: 16978.4512\n",
            "Epoch [85/100], Step [601/891], Loss: 16804.4102\n",
            "Epoch [85/100], Step [701/891], Loss: 16506.9043\n",
            "Epoch [85/100], Step [801/891], Loss: 17456.4707\n",
            "Epoch [86/100], Step [1/891], Loss: 16760.0156\n",
            "Epoch [86/100], Step [101/891], Loss: 17496.1172\n",
            "Epoch [86/100], Step [201/891], Loss: 16825.9707\n",
            "Epoch [86/100], Step [301/891], Loss: 16372.9863\n",
            "Epoch [86/100], Step [401/891], Loss: 17381.6504\n",
            "Epoch [86/100], Step [501/891], Loss: 17306.0508\n",
            "Epoch [86/100], Step [601/891], Loss: 16243.4834\n",
            "Epoch [86/100], Step [701/891], Loss: 16780.9473\n",
            "Epoch [86/100], Step [801/891], Loss: 17353.9492\n",
            "Epoch [87/100], Step [1/891], Loss: 16359.1973\n",
            "Epoch [87/100], Step [101/891], Loss: 17253.8379\n",
            "Epoch [87/100], Step [201/891], Loss: 17915.0547\n",
            "Epoch [87/100], Step [301/891], Loss: 16816.4141\n",
            "Epoch [87/100], Step [401/891], Loss: 16018.9482\n",
            "Epoch [87/100], Step [501/891], Loss: 17212.8828\n",
            "Epoch [87/100], Step [601/891], Loss: 16152.2393\n",
            "Epoch [87/100], Step [701/891], Loss: 17730.3359\n",
            "Epoch [87/100], Step [801/891], Loss: 16047.4092\n",
            "Epoch [88/100], Step [1/891], Loss: 17266.4473\n",
            "Epoch [88/100], Step [101/891], Loss: 16513.1641\n",
            "Epoch [88/100], Step [201/891], Loss: 15989.7627\n",
            "Epoch [88/100], Step [301/891], Loss: 16602.7695\n",
            "Epoch [88/100], Step [401/891], Loss: 16300.3398\n",
            "Epoch [88/100], Step [501/891], Loss: 17558.4668\n",
            "Epoch [88/100], Step [601/891], Loss: 16766.5410\n",
            "Epoch [88/100], Step [701/891], Loss: 17816.4648\n",
            "Epoch [88/100], Step [801/891], Loss: 16255.1768\n",
            "Epoch [89/100], Step [1/891], Loss: 18004.4609\n",
            "Epoch [89/100], Step [101/891], Loss: 16929.2148\n",
            "Epoch [89/100], Step [201/891], Loss: 16711.8164\n",
            "Epoch [89/100], Step [301/891], Loss: 16317.5879\n",
            "Epoch [89/100], Step [401/891], Loss: 15914.9014\n",
            "Epoch [89/100], Step [501/891], Loss: 17279.2812\n",
            "Epoch [89/100], Step [601/891], Loss: 17024.0605\n",
            "Epoch [89/100], Step [701/891], Loss: 18197.4746\n",
            "Epoch [89/100], Step [801/891], Loss: 16224.8994\n",
            "Epoch [90/100], Step [1/891], Loss: 15786.1084\n",
            "Epoch [90/100], Step [101/891], Loss: 15797.4180\n",
            "Epoch [90/100], Step [201/891], Loss: 17459.8262\n",
            "Epoch [90/100], Step [301/891], Loss: 17447.6973\n",
            "Epoch [90/100], Step [401/891], Loss: 17051.2852\n",
            "Epoch [90/100], Step [501/891], Loss: 17210.8320\n",
            "Epoch [90/100], Step [601/891], Loss: 17778.3301\n",
            "Epoch [90/100], Step [701/891], Loss: 17454.6348\n",
            "Epoch [90/100], Step [801/891], Loss: 16586.5625\n",
            "Epoch [91/100], Step [1/891], Loss: 16692.8008\n",
            "Epoch [91/100], Step [101/891], Loss: 17440.9570\n",
            "Epoch [91/100], Step [201/891], Loss: 17870.4980\n",
            "Epoch [91/100], Step [301/891], Loss: 17512.3105\n",
            "Epoch [91/100], Step [401/891], Loss: 16988.7402\n",
            "Epoch [91/100], Step [501/891], Loss: 17265.8789\n",
            "Epoch [91/100], Step [601/891], Loss: 16906.0371\n",
            "Epoch [91/100], Step [701/891], Loss: 16420.5723\n",
            "Epoch [91/100], Step [801/891], Loss: 16250.7412\n",
            "Epoch [92/100], Step [1/891], Loss: 16452.8477\n",
            "Epoch [92/100], Step [101/891], Loss: 16805.8320\n",
            "Epoch [92/100], Step [201/891], Loss: 16691.1562\n",
            "Epoch [92/100], Step [301/891], Loss: 17174.1914\n",
            "Epoch [92/100], Step [401/891], Loss: 17003.1797\n",
            "Epoch [92/100], Step [501/891], Loss: 16888.2773\n",
            "Epoch [92/100], Step [601/891], Loss: 16918.7695\n",
            "Epoch [92/100], Step [701/891], Loss: 16219.1348\n",
            "Epoch [92/100], Step [801/891], Loss: 16541.8867\n",
            "Epoch [93/100], Step [1/891], Loss: 17469.3242\n",
            "Epoch [93/100], Step [101/891], Loss: 16376.5137\n",
            "Epoch [93/100], Step [201/891], Loss: 17714.1855\n",
            "Epoch [93/100], Step [301/891], Loss: 17638.9668\n",
            "Epoch [93/100], Step [401/891], Loss: 16325.6299\n",
            "Epoch [93/100], Step [501/891], Loss: 16687.3262\n",
            "Epoch [93/100], Step [601/891], Loss: 17452.5312\n",
            "Epoch [93/100], Step [701/891], Loss: 16780.9004\n",
            "Epoch [93/100], Step [801/891], Loss: 16776.7090\n",
            "Epoch [94/100], Step [1/891], Loss: 16703.1211\n",
            "Epoch [94/100], Step [101/891], Loss: 16616.3398\n",
            "Epoch [94/100], Step [201/891], Loss: 16566.0098\n",
            "Epoch [94/100], Step [301/891], Loss: 18523.8984\n",
            "Epoch [94/100], Step [401/891], Loss: 17152.7930\n",
            "Epoch [94/100], Step [501/891], Loss: 17295.5918\n",
            "Epoch [94/100], Step [601/891], Loss: 16959.5508\n",
            "Epoch [94/100], Step [701/891], Loss: 17746.0410\n",
            "Epoch [94/100], Step [801/891], Loss: 16548.1562\n",
            "Epoch [95/100], Step [1/891], Loss: 16423.0254\n",
            "Epoch [95/100], Step [101/891], Loss: 15966.4375\n",
            "Epoch [95/100], Step [201/891], Loss: 17573.7539\n",
            "Epoch [95/100], Step [301/891], Loss: 17930.8398\n",
            "Epoch [95/100], Step [401/891], Loss: 16986.3809\n",
            "Epoch [95/100], Step [501/891], Loss: 15753.9150\n",
            "Epoch [95/100], Step [601/891], Loss: 15918.2500\n",
            "Epoch [95/100], Step [701/891], Loss: 17953.9043\n",
            "Epoch [95/100], Step [801/891], Loss: 17071.0410\n",
            "Epoch [96/100], Step [1/891], Loss: 17848.4648\n",
            "Epoch [96/100], Step [101/891], Loss: 17821.2949\n",
            "Epoch [96/100], Step [201/891], Loss: 17089.9531\n",
            "Epoch [96/100], Step [301/891], Loss: 16760.5625\n",
            "Epoch [96/100], Step [401/891], Loss: 17379.7617\n",
            "Epoch [96/100], Step [501/891], Loss: 16390.3672\n",
            "Epoch [96/100], Step [601/891], Loss: 16594.6250\n",
            "Epoch [96/100], Step [701/891], Loss: 17636.0820\n",
            "Epoch [96/100], Step [801/891], Loss: 16714.7520\n",
            "Epoch [97/100], Step [1/891], Loss: 15793.1416\n",
            "Epoch [97/100], Step [101/891], Loss: 16294.3672\n",
            "Epoch [97/100], Step [201/891], Loss: 16827.6211\n",
            "Epoch [97/100], Step [301/891], Loss: 17332.1875\n",
            "Epoch [97/100], Step [401/891], Loss: 16664.9023\n",
            "Epoch [97/100], Step [501/891], Loss: 18045.3262\n",
            "Epoch [97/100], Step [601/891], Loss: 17086.5703\n",
            "Epoch [97/100], Step [701/891], Loss: 17234.2578\n",
            "Epoch [97/100], Step [801/891], Loss: 17958.7480\n",
            "Epoch [98/100], Step [1/891], Loss: 17524.5898\n",
            "Epoch [98/100], Step [101/891], Loss: 18355.8711\n",
            "Epoch [98/100], Step [201/891], Loss: 16466.3398\n",
            "Epoch [98/100], Step [301/891], Loss: 16887.0586\n",
            "Epoch [98/100], Step [401/891], Loss: 16910.3320\n",
            "Epoch [98/100], Step [501/891], Loss: 16853.0801\n",
            "Epoch [98/100], Step [601/891], Loss: 17324.4824\n",
            "Epoch [98/100], Step [701/891], Loss: 18291.4648\n",
            "Epoch [98/100], Step [801/891], Loss: 17090.1602\n",
            "Epoch [99/100], Step [1/891], Loss: 17705.2480\n",
            "Epoch [99/100], Step [101/891], Loss: 16622.9355\n",
            "Epoch [99/100], Step [201/891], Loss: 15620.6133\n",
            "Epoch [99/100], Step [301/891], Loss: 16976.0059\n",
            "Epoch [99/100], Step [401/891], Loss: 17317.2773\n",
            "Epoch [99/100], Step [501/891], Loss: 17833.3027\n",
            "Epoch [99/100], Step [601/891], Loss: 16157.7441\n",
            "Epoch [99/100], Step [701/891], Loss: 16755.7109\n",
            "Epoch [99/100], Step [801/891], Loss: 17451.5312\n",
            "Epoch [100/100], Step [1/891], Loss: 16798.6426\n",
            "Epoch [100/100], Step [101/891], Loss: 16166.4404\n",
            "Epoch [100/100], Step [201/891], Loss: 15808.7363\n",
            "Epoch [100/100], Step [301/891], Loss: 17071.3008\n",
            "Epoch [100/100], Step [401/891], Loss: 17645.3262\n",
            "Epoch [100/100], Step [501/891], Loss: 17675.8418\n",
            "Epoch [100/100], Step [601/891], Loss: 17786.7383\n",
            "Epoch [100/100], Step [701/891], Loss: 17633.2500\n",
            "Epoch [100/100], Step [801/891], Loss: 16662.5762\n",
            "Accuracy of the SVM on the test images with 3000 labeled images: 81.66%\n",
            "Epoch [1/100], Step [1/936], Loss: 38208.3789\n",
            "Epoch [1/100], Step [101/936], Loss: 12526.1641\n",
            "Epoch [1/100], Step [201/936], Loss: 12814.3828\n",
            "Epoch [1/100], Step [301/936], Loss: 12133.5146\n",
            "Epoch [1/100], Step [401/936], Loss: 11272.7041\n",
            "Epoch [1/100], Step [501/936], Loss: 11276.4648\n",
            "Epoch [1/100], Step [601/936], Loss: 11513.3867\n",
            "Epoch [1/100], Step [701/936], Loss: 11566.7920\n",
            "Epoch [1/100], Step [801/936], Loss: 11188.6738\n",
            "Epoch [1/100], Step [901/936], Loss: 11365.7627\n",
            "Epoch [2/100], Step [1/936], Loss: 10679.2539\n",
            "Epoch [2/100], Step [101/936], Loss: 11168.6992\n",
            "Epoch [2/100], Step [201/936], Loss: 10501.7627\n",
            "Epoch [2/100], Step [301/936], Loss: 10161.9795\n",
            "Epoch [2/100], Step [401/936], Loss: 11041.3164\n",
            "Epoch [2/100], Step [501/936], Loss: 11074.2939\n",
            "Epoch [2/100], Step [601/936], Loss: 11086.7305\n",
            "Epoch [2/100], Step [701/936], Loss: 10296.1992\n",
            "Epoch [2/100], Step [801/936], Loss: 10667.0439\n",
            "Epoch [2/100], Step [901/936], Loss: 9878.3633\n",
            "Epoch [3/100], Step [1/936], Loss: 10628.9492\n",
            "Epoch [3/100], Step [101/936], Loss: 10223.3750\n",
            "Epoch [3/100], Step [201/936], Loss: 10095.7109\n",
            "Epoch [3/100], Step [301/936], Loss: 9923.1406\n",
            "Epoch [3/100], Step [401/936], Loss: 10690.6201\n",
            "Epoch [3/100], Step [501/936], Loss: 10693.3535\n",
            "Epoch [3/100], Step [601/936], Loss: 10007.7832\n",
            "Epoch [3/100], Step [701/936], Loss: 9751.1572\n",
            "Epoch [3/100], Step [801/936], Loss: 10515.9756\n",
            "Epoch [3/100], Step [901/936], Loss: 9649.0654\n",
            "Epoch [4/100], Step [1/936], Loss: 9710.5537\n",
            "Epoch [4/100], Step [101/936], Loss: 10020.5635\n",
            "Epoch [4/100], Step [201/936], Loss: 10085.4785\n",
            "Epoch [4/100], Step [301/936], Loss: 10333.8350\n",
            "Epoch [4/100], Step [401/936], Loss: 10146.8809\n",
            "Epoch [4/100], Step [501/936], Loss: 10165.1582\n",
            "Epoch [4/100], Step [601/936], Loss: 10318.6045\n",
            "Epoch [4/100], Step [701/936], Loss: 9742.1064\n",
            "Epoch [4/100], Step [801/936], Loss: 9544.6299\n",
            "Epoch [4/100], Step [901/936], Loss: 10211.8750\n",
            "Epoch [5/100], Step [1/936], Loss: 9634.2256\n",
            "Epoch [5/100], Step [101/936], Loss: 10018.9941\n",
            "Epoch [5/100], Step [201/936], Loss: 9953.0225\n",
            "Epoch [5/100], Step [301/936], Loss: 9954.1836\n",
            "Epoch [5/100], Step [401/936], Loss: 10077.7500\n",
            "Epoch [5/100], Step [501/936], Loss: 10264.8105\n",
            "Epoch [5/100], Step [601/936], Loss: 9678.0293\n",
            "Epoch [5/100], Step [701/936], Loss: 9152.6895\n",
            "Epoch [5/100], Step [801/936], Loss: 10004.9590\n",
            "Epoch [5/100], Step [901/936], Loss: 9785.0371\n",
            "Epoch [6/100], Step [1/936], Loss: 10338.9131\n",
            "Epoch [6/100], Step [101/936], Loss: 9769.6494\n",
            "Epoch [6/100], Step [201/936], Loss: 9141.9219\n",
            "Epoch [6/100], Step [301/936], Loss: 9813.1104\n",
            "Epoch [6/100], Step [401/936], Loss: 9769.9258\n",
            "Epoch [6/100], Step [501/936], Loss: 9574.5029\n",
            "Epoch [6/100], Step [601/936], Loss: 9765.3848\n",
            "Epoch [6/100], Step [701/936], Loss: 9498.2354\n",
            "Epoch [6/100], Step [801/936], Loss: 9802.6309\n",
            "Epoch [6/100], Step [901/936], Loss: 9938.0938\n",
            "Epoch [7/100], Step [1/936], Loss: 9531.7939\n",
            "Epoch [7/100], Step [101/936], Loss: 9254.1865\n",
            "Epoch [7/100], Step [201/936], Loss: 9437.6650\n",
            "Epoch [7/100], Step [301/936], Loss: 9073.3389\n",
            "Epoch [7/100], Step [401/936], Loss: 9499.2119\n",
            "Epoch [7/100], Step [501/936], Loss: 10122.9219\n",
            "Epoch [7/100], Step [601/936], Loss: 9469.7441\n",
            "Epoch [7/100], Step [701/936], Loss: 9149.7666\n",
            "Epoch [7/100], Step [801/936], Loss: 9713.1152\n",
            "Epoch [7/100], Step [901/936], Loss: 9438.1943\n",
            "Epoch [8/100], Step [1/936], Loss: 10019.2891\n",
            "Epoch [8/100], Step [101/936], Loss: 9552.3154\n",
            "Epoch [8/100], Step [201/936], Loss: 9516.0820\n",
            "Epoch [8/100], Step [301/936], Loss: 8620.6113\n",
            "Epoch [8/100], Step [401/936], Loss: 9380.5391\n",
            "Epoch [8/100], Step [501/936], Loss: 9786.0674\n",
            "Epoch [8/100], Step [601/936], Loss: 9956.1533\n",
            "Epoch [8/100], Step [701/936], Loss: 9270.7217\n",
            "Epoch [8/100], Step [801/936], Loss: 10114.1143\n",
            "Epoch [8/100], Step [901/936], Loss: 9966.5732\n",
            "Epoch [9/100], Step [1/936], Loss: 9011.2090\n",
            "Epoch [9/100], Step [101/936], Loss: 9718.2832\n",
            "Epoch [9/100], Step [201/936], Loss: 9281.4414\n",
            "Epoch [9/100], Step [301/936], Loss: 9679.4062\n",
            "Epoch [9/100], Step [401/936], Loss: 9076.0518\n",
            "Epoch [9/100], Step [501/936], Loss: 8880.9727\n",
            "Epoch [9/100], Step [601/936], Loss: 9759.0605\n",
            "Epoch [9/100], Step [701/936], Loss: 10037.6445\n",
            "Epoch [9/100], Step [801/936], Loss: 9929.7764\n",
            "Epoch [9/100], Step [901/936], Loss: 10147.5547\n",
            "Epoch [10/100], Step [1/936], Loss: 9565.4980\n",
            "Epoch [10/100], Step [101/936], Loss: 9167.5244\n",
            "Epoch [10/100], Step [201/936], Loss: 9687.1807\n",
            "Epoch [10/100], Step [301/936], Loss: 9461.4346\n",
            "Epoch [10/100], Step [401/936], Loss: 8912.1611\n",
            "Epoch [10/100], Step [501/936], Loss: 9044.6680\n",
            "Epoch [10/100], Step [601/936], Loss: 9729.4912\n",
            "Epoch [10/100], Step [701/936], Loss: 9482.4375\n",
            "Epoch [10/100], Step [801/936], Loss: 9621.0771\n",
            "Epoch [10/100], Step [901/936], Loss: 9111.8721\n",
            "Epoch [11/100], Step [1/936], Loss: 8807.1260\n",
            "Epoch [11/100], Step [101/936], Loss: 9457.4180\n",
            "Epoch [11/100], Step [201/936], Loss: 9232.6377\n",
            "Epoch [11/100], Step [301/936], Loss: 9368.5215\n",
            "Epoch [11/100], Step [401/936], Loss: 9141.0381\n",
            "Epoch [11/100], Step [501/936], Loss: 9609.5303\n",
            "Epoch [11/100], Step [601/936], Loss: 8668.3203\n",
            "Epoch [11/100], Step [701/936], Loss: 8918.2197\n",
            "Epoch [11/100], Step [801/936], Loss: 9417.3965\n",
            "Epoch [11/100], Step [901/936], Loss: 9555.9023\n",
            "Epoch [12/100], Step [1/936], Loss: 9676.4648\n",
            "Epoch [12/100], Step [101/936], Loss: 9819.6387\n",
            "Epoch [12/100], Step [201/936], Loss: 9144.9014\n",
            "Epoch [12/100], Step [301/936], Loss: 9551.3311\n",
            "Epoch [12/100], Step [401/936], Loss: 9116.9844\n",
            "Epoch [12/100], Step [501/936], Loss: 9646.5029\n",
            "Epoch [12/100], Step [601/936], Loss: 9231.7324\n",
            "Epoch [12/100], Step [701/936], Loss: 9551.1826\n",
            "Epoch [12/100], Step [801/936], Loss: 9090.0547\n",
            "Epoch [12/100], Step [901/936], Loss: 9561.0947\n",
            "Epoch [13/100], Step [1/936], Loss: 9029.0459\n",
            "Epoch [13/100], Step [101/936], Loss: 9450.4961\n",
            "Epoch [13/100], Step [201/936], Loss: 9441.9863\n",
            "Epoch [13/100], Step [301/936], Loss: 9424.9707\n",
            "Epoch [13/100], Step [401/936], Loss: 9792.3457\n",
            "Epoch [13/100], Step [501/936], Loss: 9445.9395\n",
            "Epoch [13/100], Step [601/936], Loss: 9484.9766\n",
            "Epoch [13/100], Step [701/936], Loss: 9301.7012\n",
            "Epoch [13/100], Step [801/936], Loss: 9431.7480\n",
            "Epoch [13/100], Step [901/936], Loss: 8867.3936\n",
            "Epoch [14/100], Step [1/936], Loss: 8815.2617\n",
            "Epoch [14/100], Step [101/936], Loss: 9207.3965\n",
            "Epoch [14/100], Step [201/936], Loss: 8765.3955\n",
            "Epoch [14/100], Step [301/936], Loss: 8967.4707\n",
            "Epoch [14/100], Step [401/936], Loss: 9376.2637\n",
            "Epoch [14/100], Step [501/936], Loss: 9272.4648\n",
            "Epoch [14/100], Step [601/936], Loss: 9048.6914\n",
            "Epoch [14/100], Step [701/936], Loss: 9479.9531\n",
            "Epoch [14/100], Step [801/936], Loss: 9050.3477\n",
            "Epoch [14/100], Step [901/936], Loss: 9429.0742\n",
            "Epoch [15/100], Step [1/936], Loss: 9338.5674\n",
            "Epoch [15/100], Step [101/936], Loss: 8808.7500\n",
            "Epoch [15/100], Step [201/936], Loss: 9017.3027\n",
            "Epoch [15/100], Step [301/936], Loss: 9199.9775\n",
            "Epoch [15/100], Step [401/936], Loss: 8943.4512\n",
            "Epoch [15/100], Step [501/936], Loss: 9244.3438\n",
            "Epoch [15/100], Step [601/936], Loss: 9332.1025\n",
            "Epoch [15/100], Step [701/936], Loss: 9303.9062\n",
            "Epoch [15/100], Step [801/936], Loss: 9481.6621\n",
            "Epoch [15/100], Step [901/936], Loss: 9221.5449\n",
            "Epoch [16/100], Step [1/936], Loss: 9580.4180\n",
            "Epoch [16/100], Step [101/936], Loss: 8926.4062\n",
            "Epoch [16/100], Step [201/936], Loss: 9077.9844\n",
            "Epoch [16/100], Step [301/936], Loss: 8576.3662\n",
            "Epoch [16/100], Step [401/936], Loss: 9441.5117\n",
            "Epoch [16/100], Step [501/936], Loss: 9376.0547\n",
            "Epoch [16/100], Step [601/936], Loss: 9434.4551\n",
            "Epoch [16/100], Step [701/936], Loss: 8905.1055\n",
            "Epoch [16/100], Step [801/936], Loss: 9326.4346\n",
            "Epoch [16/100], Step [901/936], Loss: 9682.4336\n",
            "Epoch [17/100], Step [1/936], Loss: 9080.7754\n",
            "Epoch [17/100], Step [101/936], Loss: 9142.9492\n",
            "Epoch [17/100], Step [201/936], Loss: 8866.5234\n",
            "Epoch [17/100], Step [301/936], Loss: 8862.1465\n",
            "Epoch [17/100], Step [401/936], Loss: 9319.0947\n",
            "Epoch [17/100], Step [501/936], Loss: 9119.6602\n",
            "Epoch [17/100], Step [601/936], Loss: 8966.8750\n",
            "Epoch [17/100], Step [701/936], Loss: 8611.0479\n",
            "Epoch [17/100], Step [801/936], Loss: 9377.3896\n",
            "Epoch [17/100], Step [901/936], Loss: 8908.3193\n",
            "Epoch [18/100], Step [1/936], Loss: 8719.2324\n",
            "Epoch [18/100], Step [101/936], Loss: 9012.0098\n",
            "Epoch [18/100], Step [201/936], Loss: 9276.8115\n",
            "Epoch [18/100], Step [301/936], Loss: 8794.6826\n",
            "Epoch [18/100], Step [401/936], Loss: 9056.6689\n",
            "Epoch [18/100], Step [501/936], Loss: 9367.4346\n",
            "Epoch [18/100], Step [601/936], Loss: 9327.7412\n",
            "Epoch [18/100], Step [701/936], Loss: 8989.2031\n",
            "Epoch [18/100], Step [801/936], Loss: 9013.0625\n",
            "Epoch [18/100], Step [901/936], Loss: 8781.5078\n",
            "Epoch [19/100], Step [1/936], Loss: 9717.7598\n",
            "Epoch [19/100], Step [101/936], Loss: 8728.1494\n",
            "Epoch [19/100], Step [201/936], Loss: 8739.9600\n",
            "Epoch [19/100], Step [301/936], Loss: 8880.5713\n",
            "Epoch [19/100], Step [401/936], Loss: 9107.7197\n",
            "Epoch [19/100], Step [501/936], Loss: 9096.9922\n",
            "Epoch [19/100], Step [601/936], Loss: 9042.6797\n",
            "Epoch [19/100], Step [701/936], Loss: 9081.3809\n",
            "Epoch [19/100], Step [801/936], Loss: 9054.8115\n",
            "Epoch [19/100], Step [901/936], Loss: 8502.0039\n",
            "Epoch [20/100], Step [1/936], Loss: 8989.6689\n",
            "Epoch [20/100], Step [101/936], Loss: 9124.6123\n",
            "Epoch [20/100], Step [201/936], Loss: 9004.6631\n",
            "Epoch [20/100], Step [301/936], Loss: 8986.9336\n",
            "Epoch [20/100], Step [401/936], Loss: 9076.1602\n",
            "Epoch [20/100], Step [501/936], Loss: 9001.5752\n",
            "Epoch [20/100], Step [601/936], Loss: 9155.3359\n",
            "Epoch [20/100], Step [701/936], Loss: 9579.7666\n",
            "Epoch [20/100], Step [801/936], Loss: 8911.4727\n",
            "Epoch [20/100], Step [901/936], Loss: 9012.6777\n",
            "Epoch [21/100], Step [1/936], Loss: 8847.0488\n",
            "Epoch [21/100], Step [101/936], Loss: 8804.1719\n",
            "Epoch [21/100], Step [201/936], Loss: 9368.8105\n",
            "Epoch [21/100], Step [301/936], Loss: 8458.9727\n",
            "Epoch [21/100], Step [401/936], Loss: 8816.0332\n",
            "Epoch [21/100], Step [501/936], Loss: 8979.9053\n",
            "Epoch [21/100], Step [601/936], Loss: 8837.1045\n",
            "Epoch [21/100], Step [701/936], Loss: 9380.0674\n",
            "Epoch [21/100], Step [801/936], Loss: 9406.4883\n",
            "Epoch [21/100], Step [901/936], Loss: 9144.7471\n",
            "Epoch [22/100], Step [1/936], Loss: 8586.3066\n",
            "Epoch [22/100], Step [101/936], Loss: 9343.0713\n",
            "Epoch [22/100], Step [201/936], Loss: 8738.7637\n",
            "Epoch [22/100], Step [301/936], Loss: 8832.9648\n",
            "Epoch [22/100], Step [401/936], Loss: 9501.1104\n",
            "Epoch [22/100], Step [501/936], Loss: 9103.0430\n",
            "Epoch [22/100], Step [601/936], Loss: 9341.5264\n",
            "Epoch [22/100], Step [701/936], Loss: 9098.4600\n",
            "Epoch [22/100], Step [801/936], Loss: 8928.7852\n",
            "Epoch [22/100], Step [901/936], Loss: 9121.4795\n",
            "Epoch [23/100], Step [1/936], Loss: 9124.2900\n",
            "Epoch [23/100], Step [101/936], Loss: 8988.8545\n",
            "Epoch [23/100], Step [201/936], Loss: 8840.9033\n",
            "Epoch [23/100], Step [301/936], Loss: 8776.9980\n",
            "Epoch [23/100], Step [401/936], Loss: 8337.3975\n",
            "Epoch [23/100], Step [501/936], Loss: 8801.1436\n",
            "Epoch [23/100], Step [601/936], Loss: 8583.6943\n",
            "Epoch [23/100], Step [701/936], Loss: 9443.0254\n",
            "Epoch [23/100], Step [801/936], Loss: 9348.6914\n",
            "Epoch [23/100], Step [901/936], Loss: 9004.0156\n",
            "Epoch [24/100], Step [1/936], Loss: 8984.0693\n",
            "Epoch [24/100], Step [101/936], Loss: 8851.5312\n",
            "Epoch [24/100], Step [201/936], Loss: 9079.7031\n",
            "Epoch [24/100], Step [301/936], Loss: 8419.2051\n",
            "Epoch [24/100], Step [401/936], Loss: 9982.6436\n",
            "Epoch [24/100], Step [501/936], Loss: 9194.8711\n",
            "Epoch [24/100], Step [601/936], Loss: 8633.2148\n",
            "Epoch [24/100], Step [701/936], Loss: 8622.3887\n",
            "Epoch [24/100], Step [801/936], Loss: 9387.9795\n",
            "Epoch [24/100], Step [901/936], Loss: 8917.4619\n",
            "Epoch [25/100], Step [1/936], Loss: 9518.5449\n",
            "Epoch [25/100], Step [101/936], Loss: 8692.1016\n",
            "Epoch [25/100], Step [201/936], Loss: 8749.4766\n",
            "Epoch [25/100], Step [301/936], Loss: 9073.6299\n",
            "Epoch [25/100], Step [401/936], Loss: 9316.8662\n",
            "Epoch [25/100], Step [501/936], Loss: 9187.2266\n",
            "Epoch [25/100], Step [601/936], Loss: 8941.0850\n",
            "Epoch [25/100], Step [701/936], Loss: 8965.8311\n",
            "Epoch [25/100], Step [801/936], Loss: 8938.0625\n",
            "Epoch [25/100], Step [901/936], Loss: 9168.1016\n",
            "Epoch [26/100], Step [1/936], Loss: 8310.0449\n",
            "Epoch [26/100], Step [101/936], Loss: 8837.7607\n",
            "Epoch [26/100], Step [201/936], Loss: 9028.1973\n",
            "Epoch [26/100], Step [301/936], Loss: 9272.6885\n",
            "Epoch [26/100], Step [401/936], Loss: 9048.8135\n",
            "Epoch [26/100], Step [501/936], Loss: 9266.3086\n",
            "Epoch [26/100], Step [601/936], Loss: 9514.5107\n",
            "Epoch [26/100], Step [701/936], Loss: 8464.3135\n",
            "Epoch [26/100], Step [801/936], Loss: 9029.8164\n",
            "Epoch [26/100], Step [901/936], Loss: 8831.3945\n",
            "Epoch [27/100], Step [1/936], Loss: 8794.8438\n",
            "Epoch [27/100], Step [101/936], Loss: 8670.7168\n",
            "Epoch [27/100], Step [201/936], Loss: 8959.4150\n",
            "Epoch [27/100], Step [301/936], Loss: 8763.5137\n",
            "Epoch [27/100], Step [401/936], Loss: 9278.4346\n",
            "Epoch [27/100], Step [501/936], Loss: 8733.4482\n",
            "Epoch [27/100], Step [601/936], Loss: 8849.5801\n",
            "Epoch [27/100], Step [701/936], Loss: 9063.7607\n",
            "Epoch [27/100], Step [801/936], Loss: 9342.3906\n",
            "Epoch [27/100], Step [901/936], Loss: 8891.9941\n",
            "Epoch [28/100], Step [1/936], Loss: 9165.9297\n",
            "Epoch [28/100], Step [101/936], Loss: 9247.7559\n",
            "Epoch [28/100], Step [201/936], Loss: 8778.4756\n",
            "Epoch [28/100], Step [301/936], Loss: 8899.5459\n",
            "Epoch [28/100], Step [401/936], Loss: 9502.0479\n",
            "Epoch [28/100], Step [501/936], Loss: 8463.8457\n",
            "Epoch [28/100], Step [601/936], Loss: 8973.4951\n",
            "Epoch [28/100], Step [701/936], Loss: 9469.4883\n",
            "Epoch [28/100], Step [801/936], Loss: 9366.5869\n",
            "Epoch [28/100], Step [901/936], Loss: 8868.5479\n",
            "Epoch [29/100], Step [1/936], Loss: 8867.1025\n",
            "Epoch [29/100], Step [101/936], Loss: 8500.5791\n",
            "Epoch [29/100], Step [201/936], Loss: 8715.7480\n",
            "Epoch [29/100], Step [301/936], Loss: 8763.9590\n",
            "Epoch [29/100], Step [401/936], Loss: 9271.5117\n",
            "Epoch [29/100], Step [501/936], Loss: 8424.0889\n",
            "Epoch [29/100], Step [601/936], Loss: 8413.7197\n",
            "Epoch [29/100], Step [701/936], Loss: 8621.8115\n",
            "Epoch [29/100], Step [801/936], Loss: 9342.1572\n",
            "Epoch [29/100], Step [901/936], Loss: 8683.6377\n",
            "Epoch [30/100], Step [1/936], Loss: 9344.1328\n",
            "Epoch [30/100], Step [101/936], Loss: 8904.0605\n",
            "Epoch [30/100], Step [201/936], Loss: 9140.6504\n",
            "Epoch [30/100], Step [301/936], Loss: 8998.8037\n",
            "Epoch [30/100], Step [401/936], Loss: 8853.6973\n",
            "Epoch [30/100], Step [501/936], Loss: 9213.4336\n",
            "Epoch [30/100], Step [601/936], Loss: 8783.7246\n",
            "Epoch [30/100], Step [701/936], Loss: 9184.4688\n",
            "Epoch [30/100], Step [801/936], Loss: 8437.1309\n",
            "Epoch [30/100], Step [901/936], Loss: 8709.7656\n",
            "Epoch [31/100], Step [1/936], Loss: 9315.2871\n",
            "Epoch [31/100], Step [101/936], Loss: 9155.4297\n",
            "Epoch [31/100], Step [201/936], Loss: 8641.5176\n",
            "Epoch [31/100], Step [301/936], Loss: 9203.9551\n",
            "Epoch [31/100], Step [401/936], Loss: 8779.7871\n",
            "Epoch [31/100], Step [501/936], Loss: 9752.9688\n",
            "Epoch [31/100], Step [601/936], Loss: 9180.0469\n",
            "Epoch [31/100], Step [701/936], Loss: 9378.8340\n",
            "Epoch [31/100], Step [801/936], Loss: 9325.9150\n",
            "Epoch [31/100], Step [901/936], Loss: 8863.6055\n",
            "Epoch [32/100], Step [1/936], Loss: 9369.9072\n",
            "Epoch [32/100], Step [101/936], Loss: 9027.4336\n",
            "Epoch [32/100], Step [201/936], Loss: 9400.7637\n",
            "Epoch [32/100], Step [301/936], Loss: 8769.3633\n",
            "Epoch [32/100], Step [401/936], Loss: 9316.2422\n",
            "Epoch [32/100], Step [501/936], Loss: 9458.1221\n",
            "Epoch [32/100], Step [601/936], Loss: 8542.3877\n",
            "Epoch [32/100], Step [701/936], Loss: 8929.8965\n",
            "Epoch [32/100], Step [801/936], Loss: 9313.7568\n",
            "Epoch [32/100], Step [901/936], Loss: 8880.2490\n",
            "Epoch [33/100], Step [1/936], Loss: 8666.2676\n",
            "Epoch [33/100], Step [101/936], Loss: 8412.1162\n",
            "Epoch [33/100], Step [201/936], Loss: 8596.4893\n",
            "Epoch [33/100], Step [301/936], Loss: 8800.1396\n",
            "Epoch [33/100], Step [401/936], Loss: 9054.3311\n",
            "Epoch [33/100], Step [501/936], Loss: 8364.5039\n",
            "Epoch [33/100], Step [601/936], Loss: 9390.4189\n",
            "Epoch [33/100], Step [701/936], Loss: 9081.3975\n",
            "Epoch [33/100], Step [801/936], Loss: 8865.3008\n",
            "Epoch [33/100], Step [901/936], Loss: 9457.9033\n",
            "Epoch [34/100], Step [1/936], Loss: 8964.2783\n",
            "Epoch [34/100], Step [101/936], Loss: 8814.1816\n",
            "Epoch [34/100], Step [201/936], Loss: 8327.1992\n",
            "Epoch [34/100], Step [301/936], Loss: 8761.5342\n",
            "Epoch [34/100], Step [401/936], Loss: 8806.2178\n",
            "Epoch [34/100], Step [501/936], Loss: 8904.0957\n",
            "Epoch [34/100], Step [601/936], Loss: 9310.2627\n",
            "Epoch [34/100], Step [701/936], Loss: 9190.5742\n",
            "Epoch [34/100], Step [801/936], Loss: 9246.1113\n",
            "Epoch [34/100], Step [901/936], Loss: 9126.1787\n",
            "Epoch [35/100], Step [1/936], Loss: 8672.5352\n",
            "Epoch [35/100], Step [101/936], Loss: 8466.3789\n",
            "Epoch [35/100], Step [201/936], Loss: 8972.7148\n",
            "Epoch [35/100], Step [301/936], Loss: 9174.3271\n",
            "Epoch [35/100], Step [401/936], Loss: 8797.0635\n",
            "Epoch [35/100], Step [501/936], Loss: 9519.8740\n",
            "Epoch [35/100], Step [601/936], Loss: 9204.5967\n",
            "Epoch [35/100], Step [701/936], Loss: 8973.4980\n",
            "Epoch [35/100], Step [801/936], Loss: 8723.1680\n",
            "Epoch [35/100], Step [901/936], Loss: 8456.7246\n",
            "Epoch [36/100], Step [1/936], Loss: 8886.7480\n",
            "Epoch [36/100], Step [101/936], Loss: 8864.7207\n",
            "Epoch [36/100], Step [201/936], Loss: 8677.7090\n",
            "Epoch [36/100], Step [301/936], Loss: 9075.9209\n",
            "Epoch [36/100], Step [401/936], Loss: 9223.0068\n",
            "Epoch [36/100], Step [501/936], Loss: 9504.0176\n",
            "Epoch [36/100], Step [601/936], Loss: 8994.1523\n",
            "Epoch [36/100], Step [701/936], Loss: 8607.0781\n",
            "Epoch [36/100], Step [801/936], Loss: 9122.8008\n",
            "Epoch [36/100], Step [901/936], Loss: 9141.0674\n",
            "Epoch [37/100], Step [1/936], Loss: 9033.5146\n",
            "Epoch [37/100], Step [101/936], Loss: 8784.2959\n",
            "Epoch [37/100], Step [201/936], Loss: 9022.7656\n",
            "Epoch [37/100], Step [301/936], Loss: 8661.2803\n",
            "Epoch [37/100], Step [401/936], Loss: 8636.9951\n",
            "Epoch [37/100], Step [501/936], Loss: 9258.3086\n",
            "Epoch [37/100], Step [601/936], Loss: 9008.1631\n",
            "Epoch [37/100], Step [701/936], Loss: 8792.2041\n",
            "Epoch [37/100], Step [801/936], Loss: 9318.5801\n",
            "Epoch [37/100], Step [901/936], Loss: 8880.6523\n",
            "Epoch [38/100], Step [1/936], Loss: 9045.0889\n",
            "Epoch [38/100], Step [101/936], Loss: 9077.9912\n",
            "Epoch [38/100], Step [201/936], Loss: 8816.1875\n",
            "Epoch [38/100], Step [301/936], Loss: 9159.2803\n",
            "Epoch [38/100], Step [401/936], Loss: 8767.3604\n",
            "Epoch [38/100], Step [501/936], Loss: 8736.9355\n",
            "Epoch [38/100], Step [601/936], Loss: 8918.2539\n",
            "Epoch [38/100], Step [701/936], Loss: 9130.1279\n",
            "Epoch [38/100], Step [801/936], Loss: 9250.5791\n",
            "Epoch [38/100], Step [901/936], Loss: 8898.1270\n",
            "Epoch [39/100], Step [1/936], Loss: 9258.1816\n",
            "Epoch [39/100], Step [101/936], Loss: 9296.0283\n",
            "Epoch [39/100], Step [201/936], Loss: 8387.4395\n",
            "Epoch [39/100], Step [301/936], Loss: 8925.4951\n",
            "Epoch [39/100], Step [401/936], Loss: 8587.8076\n",
            "Epoch [39/100], Step [501/936], Loss: 8636.3613\n",
            "Epoch [39/100], Step [601/936], Loss: 9435.2461\n",
            "Epoch [39/100], Step [701/936], Loss: 8670.0732\n",
            "Epoch [39/100], Step [801/936], Loss: 8971.8125\n",
            "Epoch [39/100], Step [901/936], Loss: 8377.5869\n",
            "Epoch [40/100], Step [1/936], Loss: 8897.3311\n",
            "Epoch [40/100], Step [101/936], Loss: 8709.4180\n",
            "Epoch [40/100], Step [201/936], Loss: 9276.6260\n",
            "Epoch [40/100], Step [301/936], Loss: 8769.4922\n",
            "Epoch [40/100], Step [401/936], Loss: 8750.8936\n",
            "Epoch [40/100], Step [501/936], Loss: 8678.0215\n",
            "Epoch [40/100], Step [601/936], Loss: 9084.9561\n",
            "Epoch [40/100], Step [701/936], Loss: 8892.0205\n",
            "Epoch [40/100], Step [801/936], Loss: 8790.1016\n",
            "Epoch [40/100], Step [901/936], Loss: 9265.4746\n",
            "Epoch [41/100], Step [1/936], Loss: 9083.1602\n",
            "Epoch [41/100], Step [101/936], Loss: 9067.4199\n",
            "Epoch [41/100], Step [201/936], Loss: 8417.6846\n",
            "Epoch [41/100], Step [301/936], Loss: 9076.5654\n",
            "Epoch [41/100], Step [401/936], Loss: 8883.7031\n",
            "Epoch [41/100], Step [501/936], Loss: 9051.1504\n",
            "Epoch [41/100], Step [601/936], Loss: 8814.6992\n",
            "Epoch [41/100], Step [701/936], Loss: 9365.2754\n",
            "Epoch [41/100], Step [801/936], Loss: 8862.7061\n",
            "Epoch [41/100], Step [901/936], Loss: 9398.4570\n",
            "Epoch [42/100], Step [1/936], Loss: 8835.6924\n",
            "Epoch [42/100], Step [101/936], Loss: 8872.3398\n",
            "Epoch [42/100], Step [201/936], Loss: 9362.1621\n",
            "Epoch [42/100], Step [301/936], Loss: 8248.7109\n",
            "Epoch [42/100], Step [401/936], Loss: 8925.2969\n",
            "Epoch [42/100], Step [501/936], Loss: 9373.6689\n",
            "Epoch [42/100], Step [601/936], Loss: 9061.4805\n",
            "Epoch [42/100], Step [701/936], Loss: 8817.9609\n",
            "Epoch [42/100], Step [801/936], Loss: 8784.8809\n",
            "Epoch [42/100], Step [901/936], Loss: 8949.6133\n",
            "Epoch [43/100], Step [1/936], Loss: 8842.9854\n",
            "Epoch [43/100], Step [101/936], Loss: 8780.1621\n",
            "Epoch [43/100], Step [201/936], Loss: 8818.6445\n",
            "Epoch [43/100], Step [301/936], Loss: 8576.0596\n",
            "Epoch [43/100], Step [401/936], Loss: 9283.1230\n",
            "Epoch [43/100], Step [501/936], Loss: 8714.3691\n",
            "Epoch [43/100], Step [601/936], Loss: 9465.5449\n",
            "Epoch [43/100], Step [701/936], Loss: 9383.2266\n",
            "Epoch [43/100], Step [801/936], Loss: 9136.4053\n",
            "Epoch [43/100], Step [901/936], Loss: 9095.9727\n",
            "Epoch [44/100], Step [1/936], Loss: 8574.7412\n",
            "Epoch [44/100], Step [101/936], Loss: 8673.4189\n",
            "Epoch [44/100], Step [201/936], Loss: 8707.3887\n",
            "Epoch [44/100], Step [301/936], Loss: 8302.2832\n",
            "Epoch [44/100], Step [401/936], Loss: 9084.2676\n",
            "Epoch [44/100], Step [501/936], Loss: 8763.3428\n",
            "Epoch [44/100], Step [601/936], Loss: 8579.2539\n",
            "Epoch [44/100], Step [701/936], Loss: 8785.6641\n",
            "Epoch [44/100], Step [801/936], Loss: 8995.3496\n",
            "Epoch [44/100], Step [901/936], Loss: 8689.5361\n",
            "Epoch [45/100], Step [1/936], Loss: 8637.4648\n",
            "Epoch [45/100], Step [101/936], Loss: 8971.0781\n",
            "Epoch [45/100], Step [201/936], Loss: 8973.6875\n",
            "Epoch [45/100], Step [301/936], Loss: 9383.2012\n",
            "Epoch [45/100], Step [401/936], Loss: 9106.4551\n",
            "Epoch [45/100], Step [501/936], Loss: 8577.1436\n",
            "Epoch [45/100], Step [601/936], Loss: 8450.2354\n",
            "Epoch [45/100], Step [701/936], Loss: 8953.6660\n",
            "Epoch [45/100], Step [801/936], Loss: 8955.5117\n",
            "Epoch [45/100], Step [901/936], Loss: 9109.8838\n",
            "Epoch [46/100], Step [1/936], Loss: 8547.9336\n",
            "Epoch [46/100], Step [101/936], Loss: 8529.3301\n",
            "Epoch [46/100], Step [201/936], Loss: 8794.8477\n",
            "Epoch [46/100], Step [301/936], Loss: 8546.8047\n",
            "Epoch [46/100], Step [401/936], Loss: 9199.2188\n",
            "Epoch [46/100], Step [501/936], Loss: 8812.0186\n",
            "Epoch [46/100], Step [601/936], Loss: 8542.6807\n",
            "Epoch [46/100], Step [701/936], Loss: 8953.2617\n",
            "Epoch [46/100], Step [801/936], Loss: 9082.4395\n",
            "Epoch [46/100], Step [901/936], Loss: 8474.0928\n",
            "Epoch [47/100], Step [1/936], Loss: 8849.9072\n",
            "Epoch [47/100], Step [101/936], Loss: 8882.7754\n",
            "Epoch [47/100], Step [201/936], Loss: 9297.5576\n",
            "Epoch [47/100], Step [301/936], Loss: 9271.4248\n",
            "Epoch [47/100], Step [401/936], Loss: 8879.5107\n",
            "Epoch [47/100], Step [501/936], Loss: 8982.1074\n",
            "Epoch [47/100], Step [601/936], Loss: 8806.1270\n",
            "Epoch [47/100], Step [701/936], Loss: 9589.2041\n",
            "Epoch [47/100], Step [801/936], Loss: 8672.1250\n",
            "Epoch [47/100], Step [901/936], Loss: 9238.5391\n",
            "Epoch [48/100], Step [1/936], Loss: 8457.2461\n",
            "Epoch [48/100], Step [101/936], Loss: 8622.7207\n",
            "Epoch [48/100], Step [201/936], Loss: 8565.0742\n",
            "Epoch [48/100], Step [301/936], Loss: 9103.7354\n",
            "Epoch [48/100], Step [401/936], Loss: 8963.5518\n",
            "Epoch [48/100], Step [501/936], Loss: 8939.4844\n",
            "Epoch [48/100], Step [601/936], Loss: 9009.6885\n",
            "Epoch [48/100], Step [701/936], Loss: 8684.8086\n",
            "Epoch [48/100], Step [801/936], Loss: 8740.3750\n",
            "Epoch [48/100], Step [901/936], Loss: 8994.5918\n",
            "Epoch [49/100], Step [1/936], Loss: 9004.9033\n",
            "Epoch [49/100], Step [101/936], Loss: 9186.9326\n",
            "Epoch [49/100], Step [201/936], Loss: 8863.0498\n",
            "Epoch [49/100], Step [301/936], Loss: 9005.8818\n",
            "Epoch [49/100], Step [401/936], Loss: 8894.4277\n",
            "Epoch [49/100], Step [501/936], Loss: 8560.9023\n",
            "Epoch [49/100], Step [601/936], Loss: 8434.8604\n",
            "Epoch [49/100], Step [701/936], Loss: 8703.8760\n",
            "Epoch [49/100], Step [801/936], Loss: 8733.2793\n",
            "Epoch [49/100], Step [901/936], Loss: 8924.1758\n",
            "Epoch [50/100], Step [1/936], Loss: 9179.2129\n",
            "Epoch [50/100], Step [101/936], Loss: 8399.6201\n",
            "Epoch [50/100], Step [201/936], Loss: 8708.9561\n",
            "Epoch [50/100], Step [301/936], Loss: 9066.8203\n",
            "Epoch [50/100], Step [401/936], Loss: 8889.3350\n",
            "Epoch [50/100], Step [501/936], Loss: 9105.5840\n",
            "Epoch [50/100], Step [601/936], Loss: 8843.4766\n",
            "Epoch [50/100], Step [701/936], Loss: 8601.0625\n",
            "Epoch [50/100], Step [801/936], Loss: 8665.2100\n",
            "Epoch [50/100], Step [901/936], Loss: 8787.9141\n",
            "Epoch [51/100], Step [1/936], Loss: 9015.1924\n",
            "Epoch [51/100], Step [101/936], Loss: 9080.7070\n",
            "Epoch [51/100], Step [201/936], Loss: 9282.9541\n",
            "Epoch [51/100], Step [301/936], Loss: 8762.3555\n",
            "Epoch [51/100], Step [401/936], Loss: 8027.6675\n",
            "Epoch [51/100], Step [501/936], Loss: 9321.3672\n",
            "Epoch [51/100], Step [601/936], Loss: 8803.9258\n",
            "Epoch [51/100], Step [701/936], Loss: 9155.4023\n",
            "Epoch [51/100], Step [801/936], Loss: 8746.0645\n",
            "Epoch [51/100], Step [901/936], Loss: 8849.1064\n",
            "Epoch [52/100], Step [1/936], Loss: 8714.6855\n",
            "Epoch [52/100], Step [101/936], Loss: 8951.9336\n",
            "Epoch [52/100], Step [201/936], Loss: 8963.6436\n",
            "Epoch [52/100], Step [301/936], Loss: 8764.3105\n",
            "Epoch [52/100], Step [401/936], Loss: 9299.9395\n",
            "Epoch [52/100], Step [501/936], Loss: 9117.8672\n",
            "Epoch [52/100], Step [601/936], Loss: 9015.0391\n",
            "Epoch [52/100], Step [701/936], Loss: 9100.1299\n",
            "Epoch [52/100], Step [801/936], Loss: 9038.9668\n",
            "Epoch [52/100], Step [901/936], Loss: 8707.8232\n",
            "Epoch [53/100], Step [1/936], Loss: 9167.0859\n",
            "Epoch [53/100], Step [101/936], Loss: 8569.6602\n",
            "Epoch [53/100], Step [201/936], Loss: 8672.3691\n",
            "Epoch [53/100], Step [301/936], Loss: 8913.4316\n",
            "Epoch [53/100], Step [401/936], Loss: 8763.8252\n",
            "Epoch [53/100], Step [501/936], Loss: 8902.5137\n",
            "Epoch [53/100], Step [601/936], Loss: 9082.6934\n",
            "Epoch [53/100], Step [701/936], Loss: 8620.6914\n",
            "Epoch [53/100], Step [801/936], Loss: 8910.8232\n",
            "Epoch [53/100], Step [901/936], Loss: 9074.5488\n",
            "Epoch [54/100], Step [1/936], Loss: 9257.1953\n",
            "Epoch [54/100], Step [101/936], Loss: 8724.1836\n",
            "Epoch [54/100], Step [201/936], Loss: 8452.8203\n",
            "Epoch [54/100], Step [301/936], Loss: 8864.4805\n",
            "Epoch [54/100], Step [401/936], Loss: 9006.2422\n",
            "Epoch [54/100], Step [501/936], Loss: 8899.2910\n",
            "Epoch [54/100], Step [601/936], Loss: 9371.6182\n",
            "Epoch [54/100], Step [701/936], Loss: 9221.5234\n",
            "Epoch [54/100], Step [801/936], Loss: 8770.3730\n",
            "Epoch [54/100], Step [901/936], Loss: 9025.8105\n",
            "Epoch [55/100], Step [1/936], Loss: 8947.0234\n",
            "Epoch [55/100], Step [101/936], Loss: 9027.8057\n",
            "Epoch [55/100], Step [201/936], Loss: 9150.4385\n",
            "Epoch [55/100], Step [301/936], Loss: 8366.6016\n",
            "Epoch [55/100], Step [401/936], Loss: 8471.8047\n",
            "Epoch [55/100], Step [501/936], Loss: 8719.2334\n",
            "Epoch [55/100], Step [601/936], Loss: 8495.3887\n",
            "Epoch [55/100], Step [701/936], Loss: 9210.9736\n",
            "Epoch [55/100], Step [801/936], Loss: 8679.7529\n",
            "Epoch [55/100], Step [901/936], Loss: 9088.5547\n",
            "Epoch [56/100], Step [1/936], Loss: 8649.3770\n",
            "Epoch [56/100], Step [101/936], Loss: 8488.9580\n",
            "Epoch [56/100], Step [201/936], Loss: 8590.8828\n",
            "Epoch [56/100], Step [301/936], Loss: 8550.9023\n",
            "Epoch [56/100], Step [401/936], Loss: 8567.1396\n",
            "Epoch [56/100], Step [501/936], Loss: 8893.3291\n",
            "Epoch [56/100], Step [601/936], Loss: 8480.8643\n",
            "Epoch [56/100], Step [701/936], Loss: 8580.0879\n",
            "Epoch [56/100], Step [801/936], Loss: 8692.1094\n",
            "Epoch [56/100], Step [901/936], Loss: 9027.6279\n",
            "Epoch [57/100], Step [1/936], Loss: 7881.5938\n",
            "Epoch [57/100], Step [101/936], Loss: 8439.1465\n",
            "Epoch [57/100], Step [201/936], Loss: 8902.6396\n",
            "Epoch [57/100], Step [301/936], Loss: 9057.4873\n",
            "Epoch [57/100], Step [401/936], Loss: 8888.0547\n",
            "Epoch [57/100], Step [501/936], Loss: 9450.2168\n",
            "Epoch [57/100], Step [601/936], Loss: 8716.5029\n",
            "Epoch [57/100], Step [701/936], Loss: 9434.5869\n",
            "Epoch [57/100], Step [801/936], Loss: 8217.5928\n",
            "Epoch [57/100], Step [901/936], Loss: 8745.7520\n",
            "Epoch [58/100], Step [1/936], Loss: 8906.3057\n",
            "Epoch [58/100], Step [101/936], Loss: 8720.8613\n",
            "Epoch [58/100], Step [201/936], Loss: 9100.3369\n",
            "Epoch [58/100], Step [301/936], Loss: 8956.6045\n",
            "Epoch [58/100], Step [401/936], Loss: 8503.4375\n",
            "Epoch [58/100], Step [501/936], Loss: 8643.4590\n",
            "Epoch [58/100], Step [601/936], Loss: 8708.4766\n",
            "Epoch [58/100], Step [701/936], Loss: 8291.0420\n",
            "Epoch [58/100], Step [801/936], Loss: 8927.7236\n",
            "Epoch [58/100], Step [901/936], Loss: 8712.7422\n",
            "Epoch [59/100], Step [1/936], Loss: 8059.6465\n",
            "Epoch [59/100], Step [101/936], Loss: 8726.0078\n",
            "Epoch [59/100], Step [201/936], Loss: 9069.4014\n",
            "Epoch [59/100], Step [301/936], Loss: 9349.3828\n",
            "Epoch [59/100], Step [401/936], Loss: 8486.2197\n",
            "Epoch [59/100], Step [501/936], Loss: 9035.8984\n",
            "Epoch [59/100], Step [601/936], Loss: 9073.9697\n",
            "Epoch [59/100], Step [701/936], Loss: 9263.9424\n",
            "Epoch [59/100], Step [801/936], Loss: 8989.7773\n",
            "Epoch [59/100], Step [901/936], Loss: 8738.4814\n",
            "Epoch [60/100], Step [1/936], Loss: 8510.0820\n",
            "Epoch [60/100], Step [101/936], Loss: 9420.3262\n",
            "Epoch [60/100], Step [201/936], Loss: 8964.1035\n",
            "Epoch [60/100], Step [301/936], Loss: 8680.7021\n",
            "Epoch [60/100], Step [401/936], Loss: 8934.3896\n",
            "Epoch [60/100], Step [501/936], Loss: 8732.2559\n",
            "Epoch [60/100], Step [601/936], Loss: 8755.2637\n",
            "Epoch [60/100], Step [701/936], Loss: 8652.1787\n",
            "Epoch [60/100], Step [801/936], Loss: 8820.4551\n",
            "Epoch [60/100], Step [901/936], Loss: 8459.1113\n",
            "Epoch [61/100], Step [1/936], Loss: 8699.4258\n",
            "Epoch [61/100], Step [101/936], Loss: 8521.4795\n",
            "Epoch [61/100], Step [201/936], Loss: 8276.8223\n",
            "Epoch [61/100], Step [301/936], Loss: 8859.9160\n",
            "Epoch [61/100], Step [401/936], Loss: 8880.5527\n",
            "Epoch [61/100], Step [501/936], Loss: 8539.7188\n",
            "Epoch [61/100], Step [601/936], Loss: 8565.3623\n",
            "Epoch [61/100], Step [701/936], Loss: 8656.1084\n",
            "Epoch [61/100], Step [801/936], Loss: 9008.1680\n",
            "Epoch [61/100], Step [901/936], Loss: 8723.8662\n",
            "Epoch [62/100], Step [1/936], Loss: 8680.2158\n",
            "Epoch [62/100], Step [101/936], Loss: 8542.4219\n",
            "Epoch [62/100], Step [201/936], Loss: 8341.9336\n",
            "Epoch [62/100], Step [301/936], Loss: 8335.4990\n",
            "Epoch [62/100], Step [401/936], Loss: 8586.5986\n",
            "Epoch [62/100], Step [501/936], Loss: 9042.0977\n",
            "Epoch [62/100], Step [601/936], Loss: 8860.4219\n",
            "Epoch [62/100], Step [701/936], Loss: 8395.3926\n",
            "Epoch [62/100], Step [801/936], Loss: 8892.8555\n",
            "Epoch [62/100], Step [901/936], Loss: 8689.8066\n",
            "Epoch [63/100], Step [1/936], Loss: 9141.0049\n",
            "Epoch [63/100], Step [101/936], Loss: 8960.0518\n",
            "Epoch [63/100], Step [201/936], Loss: 8625.0264\n",
            "Epoch [63/100], Step [301/936], Loss: 9119.2363\n",
            "Epoch [63/100], Step [401/936], Loss: 8572.7646\n",
            "Epoch [63/100], Step [501/936], Loss: 8990.9795\n",
            "Epoch [63/100], Step [601/936], Loss: 8439.4395\n",
            "Epoch [63/100], Step [701/936], Loss: 8443.4727\n",
            "Epoch [63/100], Step [801/936], Loss: 8848.9512\n",
            "Epoch [63/100], Step [901/936], Loss: 9299.1836\n",
            "Epoch [64/100], Step [1/936], Loss: 8473.0137\n",
            "Epoch [64/100], Step [101/936], Loss: 8722.3223\n",
            "Epoch [64/100], Step [201/936], Loss: 8946.4658\n",
            "Epoch [64/100], Step [301/936], Loss: 8632.2568\n",
            "Epoch [64/100], Step [401/936], Loss: 9003.8770\n",
            "Epoch [64/100], Step [501/936], Loss: 8478.7090\n",
            "Epoch [64/100], Step [601/936], Loss: 8915.0342\n",
            "Epoch [64/100], Step [701/936], Loss: 8935.5283\n",
            "Epoch [64/100], Step [801/936], Loss: 8524.2979\n",
            "Epoch [64/100], Step [901/936], Loss: 8912.7314\n",
            "Epoch [65/100], Step [1/936], Loss: 8384.8564\n",
            "Epoch [65/100], Step [101/936], Loss: 8676.4355\n",
            "Epoch [65/100], Step [201/936], Loss: 8752.0605\n",
            "Epoch [65/100], Step [301/936], Loss: 9117.6377\n",
            "Epoch [65/100], Step [401/936], Loss: 8927.4688\n",
            "Epoch [65/100], Step [501/936], Loss: 8464.9873\n",
            "Epoch [65/100], Step [601/936], Loss: 8581.7529\n",
            "Epoch [65/100], Step [701/936], Loss: 9133.5068\n",
            "Epoch [65/100], Step [801/936], Loss: 8742.4961\n",
            "Epoch [65/100], Step [901/936], Loss: 8838.2803\n",
            "Epoch [66/100], Step [1/936], Loss: 8060.9844\n",
            "Epoch [66/100], Step [101/936], Loss: 8370.5234\n",
            "Epoch [66/100], Step [201/936], Loss: 8437.6963\n",
            "Epoch [66/100], Step [301/936], Loss: 8857.4111\n",
            "Epoch [66/100], Step [401/936], Loss: 8770.3281\n",
            "Epoch [66/100], Step [501/936], Loss: 9192.3438\n",
            "Epoch [66/100], Step [601/936], Loss: 8809.5361\n",
            "Epoch [66/100], Step [701/936], Loss: 8839.2139\n",
            "Epoch [66/100], Step [801/936], Loss: 8718.9395\n",
            "Epoch [66/100], Step [901/936], Loss: 8854.5000\n",
            "Epoch [67/100], Step [1/936], Loss: 8821.2451\n",
            "Epoch [67/100], Step [101/936], Loss: 8655.1660\n",
            "Epoch [67/100], Step [201/936], Loss: 8551.9863\n",
            "Epoch [67/100], Step [301/936], Loss: 8767.8721\n",
            "Epoch [67/100], Step [401/936], Loss: 8851.5068\n",
            "Epoch [67/100], Step [501/936], Loss: 8871.6875\n",
            "Epoch [67/100], Step [601/936], Loss: 8937.6064\n",
            "Epoch [67/100], Step [701/936], Loss: 8563.5244\n",
            "Epoch [67/100], Step [801/936], Loss: 9338.9531\n",
            "Epoch [67/100], Step [901/936], Loss: 8900.0498\n",
            "Epoch [68/100], Step [1/936], Loss: 8160.5264\n",
            "Epoch [68/100], Step [101/936], Loss: 8550.6191\n",
            "Epoch [68/100], Step [201/936], Loss: 8999.5723\n",
            "Epoch [68/100], Step [301/936], Loss: 8781.9844\n",
            "Epoch [68/100], Step [401/936], Loss: 8926.5615\n",
            "Epoch [68/100], Step [501/936], Loss: 8149.8408\n",
            "Epoch [68/100], Step [601/936], Loss: 8801.9326\n",
            "Epoch [68/100], Step [701/936], Loss: 8866.6621\n",
            "Epoch [68/100], Step [801/936], Loss: 8901.0469\n",
            "Epoch [68/100], Step [901/936], Loss: 8212.5996\n",
            "Epoch [69/100], Step [1/936], Loss: 9287.2949\n",
            "Epoch [69/100], Step [101/936], Loss: 8616.9072\n",
            "Epoch [69/100], Step [201/936], Loss: 8814.4385\n",
            "Epoch [69/100], Step [301/936], Loss: 8376.3984\n",
            "Epoch [69/100], Step [401/936], Loss: 9102.1650\n",
            "Epoch [69/100], Step [501/936], Loss: 9386.1641\n",
            "Epoch [69/100], Step [601/936], Loss: 8489.1816\n",
            "Epoch [69/100], Step [701/936], Loss: 8590.5342\n",
            "Epoch [69/100], Step [801/936], Loss: 8711.8281\n",
            "Epoch [69/100], Step [901/936], Loss: 8554.3770\n",
            "Epoch [70/100], Step [1/936], Loss: 8235.0107\n",
            "Epoch [70/100], Step [101/936], Loss: 8567.7607\n",
            "Epoch [70/100], Step [201/936], Loss: 8282.4902\n",
            "Epoch [70/100], Step [301/936], Loss: 8934.4336\n",
            "Epoch [70/100], Step [401/936], Loss: 8292.5557\n",
            "Epoch [70/100], Step [501/936], Loss: 8820.7773\n",
            "Epoch [70/100], Step [601/936], Loss: 9013.8340\n",
            "Epoch [70/100], Step [701/936], Loss: 8501.5518\n",
            "Epoch [70/100], Step [801/936], Loss: 9296.1846\n",
            "Epoch [70/100], Step [901/936], Loss: 9082.7051\n",
            "Epoch [71/100], Step [1/936], Loss: 8694.6172\n",
            "Epoch [71/100], Step [101/936], Loss: 8804.9219\n",
            "Epoch [71/100], Step [201/936], Loss: 8382.6836\n",
            "Epoch [71/100], Step [301/936], Loss: 8626.4453\n",
            "Epoch [71/100], Step [401/936], Loss: 8878.5967\n",
            "Epoch [71/100], Step [501/936], Loss: 8756.8018\n",
            "Epoch [71/100], Step [601/936], Loss: 9407.5645\n",
            "Epoch [71/100], Step [701/936], Loss: 8825.5918\n",
            "Epoch [71/100], Step [801/936], Loss: 8546.8359\n",
            "Epoch [71/100], Step [901/936], Loss: 8763.2363\n",
            "Epoch [72/100], Step [1/936], Loss: 8928.8926\n",
            "Epoch [72/100], Step [101/936], Loss: 8671.8213\n",
            "Epoch [72/100], Step [201/936], Loss: 9018.0664\n",
            "Epoch [72/100], Step [301/936], Loss: 9059.1328\n",
            "Epoch [72/100], Step [401/936], Loss: 9317.3105\n",
            "Epoch [72/100], Step [501/936], Loss: 8613.5391\n",
            "Epoch [72/100], Step [601/936], Loss: 9013.7363\n",
            "Epoch [72/100], Step [701/936], Loss: 8946.8359\n",
            "Epoch [72/100], Step [801/936], Loss: 8930.0283\n",
            "Epoch [72/100], Step [901/936], Loss: 8525.7910\n",
            "Epoch [73/100], Step [1/936], Loss: 8902.2480\n",
            "Epoch [73/100], Step [101/936], Loss: 8417.2305\n",
            "Epoch [73/100], Step [201/936], Loss: 8589.8887\n",
            "Epoch [73/100], Step [301/936], Loss: 9344.6055\n",
            "Epoch [73/100], Step [401/936], Loss: 8805.2148\n",
            "Epoch [73/100], Step [501/936], Loss: 8487.1562\n",
            "Epoch [73/100], Step [601/936], Loss: 8444.3604\n",
            "Epoch [73/100], Step [701/936], Loss: 8513.8330\n",
            "Epoch [73/100], Step [801/936], Loss: 8771.6914\n",
            "Epoch [73/100], Step [901/936], Loss: 8601.2559\n",
            "Epoch [74/100], Step [1/936], Loss: 8589.3066\n",
            "Epoch [74/100], Step [101/936], Loss: 8572.4502\n",
            "Epoch [74/100], Step [201/936], Loss: 8690.0547\n",
            "Epoch [74/100], Step [301/936], Loss: 8367.0488\n",
            "Epoch [74/100], Step [401/936], Loss: 8772.1523\n",
            "Epoch [74/100], Step [501/936], Loss: 8515.7539\n",
            "Epoch [74/100], Step [601/936], Loss: 8848.1807\n",
            "Epoch [74/100], Step [701/936], Loss: 8623.9590\n",
            "Epoch [74/100], Step [801/936], Loss: 8717.9609\n",
            "Epoch [74/100], Step [901/936], Loss: 8734.7725\n",
            "Epoch [75/100], Step [1/936], Loss: 8446.1729\n",
            "Epoch [75/100], Step [101/936], Loss: 8674.4834\n",
            "Epoch [75/100], Step [201/936], Loss: 8826.7363\n",
            "Epoch [75/100], Step [301/936], Loss: 9129.9580\n",
            "Epoch [75/100], Step [401/936], Loss: 8931.5566\n",
            "Epoch [75/100], Step [501/936], Loss: 9085.2324\n",
            "Epoch [75/100], Step [601/936], Loss: 8635.8711\n",
            "Epoch [75/100], Step [701/936], Loss: 9037.4023\n",
            "Epoch [75/100], Step [801/936], Loss: 8697.3604\n",
            "Epoch [75/100], Step [901/936], Loss: 8666.8174\n",
            "Epoch [76/100], Step [1/936], Loss: 8494.0869\n",
            "Epoch [76/100], Step [101/936], Loss: 8255.8633\n",
            "Epoch [76/100], Step [201/936], Loss: 8888.9023\n",
            "Epoch [76/100], Step [301/936], Loss: 8959.0176\n",
            "Epoch [76/100], Step [401/936], Loss: 9124.2383\n",
            "Epoch [76/100], Step [501/936], Loss: 8885.0410\n",
            "Epoch [76/100], Step [601/936], Loss: 8868.7393\n",
            "Epoch [76/100], Step [701/936], Loss: 9071.0723\n",
            "Epoch [76/100], Step [801/936], Loss: 8489.7178\n",
            "Epoch [76/100], Step [901/936], Loss: 8599.0957\n",
            "Epoch [77/100], Step [1/936], Loss: 8924.0869\n",
            "Epoch [77/100], Step [101/936], Loss: 8824.2734\n",
            "Epoch [77/100], Step [201/936], Loss: 9348.4492\n",
            "Epoch [77/100], Step [301/936], Loss: 9166.3242\n",
            "Epoch [77/100], Step [401/936], Loss: 8489.5391\n",
            "Epoch [77/100], Step [501/936], Loss: 8994.0850\n",
            "Epoch [77/100], Step [601/936], Loss: 8952.3984\n",
            "Epoch [77/100], Step [701/936], Loss: 8680.2217\n",
            "Epoch [77/100], Step [801/936], Loss: 8624.9375\n",
            "Epoch [77/100], Step [901/936], Loss: 8907.3975\n",
            "Epoch [78/100], Step [1/936], Loss: 9111.3984\n",
            "Epoch [78/100], Step [101/936], Loss: 8821.0430\n",
            "Epoch [78/100], Step [201/936], Loss: 8675.7070\n",
            "Epoch [78/100], Step [301/936], Loss: 8894.7109\n",
            "Epoch [78/100], Step [401/936], Loss: 8742.2900\n",
            "Epoch [78/100], Step [501/936], Loss: 8788.2295\n",
            "Epoch [78/100], Step [601/936], Loss: 8602.8066\n",
            "Epoch [78/100], Step [701/936], Loss: 8248.2402\n",
            "Epoch [78/100], Step [801/936], Loss: 8471.4824\n",
            "Epoch [78/100], Step [901/936], Loss: 8353.5273\n",
            "Epoch [79/100], Step [1/936], Loss: 8509.4805\n",
            "Epoch [79/100], Step [101/936], Loss: 8433.0322\n",
            "Epoch [79/100], Step [201/936], Loss: 9012.0869\n",
            "Epoch [79/100], Step [301/936], Loss: 8494.7734\n",
            "Epoch [79/100], Step [401/936], Loss: 8217.6963\n",
            "Epoch [79/100], Step [501/936], Loss: 8530.4600\n",
            "Epoch [79/100], Step [601/936], Loss: 8413.3047\n",
            "Epoch [79/100], Step [701/936], Loss: 8592.1074\n",
            "Epoch [79/100], Step [801/936], Loss: 8708.8623\n",
            "Epoch [79/100], Step [901/936], Loss: 8809.3838\n",
            "Epoch [80/100], Step [1/936], Loss: 8526.7598\n",
            "Epoch [80/100], Step [101/936], Loss: 9203.2236\n",
            "Epoch [80/100], Step [201/936], Loss: 8545.7812\n",
            "Epoch [80/100], Step [301/936], Loss: 8675.6738\n",
            "Epoch [80/100], Step [401/936], Loss: 8419.0625\n",
            "Epoch [80/100], Step [501/936], Loss: 8968.2539\n",
            "Epoch [80/100], Step [601/936], Loss: 9043.5742\n",
            "Epoch [80/100], Step [701/936], Loss: 9173.1172\n",
            "Epoch [80/100], Step [801/936], Loss: 8734.8438\n",
            "Epoch [80/100], Step [901/936], Loss: 8387.5693\n",
            "Epoch [81/100], Step [1/936], Loss: 8715.8066\n",
            "Epoch [81/100], Step [101/936], Loss: 8832.5215\n",
            "Epoch [81/100], Step [201/936], Loss: 9076.1152\n",
            "Epoch [81/100], Step [301/936], Loss: 8523.6934\n",
            "Epoch [81/100], Step [401/936], Loss: 9124.9463\n",
            "Epoch [81/100], Step [501/936], Loss: 8751.4639\n",
            "Epoch [81/100], Step [601/936], Loss: 8861.0225\n",
            "Epoch [81/100], Step [701/936], Loss: 8917.6719\n",
            "Epoch [81/100], Step [801/936], Loss: 9062.2715\n",
            "Epoch [81/100], Step [901/936], Loss: 8723.1582\n",
            "Epoch [82/100], Step [1/936], Loss: 8793.2871\n",
            "Epoch [82/100], Step [101/936], Loss: 8705.8750\n",
            "Epoch [82/100], Step [201/936], Loss: 8468.2441\n",
            "Epoch [82/100], Step [301/936], Loss: 8821.9277\n",
            "Epoch [82/100], Step [401/936], Loss: 8768.6377\n",
            "Epoch [82/100], Step [501/936], Loss: 8583.7842\n",
            "Epoch [82/100], Step [601/936], Loss: 8893.8662\n",
            "Epoch [82/100], Step [701/936], Loss: 8882.6074\n",
            "Epoch [82/100], Step [801/936], Loss: 8366.0947\n",
            "Epoch [82/100], Step [901/936], Loss: 9013.4434\n",
            "Epoch [83/100], Step [1/936], Loss: 9369.9248\n",
            "Epoch [83/100], Step [101/936], Loss: 8731.2910\n",
            "Epoch [83/100], Step [201/936], Loss: 8923.2051\n",
            "Epoch [83/100], Step [301/936], Loss: 8586.8711\n",
            "Epoch [83/100], Step [401/936], Loss: 8593.1602\n",
            "Epoch [83/100], Step [501/936], Loss: 9171.7119\n",
            "Epoch [83/100], Step [601/936], Loss: 8693.6367\n",
            "Epoch [83/100], Step [701/936], Loss: 8884.3984\n",
            "Epoch [83/100], Step [801/936], Loss: 8521.8691\n",
            "Epoch [83/100], Step [901/936], Loss: 8538.6182\n",
            "Epoch [84/100], Step [1/936], Loss: 8586.6768\n",
            "Epoch [84/100], Step [101/936], Loss: 8899.4072\n",
            "Epoch [84/100], Step [201/936], Loss: 8977.1514\n",
            "Epoch [84/100], Step [301/936], Loss: 9164.5820\n",
            "Epoch [84/100], Step [401/936], Loss: 8474.1719\n",
            "Epoch [84/100], Step [501/936], Loss: 8639.1094\n",
            "Epoch [84/100], Step [601/936], Loss: 8537.1436\n",
            "Epoch [84/100], Step [701/936], Loss: 9067.6973\n",
            "Epoch [84/100], Step [801/936], Loss: 8566.1943\n",
            "Epoch [84/100], Step [901/936], Loss: 8831.1592\n",
            "Epoch [85/100], Step [1/936], Loss: 8081.6748\n",
            "Epoch [85/100], Step [101/936], Loss: 9014.1855\n",
            "Epoch [85/100], Step [201/936], Loss: 8679.2949\n",
            "Epoch [85/100], Step [301/936], Loss: 8402.6338\n",
            "Epoch [85/100], Step [401/936], Loss: 8566.3496\n",
            "Epoch [85/100], Step [501/936], Loss: 8842.5986\n",
            "Epoch [85/100], Step [601/936], Loss: 8501.0859\n",
            "Epoch [85/100], Step [701/936], Loss: 9100.1465\n",
            "Epoch [85/100], Step [801/936], Loss: 8638.5107\n",
            "Epoch [85/100], Step [901/936], Loss: 8501.6641\n",
            "Epoch [86/100], Step [1/936], Loss: 8906.4492\n",
            "Epoch [86/100], Step [101/936], Loss: 8578.2295\n",
            "Epoch [86/100], Step [201/936], Loss: 8325.0508\n",
            "Epoch [86/100], Step [301/936], Loss: 8588.4072\n",
            "Epoch [86/100], Step [401/936], Loss: 8435.4014\n",
            "Epoch [86/100], Step [501/936], Loss: 8820.6719\n",
            "Epoch [86/100], Step [601/936], Loss: 9247.4473\n",
            "Epoch [86/100], Step [701/936], Loss: 9010.1797\n",
            "Epoch [86/100], Step [801/936], Loss: 8906.8145\n",
            "Epoch [86/100], Step [901/936], Loss: 8608.0098\n",
            "Epoch [87/100], Step [1/936], Loss: 8778.3145\n",
            "Epoch [87/100], Step [101/936], Loss: 8495.6211\n",
            "Epoch [87/100], Step [201/936], Loss: 8832.3340\n",
            "Epoch [87/100], Step [301/936], Loss: 8920.6045\n",
            "Epoch [87/100], Step [401/936], Loss: 9005.9307\n",
            "Epoch [87/100], Step [501/936], Loss: 8721.4902\n",
            "Epoch [87/100], Step [601/936], Loss: 8519.3477\n",
            "Epoch [87/100], Step [701/936], Loss: 9205.3115\n",
            "Epoch [87/100], Step [801/936], Loss: 8157.2456\n",
            "Epoch [87/100], Step [901/936], Loss: 8590.4824\n",
            "Epoch [88/100], Step [1/936], Loss: 8730.8613\n",
            "Epoch [88/100], Step [101/936], Loss: 8082.5811\n",
            "Epoch [88/100], Step [201/936], Loss: 9112.9404\n",
            "Epoch [88/100], Step [301/936], Loss: 8931.1895\n",
            "Epoch [88/100], Step [401/936], Loss: 9179.1777\n",
            "Epoch [88/100], Step [501/936], Loss: 9185.0527\n",
            "Epoch [88/100], Step [601/936], Loss: 8892.5186\n",
            "Epoch [88/100], Step [701/936], Loss: 8680.7627\n",
            "Epoch [88/100], Step [801/936], Loss: 9214.3604\n",
            "Epoch [88/100], Step [901/936], Loss: 8560.7510\n",
            "Epoch [89/100], Step [1/936], Loss: 9076.7793\n",
            "Epoch [89/100], Step [101/936], Loss: 8808.6895\n",
            "Epoch [89/100], Step [201/936], Loss: 8392.6865\n",
            "Epoch [89/100], Step [301/936], Loss: 8591.0078\n",
            "Epoch [89/100], Step [401/936], Loss: 9035.2109\n",
            "Epoch [89/100], Step [501/936], Loss: 9248.3281\n",
            "Epoch [89/100], Step [601/936], Loss: 8900.7549\n",
            "Epoch [89/100], Step [701/936], Loss: 8844.5205\n",
            "Epoch [89/100], Step [801/936], Loss: 8879.8096\n",
            "Epoch [89/100], Step [901/936], Loss: 8762.2500\n",
            "Epoch [90/100], Step [1/936], Loss: 8508.8799\n",
            "Epoch [90/100], Step [101/936], Loss: 8907.8350\n",
            "Epoch [90/100], Step [201/936], Loss: 8905.9209\n",
            "Epoch [90/100], Step [301/936], Loss: 8734.6094\n",
            "Epoch [90/100], Step [401/936], Loss: 8847.9014\n",
            "Epoch [90/100], Step [501/936], Loss: 8562.5908\n",
            "Epoch [90/100], Step [601/936], Loss: 8740.3135\n",
            "Epoch [90/100], Step [701/936], Loss: 9319.8066\n",
            "Epoch [90/100], Step [801/936], Loss: 8449.0283\n",
            "Epoch [90/100], Step [901/936], Loss: 8102.5479\n",
            "Epoch [91/100], Step [1/936], Loss: 9422.2236\n",
            "Epoch [91/100], Step [101/936], Loss: 8795.2539\n",
            "Epoch [91/100], Step [201/936], Loss: 8925.7764\n",
            "Epoch [91/100], Step [301/936], Loss: 9139.6094\n",
            "Epoch [91/100], Step [401/936], Loss: 8782.6230\n",
            "Epoch [91/100], Step [501/936], Loss: 8772.5957\n",
            "Epoch [91/100], Step [601/936], Loss: 9151.9453\n",
            "Epoch [91/100], Step [701/936], Loss: 8481.3193\n",
            "Epoch [91/100], Step [801/936], Loss: 8906.9951\n",
            "Epoch [91/100], Step [901/936], Loss: 8975.3242\n",
            "Epoch [92/100], Step [1/936], Loss: 7957.6904\n",
            "Epoch [92/100], Step [101/936], Loss: 8783.1211\n",
            "Epoch [92/100], Step [201/936], Loss: 8963.2207\n",
            "Epoch [92/100], Step [301/936], Loss: 8707.8555\n",
            "Epoch [92/100], Step [401/936], Loss: 8431.7393\n",
            "Epoch [92/100], Step [501/936], Loss: 8208.5986\n",
            "Epoch [92/100], Step [601/936], Loss: 9128.2930\n",
            "Epoch [92/100], Step [701/936], Loss: 9060.5068\n",
            "Epoch [92/100], Step [801/936], Loss: 8292.7100\n",
            "Epoch [92/100], Step [901/936], Loss: 8560.6777\n",
            "Epoch [93/100], Step [1/936], Loss: 9067.3398\n",
            "Epoch [93/100], Step [101/936], Loss: 8910.7920\n",
            "Epoch [93/100], Step [201/936], Loss: 8861.4922\n",
            "Epoch [93/100], Step [301/936], Loss: 8750.7969\n",
            "Epoch [93/100], Step [401/936], Loss: 8184.5039\n",
            "Epoch [93/100], Step [501/936], Loss: 8610.9453\n",
            "Epoch [93/100], Step [601/936], Loss: 8659.1260\n",
            "Epoch [93/100], Step [701/936], Loss: 8837.9521\n",
            "Epoch [93/100], Step [801/936], Loss: 8661.7832\n",
            "Epoch [93/100], Step [901/936], Loss: 8815.9570\n",
            "Epoch [94/100], Step [1/936], Loss: 9080.5869\n",
            "Epoch [94/100], Step [101/936], Loss: 8645.8779\n",
            "Epoch [94/100], Step [201/936], Loss: 8868.9492\n",
            "Epoch [94/100], Step [301/936], Loss: 8624.3105\n",
            "Epoch [94/100], Step [401/936], Loss: 8883.6680\n",
            "Epoch [94/100], Step [501/936], Loss: 8257.3174\n",
            "Epoch [94/100], Step [601/936], Loss: 8528.5869\n",
            "Epoch [94/100], Step [701/936], Loss: 8639.6445\n",
            "Epoch [94/100], Step [801/936], Loss: 8609.5605\n",
            "Epoch [94/100], Step [901/936], Loss: 8534.1807\n",
            "Epoch [95/100], Step [1/936], Loss: 8715.3125\n",
            "Epoch [95/100], Step [101/936], Loss: 8807.5957\n",
            "Epoch [95/100], Step [201/936], Loss: 9188.0566\n",
            "Epoch [95/100], Step [301/936], Loss: 8968.4814\n",
            "Epoch [95/100], Step [401/936], Loss: 8213.9785\n",
            "Epoch [95/100], Step [501/936], Loss: 8918.0654\n",
            "Epoch [95/100], Step [601/936], Loss: 9072.0684\n",
            "Epoch [95/100], Step [701/936], Loss: 8327.2471\n",
            "Epoch [95/100], Step [801/936], Loss: 8574.5527\n",
            "Epoch [95/100], Step [901/936], Loss: 8869.5693\n",
            "Epoch [96/100], Step [1/936], Loss: 9063.5781\n",
            "Epoch [96/100], Step [101/936], Loss: 9001.0264\n",
            "Epoch [96/100], Step [201/936], Loss: 8564.4834\n",
            "Epoch [96/100], Step [301/936], Loss: 8997.7861\n",
            "Epoch [96/100], Step [401/936], Loss: 8383.1934\n",
            "Epoch [96/100], Step [501/936], Loss: 8945.8271\n",
            "Epoch [96/100], Step [601/936], Loss: 8628.5732\n",
            "Epoch [96/100], Step [701/936], Loss: 8761.0654\n",
            "Epoch [96/100], Step [801/936], Loss: 9103.4824\n",
            "Epoch [96/100], Step [901/936], Loss: 8623.8623\n",
            "Epoch [97/100], Step [1/936], Loss: 9209.4707\n",
            "Epoch [97/100], Step [101/936], Loss: 8824.5957\n",
            "Epoch [97/100], Step [201/936], Loss: 8595.4121\n",
            "Epoch [97/100], Step [301/936], Loss: 8626.2930\n",
            "Epoch [97/100], Step [401/936], Loss: 8657.3926\n",
            "Epoch [97/100], Step [501/936], Loss: 9141.8809\n",
            "Epoch [97/100], Step [601/936], Loss: 8772.6260\n",
            "Epoch [97/100], Step [701/936], Loss: 9320.0215\n",
            "Epoch [97/100], Step [801/936], Loss: 8689.9316\n",
            "Epoch [97/100], Step [901/936], Loss: 8858.9658\n",
            "Epoch [98/100], Step [1/936], Loss: 8302.9219\n",
            "Epoch [98/100], Step [101/936], Loss: 8693.2227\n",
            "Epoch [98/100], Step [201/936], Loss: 8717.8193\n",
            "Epoch [98/100], Step [301/936], Loss: 8599.6367\n",
            "Epoch [98/100], Step [401/936], Loss: 8522.0078\n",
            "Epoch [98/100], Step [501/936], Loss: 8671.7725\n",
            "Epoch [98/100], Step [601/936], Loss: 8659.9160\n",
            "Epoch [98/100], Step [701/936], Loss: 8770.5869\n",
            "Epoch [98/100], Step [801/936], Loss: 8692.1260\n",
            "Epoch [98/100], Step [901/936], Loss: 8346.1475\n",
            "Epoch [99/100], Step [1/936], Loss: 9105.3486\n",
            "Epoch [99/100], Step [101/936], Loss: 8695.3086\n",
            "Epoch [99/100], Step [201/936], Loss: 8747.2305\n",
            "Epoch [99/100], Step [301/936], Loss: 8691.6865\n",
            "Epoch [99/100], Step [401/936], Loss: 8784.6963\n",
            "Epoch [99/100], Step [501/936], Loss: 8739.5410\n",
            "Epoch [99/100], Step [601/936], Loss: 8702.5381\n",
            "Epoch [99/100], Step [701/936], Loss: 9047.7598\n",
            "Epoch [99/100], Step [801/936], Loss: 8957.1016\n",
            "Epoch [99/100], Step [901/936], Loss: 9070.5312\n",
            "Epoch [100/100], Step [1/936], Loss: 9018.6904\n",
            "Epoch [100/100], Step [101/936], Loss: 8175.2598\n",
            "Epoch [100/100], Step [201/936], Loss: 9223.8428\n",
            "Epoch [100/100], Step [301/936], Loss: 9074.3281\n",
            "Epoch [100/100], Step [401/936], Loss: 9188.2676\n",
            "Epoch [100/100], Step [501/936], Loss: 8674.9180\n",
            "Epoch [100/100], Step [601/936], Loss: 8770.6895\n",
            "Epoch [100/100], Step [701/936], Loss: 8624.2295\n",
            "Epoch [100/100], Step [801/936], Loss: 8925.0049\n",
            "Epoch [100/100], Step [901/936], Loss: 8607.8174\n",
            "Accuracy of the SVM on the test images with 100 labeled images: 76.66%\n",
            "Epoch [1/100], Step [1/929], Loss: 38587.0234\n",
            "Epoch [1/100], Step [101/929], Loss: 13057.2168\n",
            "Epoch [1/100], Step [201/929], Loss: 12692.2305\n",
            "Epoch [1/100], Step [301/929], Loss: 12686.6592\n",
            "Epoch [1/100], Step [401/929], Loss: 12444.4102\n",
            "Epoch [1/100], Step [501/929], Loss: 11875.4004\n",
            "Epoch [1/100], Step [601/929], Loss: 11288.7979\n",
            "Epoch [1/100], Step [701/929], Loss: 11543.6377\n",
            "Epoch [1/100], Step [801/929], Loss: 11070.6963\n",
            "Epoch [1/100], Step [901/929], Loss: 11123.5107\n",
            "Epoch [2/100], Step [1/929], Loss: 10723.4170\n",
            "Epoch [2/100], Step [101/929], Loss: 11093.3408\n",
            "Epoch [2/100], Step [201/929], Loss: 11232.6963\n",
            "Epoch [2/100], Step [301/929], Loss: 10357.4932\n",
            "Epoch [2/100], Step [401/929], Loss: 10900.5156\n",
            "Epoch [2/100], Step [501/929], Loss: 11165.6533\n",
            "Epoch [2/100], Step [601/929], Loss: 10404.9814\n",
            "Epoch [2/100], Step [701/929], Loss: 9834.9922\n",
            "Epoch [2/100], Step [801/929], Loss: 10100.0479\n",
            "Epoch [2/100], Step [901/929], Loss: 10213.1777\n",
            "Epoch [3/100], Step [1/929], Loss: 10177.8486\n",
            "Epoch [3/100], Step [101/929], Loss: 10895.0156\n",
            "Epoch [3/100], Step [201/929], Loss: 10539.8105\n",
            "Epoch [3/100], Step [301/929], Loss: 10216.7705\n",
            "Epoch [3/100], Step [401/929], Loss: 10124.7324\n",
            "Epoch [3/100], Step [501/929], Loss: 10014.8047\n",
            "Epoch [3/100], Step [601/929], Loss: 10093.6602\n",
            "Epoch [3/100], Step [701/929], Loss: 10456.6504\n",
            "Epoch [3/100], Step [801/929], Loss: 9707.5498\n",
            "Epoch [3/100], Step [901/929], Loss: 9780.9688\n",
            "Epoch [4/100], Step [1/929], Loss: 9786.6621\n",
            "Epoch [4/100], Step [101/929], Loss: 10345.8057\n",
            "Epoch [4/100], Step [201/929], Loss: 9769.1914\n",
            "Epoch [4/100], Step [301/929], Loss: 9918.6016\n",
            "Epoch [4/100], Step [401/929], Loss: 9952.2266\n",
            "Epoch [4/100], Step [501/929], Loss: 9835.8477\n",
            "Epoch [4/100], Step [601/929], Loss: 10157.8359\n",
            "Epoch [4/100], Step [701/929], Loss: 10118.1699\n",
            "Epoch [4/100], Step [801/929], Loss: 9725.5264\n",
            "Epoch [4/100], Step [901/929], Loss: 10506.1113\n",
            "Epoch [5/100], Step [1/929], Loss: 9354.3486\n",
            "Epoch [5/100], Step [101/929], Loss: 10279.3594\n",
            "Epoch [5/100], Step [201/929], Loss: 9959.5938\n",
            "Epoch [5/100], Step [301/929], Loss: 10049.2549\n",
            "Epoch [5/100], Step [401/929], Loss: 9663.2812\n",
            "Epoch [5/100], Step [501/929], Loss: 9605.9043\n",
            "Epoch [5/100], Step [601/929], Loss: 9753.6338\n",
            "Epoch [5/100], Step [701/929], Loss: 9670.5098\n",
            "Epoch [5/100], Step [801/929], Loss: 9834.6201\n",
            "Epoch [5/100], Step [901/929], Loss: 9549.6836\n",
            "Epoch [6/100], Step [1/929], Loss: 9827.0566\n",
            "Epoch [6/100], Step [101/929], Loss: 9663.2344\n",
            "Epoch [6/100], Step [201/929], Loss: 9861.4131\n",
            "Epoch [6/100], Step [301/929], Loss: 10130.7236\n",
            "Epoch [6/100], Step [401/929], Loss: 10164.5312\n",
            "Epoch [6/100], Step [501/929], Loss: 9872.7832\n",
            "Epoch [6/100], Step [601/929], Loss: 9874.4551\n",
            "Epoch [6/100], Step [701/929], Loss: 9445.3545\n",
            "Epoch [6/100], Step [801/929], Loss: 9273.8174\n",
            "Epoch [6/100], Step [901/929], Loss: 10046.6113\n",
            "Epoch [7/100], Step [1/929], Loss: 10069.3887\n",
            "Epoch [7/100], Step [101/929], Loss: 9731.9551\n",
            "Epoch [7/100], Step [201/929], Loss: 10125.7305\n",
            "Epoch [7/100], Step [301/929], Loss: 9629.1758\n",
            "Epoch [7/100], Step [401/929], Loss: 9870.4141\n",
            "Epoch [7/100], Step [501/929], Loss: 9532.5254\n",
            "Epoch [7/100], Step [601/929], Loss: 9516.7119\n",
            "Epoch [7/100], Step [701/929], Loss: 9133.9453\n",
            "Epoch [7/100], Step [801/929], Loss: 9704.6299\n",
            "Epoch [7/100], Step [901/929], Loss: 9737.6953\n",
            "Epoch [8/100], Step [1/929], Loss: 9607.7061\n",
            "Epoch [8/100], Step [101/929], Loss: 9883.3809\n",
            "Epoch [8/100], Step [201/929], Loss: 9650.3135\n",
            "Epoch [8/100], Step [301/929], Loss: 9440.1592\n",
            "Epoch [8/100], Step [401/929], Loss: 9013.7910\n",
            "Epoch [8/100], Step [501/929], Loss: 9029.3984\n",
            "Epoch [8/100], Step [601/929], Loss: 9343.3066\n",
            "Epoch [8/100], Step [701/929], Loss: 9713.9805\n",
            "Epoch [8/100], Step [801/929], Loss: 9575.8945\n",
            "Epoch [8/100], Step [901/929], Loss: 9130.9238\n",
            "Epoch [9/100], Step [1/929], Loss: 9439.3604\n",
            "Epoch [9/100], Step [101/929], Loss: 9204.7979\n",
            "Epoch [9/100], Step [201/929], Loss: 9457.4639\n",
            "Epoch [9/100], Step [301/929], Loss: 9105.6074\n",
            "Epoch [9/100], Step [401/929], Loss: 9021.5244\n",
            "Epoch [9/100], Step [501/929], Loss: 9310.8154\n",
            "Epoch [9/100], Step [601/929], Loss: 9827.0869\n",
            "Epoch [9/100], Step [701/929], Loss: 9387.6797\n",
            "Epoch [9/100], Step [801/929], Loss: 9394.5361\n",
            "Epoch [9/100], Step [901/929], Loss: 9876.8145\n",
            "Epoch [10/100], Step [1/929], Loss: 9239.6455\n",
            "Epoch [10/100], Step [101/929], Loss: 9747.6963\n",
            "Epoch [10/100], Step [201/929], Loss: 9118.3965\n",
            "Epoch [10/100], Step [301/929], Loss: 9832.8496\n",
            "Epoch [10/100], Step [401/929], Loss: 9279.4619\n",
            "Epoch [10/100], Step [501/929], Loss: 9233.8770\n",
            "Epoch [10/100], Step [601/929], Loss: 9399.7754\n",
            "Epoch [10/100], Step [701/929], Loss: 9032.4160\n",
            "Epoch [10/100], Step [801/929], Loss: 9053.0234\n",
            "Epoch [10/100], Step [901/929], Loss: 9845.7969\n",
            "Epoch [11/100], Step [1/929], Loss: 9323.7803\n",
            "Epoch [11/100], Step [101/929], Loss: 9439.5938\n",
            "Epoch [11/100], Step [201/929], Loss: 9184.4189\n",
            "Epoch [11/100], Step [301/929], Loss: 8919.6797\n",
            "Epoch [11/100], Step [401/929], Loss: 9421.4707\n",
            "Epoch [11/100], Step [501/929], Loss: 9400.5127\n",
            "Epoch [11/100], Step [601/929], Loss: 9132.0479\n",
            "Epoch [11/100], Step [701/929], Loss: 9400.3887\n",
            "Epoch [11/100], Step [801/929], Loss: 9834.3594\n",
            "Epoch [11/100], Step [901/929], Loss: 9620.6279\n",
            "Epoch [12/100], Step [1/929], Loss: 9369.9951\n",
            "Epoch [12/100], Step [101/929], Loss: 9392.8457\n",
            "Epoch [12/100], Step [201/929], Loss: 9304.0078\n",
            "Epoch [12/100], Step [301/929], Loss: 9462.8633\n",
            "Epoch [12/100], Step [401/929], Loss: 8803.1309\n",
            "Epoch [12/100], Step [501/929], Loss: 9673.1562\n",
            "Epoch [12/100], Step [601/929], Loss: 9239.1348\n",
            "Epoch [12/100], Step [701/929], Loss: 9275.8691\n",
            "Epoch [12/100], Step [801/929], Loss: 8979.1143\n",
            "Epoch [12/100], Step [901/929], Loss: 9530.6807\n",
            "Epoch [13/100], Step [1/929], Loss: 9627.4648\n",
            "Epoch [13/100], Step [101/929], Loss: 9303.3184\n",
            "Epoch [13/100], Step [201/929], Loss: 8521.0781\n",
            "Epoch [13/100], Step [301/929], Loss: 9184.9023\n",
            "Epoch [13/100], Step [401/929], Loss: 9485.6699\n",
            "Epoch [13/100], Step [501/929], Loss: 9095.7979\n",
            "Epoch [13/100], Step [601/929], Loss: 9757.7939\n",
            "Epoch [13/100], Step [701/929], Loss: 9118.6641\n",
            "Epoch [13/100], Step [801/929], Loss: 9505.2139\n",
            "Epoch [13/100], Step [901/929], Loss: 8974.5322\n",
            "Epoch [14/100], Step [1/929], Loss: 9167.2617\n",
            "Epoch [14/100], Step [101/929], Loss: 9863.0645\n",
            "Epoch [14/100], Step [201/929], Loss: 9132.3770\n",
            "Epoch [14/100], Step [301/929], Loss: 9259.1240\n",
            "Epoch [14/100], Step [401/929], Loss: 8977.2500\n",
            "Epoch [14/100], Step [501/929], Loss: 8709.8330\n",
            "Epoch [14/100], Step [601/929], Loss: 9132.7705\n",
            "Epoch [14/100], Step [701/929], Loss: 9439.4199\n",
            "Epoch [14/100], Step [801/929], Loss: 9724.5732\n",
            "Epoch [14/100], Step [901/929], Loss: 9844.0186\n",
            "Epoch [15/100], Step [1/929], Loss: 9003.8545\n",
            "Epoch [15/100], Step [101/929], Loss: 9202.8525\n",
            "Epoch [15/100], Step [201/929], Loss: 9771.3721\n",
            "Epoch [15/100], Step [301/929], Loss: 9489.8770\n",
            "Epoch [15/100], Step [401/929], Loss: 9433.2119\n",
            "Epoch [15/100], Step [501/929], Loss: 9332.0596\n",
            "Epoch [15/100], Step [601/929], Loss: 8784.3984\n",
            "Epoch [15/100], Step [701/929], Loss: 9147.7256\n",
            "Epoch [15/100], Step [801/929], Loss: 9218.0205\n",
            "Epoch [15/100], Step [901/929], Loss: 9230.8770\n",
            "Epoch [16/100], Step [1/929], Loss: 9537.9404\n",
            "Epoch [16/100], Step [101/929], Loss: 9199.4287\n",
            "Epoch [16/100], Step [201/929], Loss: 9657.0625\n",
            "Epoch [16/100], Step [301/929], Loss: 9844.7832\n",
            "Epoch [16/100], Step [401/929], Loss: 8651.1299\n",
            "Epoch [16/100], Step [501/929], Loss: 8808.6768\n",
            "Epoch [16/100], Step [601/929], Loss: 9144.0693\n",
            "Epoch [16/100], Step [701/929], Loss: 9275.1152\n",
            "Epoch [16/100], Step [801/929], Loss: 9549.8887\n",
            "Epoch [16/100], Step [901/929], Loss: 9276.8057\n",
            "Epoch [17/100], Step [1/929], Loss: 9318.4814\n",
            "Epoch [17/100], Step [101/929], Loss: 9433.6270\n",
            "Epoch [17/100], Step [201/929], Loss: 8856.1445\n",
            "Epoch [17/100], Step [301/929], Loss: 8993.3877\n",
            "Epoch [17/100], Step [401/929], Loss: 9665.6211\n",
            "Epoch [17/100], Step [501/929], Loss: 9785.8047\n",
            "Epoch [17/100], Step [601/929], Loss: 8987.6074\n",
            "Epoch [17/100], Step [701/929], Loss: 9173.0342\n",
            "Epoch [17/100], Step [801/929], Loss: 8962.5527\n",
            "Epoch [17/100], Step [901/929], Loss: 9228.7891\n",
            "Epoch [18/100], Step [1/929], Loss: 8978.2822\n",
            "Epoch [18/100], Step [101/929], Loss: 9162.8740\n",
            "Epoch [18/100], Step [201/929], Loss: 9264.2939\n",
            "Epoch [18/100], Step [301/929], Loss: 9796.9893\n",
            "Epoch [18/100], Step [401/929], Loss: 9317.7549\n",
            "Epoch [18/100], Step [501/929], Loss: 9198.3691\n",
            "Epoch [18/100], Step [601/929], Loss: 8634.7490\n",
            "Epoch [18/100], Step [701/929], Loss: 9136.3975\n",
            "Epoch [18/100], Step [801/929], Loss: 8950.9824\n",
            "Epoch [18/100], Step [901/929], Loss: 9277.0420\n",
            "Epoch [19/100], Step [1/929], Loss: 8874.7500\n",
            "Epoch [19/100], Step [101/929], Loss: 8755.5898\n",
            "Epoch [19/100], Step [201/929], Loss: 9264.6270\n",
            "Epoch [19/100], Step [301/929], Loss: 8773.1162\n",
            "Epoch [19/100], Step [401/929], Loss: 8619.3379\n",
            "Epoch [19/100], Step [501/929], Loss: 8630.2627\n",
            "Epoch [19/100], Step [601/929], Loss: 9480.7188\n",
            "Epoch [19/100], Step [701/929], Loss: 9333.6074\n",
            "Epoch [19/100], Step [801/929], Loss: 9138.2100\n",
            "Epoch [19/100], Step [901/929], Loss: 8509.1865\n",
            "Epoch [20/100], Step [1/929], Loss: 9509.7168\n",
            "Epoch [20/100], Step [101/929], Loss: 8914.7090\n",
            "Epoch [20/100], Step [201/929], Loss: 8471.3564\n",
            "Epoch [20/100], Step [301/929], Loss: 9084.7656\n",
            "Epoch [20/100], Step [401/929], Loss: 9196.4766\n",
            "Epoch [20/100], Step [501/929], Loss: 9113.9580\n",
            "Epoch [20/100], Step [601/929], Loss: 9577.5039\n",
            "Epoch [20/100], Step [701/929], Loss: 9169.3926\n",
            "Epoch [20/100], Step [801/929], Loss: 8450.5293\n",
            "Epoch [20/100], Step [901/929], Loss: 9427.4580\n",
            "Epoch [21/100], Step [1/929], Loss: 9045.7021\n",
            "Epoch [21/100], Step [101/929], Loss: 9063.4072\n",
            "Epoch [21/100], Step [201/929], Loss: 9400.3994\n",
            "Epoch [21/100], Step [301/929], Loss: 9126.7900\n",
            "Epoch [21/100], Step [401/929], Loss: 9207.6240\n",
            "Epoch [21/100], Step [501/929], Loss: 8978.3145\n",
            "Epoch [21/100], Step [601/929], Loss: 8964.5156\n",
            "Epoch [21/100], Step [701/929], Loss: 9308.2686\n",
            "Epoch [21/100], Step [801/929], Loss: 8879.7676\n",
            "Epoch [21/100], Step [901/929], Loss: 9119.6191\n",
            "Epoch [22/100], Step [1/929], Loss: 8974.3916\n",
            "Epoch [22/100], Step [101/929], Loss: 9430.3633\n",
            "Epoch [22/100], Step [201/929], Loss: 8893.3047\n",
            "Epoch [22/100], Step [301/929], Loss: 9208.3770\n",
            "Epoch [22/100], Step [401/929], Loss: 9209.6797\n",
            "Epoch [22/100], Step [501/929], Loss: 8533.3496\n",
            "Epoch [22/100], Step [601/929], Loss: 8870.1484\n",
            "Epoch [22/100], Step [701/929], Loss: 9062.2773\n",
            "Epoch [22/100], Step [801/929], Loss: 8970.3281\n",
            "Epoch [22/100], Step [901/929], Loss: 9487.7852\n",
            "Epoch [23/100], Step [1/929], Loss: 9195.9463\n",
            "Epoch [23/100], Step [101/929], Loss: 8591.1025\n",
            "Epoch [23/100], Step [201/929], Loss: 9340.6895\n",
            "Epoch [23/100], Step [301/929], Loss: 9676.5010\n",
            "Epoch [23/100], Step [401/929], Loss: 9105.9580\n",
            "Epoch [23/100], Step [501/929], Loss: 9449.7168\n",
            "Epoch [23/100], Step [601/929], Loss: 9247.9346\n",
            "Epoch [23/100], Step [701/929], Loss: 9431.5225\n",
            "Epoch [23/100], Step [801/929], Loss: 8821.2861\n",
            "Epoch [23/100], Step [901/929], Loss: 9174.8936\n",
            "Epoch [24/100], Step [1/929], Loss: 9264.1133\n",
            "Epoch [24/100], Step [101/929], Loss: 9510.2334\n",
            "Epoch [24/100], Step [201/929], Loss: 9013.4316\n",
            "Epoch [24/100], Step [301/929], Loss: 8540.9590\n",
            "Epoch [24/100], Step [401/929], Loss: 9094.0625\n",
            "Epoch [24/100], Step [501/929], Loss: 9482.0869\n",
            "Epoch [24/100], Step [601/929], Loss: 9081.1309\n",
            "Epoch [24/100], Step [701/929], Loss: 8891.1621\n",
            "Epoch [24/100], Step [801/929], Loss: 9372.8408\n",
            "Epoch [24/100], Step [901/929], Loss: 9010.1699\n",
            "Epoch [25/100], Step [1/929], Loss: 8550.3164\n",
            "Epoch [25/100], Step [101/929], Loss: 8939.6680\n",
            "Epoch [25/100], Step [201/929], Loss: 8475.1318\n",
            "Epoch [25/100], Step [301/929], Loss: 9154.1709\n",
            "Epoch [25/100], Step [401/929], Loss: 8935.1670\n",
            "Epoch [25/100], Step [501/929], Loss: 9443.5625\n",
            "Epoch [25/100], Step [601/929], Loss: 8841.3340\n",
            "Epoch [25/100], Step [701/929], Loss: 9058.8818\n",
            "Epoch [25/100], Step [801/929], Loss: 9831.8428\n",
            "Epoch [25/100], Step [901/929], Loss: 9008.2051\n",
            "Epoch [26/100], Step [1/929], Loss: 9058.5459\n",
            "Epoch [26/100], Step [101/929], Loss: 9433.9238\n",
            "Epoch [26/100], Step [201/929], Loss: 9008.0439\n",
            "Epoch [26/100], Step [301/929], Loss: 8921.8135\n",
            "Epoch [26/100], Step [401/929], Loss: 9079.3701\n",
            "Epoch [26/100], Step [501/929], Loss: 8800.7119\n",
            "Epoch [26/100], Step [601/929], Loss: 8806.4727\n",
            "Epoch [26/100], Step [701/929], Loss: 9106.3447\n",
            "Epoch [26/100], Step [801/929], Loss: 8774.7705\n",
            "Epoch [26/100], Step [901/929], Loss: 8745.9805\n",
            "Epoch [27/100], Step [1/929], Loss: 8662.3359\n",
            "Epoch [27/100], Step [101/929], Loss: 8656.0391\n",
            "Epoch [27/100], Step [201/929], Loss: 9133.3633\n",
            "Epoch [27/100], Step [301/929], Loss: 8477.5088\n",
            "Epoch [27/100], Step [401/929], Loss: 8902.3955\n",
            "Epoch [27/100], Step [501/929], Loss: 9364.8203\n",
            "Epoch [27/100], Step [601/929], Loss: 9012.1475\n",
            "Epoch [27/100], Step [701/929], Loss: 9349.8955\n",
            "Epoch [27/100], Step [801/929], Loss: 8976.1162\n",
            "Epoch [27/100], Step [901/929], Loss: 8460.5762\n",
            "Epoch [28/100], Step [1/929], Loss: 9380.7441\n",
            "Epoch [28/100], Step [101/929], Loss: 9062.4131\n",
            "Epoch [28/100], Step [201/929], Loss: 9928.2607\n",
            "Epoch [28/100], Step [301/929], Loss: 9079.9756\n",
            "Epoch [28/100], Step [401/929], Loss: 9033.3867\n",
            "Epoch [28/100], Step [501/929], Loss: 9064.5186\n",
            "Epoch [28/100], Step [601/929], Loss: 8707.1055\n",
            "Epoch [28/100], Step [701/929], Loss: 9081.0771\n",
            "Epoch [28/100], Step [801/929], Loss: 8792.7842\n",
            "Epoch [28/100], Step [901/929], Loss: 8913.5283\n",
            "Epoch [29/100], Step [1/929], Loss: 9139.6016\n",
            "Epoch [29/100], Step [101/929], Loss: 8893.1777\n",
            "Epoch [29/100], Step [201/929], Loss: 8800.4248\n",
            "Epoch [29/100], Step [301/929], Loss: 8676.0176\n",
            "Epoch [29/100], Step [401/929], Loss: 9020.6895\n",
            "Epoch [29/100], Step [501/929], Loss: 9125.4062\n",
            "Epoch [29/100], Step [601/929], Loss: 9078.6123\n",
            "Epoch [29/100], Step [701/929], Loss: 8584.4209\n",
            "Epoch [29/100], Step [801/929], Loss: 8933.5742\n",
            "Epoch [29/100], Step [901/929], Loss: 9436.6875\n",
            "Epoch [30/100], Step [1/929], Loss: 9095.0244\n",
            "Epoch [30/100], Step [101/929], Loss: 8780.1055\n",
            "Epoch [30/100], Step [201/929], Loss: 8970.3848\n",
            "Epoch [30/100], Step [301/929], Loss: 8969.8984\n",
            "Epoch [30/100], Step [401/929], Loss: 8527.9668\n",
            "Epoch [30/100], Step [501/929], Loss: 8931.0117\n",
            "Epoch [30/100], Step [601/929], Loss: 8594.6875\n",
            "Epoch [30/100], Step [701/929], Loss: 9488.3027\n",
            "Epoch [30/100], Step [801/929], Loss: 9219.5371\n",
            "Epoch [30/100], Step [901/929], Loss: 9057.9570\n",
            "Epoch [31/100], Step [1/929], Loss: 8822.6172\n",
            "Epoch [31/100], Step [101/929], Loss: 9355.3203\n",
            "Epoch [31/100], Step [201/929], Loss: 8724.5254\n",
            "Epoch [31/100], Step [301/929], Loss: 9092.4199\n",
            "Epoch [31/100], Step [401/929], Loss: 8874.6777\n",
            "Epoch [31/100], Step [501/929], Loss: 8982.8701\n",
            "Epoch [31/100], Step [601/929], Loss: 9147.0381\n",
            "Epoch [31/100], Step [701/929], Loss: 9118.7236\n",
            "Epoch [31/100], Step [801/929], Loss: 8734.1270\n",
            "Epoch [31/100], Step [901/929], Loss: 8962.3545\n",
            "Epoch [32/100], Step [1/929], Loss: 9033.1836\n",
            "Epoch [32/100], Step [101/929], Loss: 9615.8662\n",
            "Epoch [32/100], Step [201/929], Loss: 9175.6289\n",
            "Epoch [32/100], Step [301/929], Loss: 9099.3750\n",
            "Epoch [32/100], Step [401/929], Loss: 8676.6631\n",
            "Epoch [32/100], Step [501/929], Loss: 8567.6699\n",
            "Epoch [32/100], Step [601/929], Loss: 8839.8926\n",
            "Epoch [32/100], Step [701/929], Loss: 8701.1133\n",
            "Epoch [32/100], Step [801/929], Loss: 8880.8662\n",
            "Epoch [32/100], Step [901/929], Loss: 8980.2695\n",
            "Epoch [33/100], Step [1/929], Loss: 8777.8145\n",
            "Epoch [33/100], Step [101/929], Loss: 8261.7129\n",
            "Epoch [33/100], Step [201/929], Loss: 8716.6758\n",
            "Epoch [33/100], Step [301/929], Loss: 9441.6670\n",
            "Epoch [33/100], Step [401/929], Loss: 9430.4180\n",
            "Epoch [33/100], Step [501/929], Loss: 9320.8398\n",
            "Epoch [33/100], Step [601/929], Loss: 9280.7695\n",
            "Epoch [33/100], Step [701/929], Loss: 9078.8379\n",
            "Epoch [33/100], Step [801/929], Loss: 8825.1016\n",
            "Epoch [33/100], Step [901/929], Loss: 8973.6143\n",
            "Epoch [34/100], Step [1/929], Loss: 8983.1973\n",
            "Epoch [34/100], Step [101/929], Loss: 9076.4619\n",
            "Epoch [34/100], Step [201/929], Loss: 9371.9785\n",
            "Epoch [34/100], Step [301/929], Loss: 8682.6514\n",
            "Epoch [34/100], Step [401/929], Loss: 9190.4814\n",
            "Epoch [34/100], Step [501/929], Loss: 9260.2773\n",
            "Epoch [34/100], Step [601/929], Loss: 9144.1758\n",
            "Epoch [34/100], Step [701/929], Loss: 8824.1221\n",
            "Epoch [34/100], Step [801/929], Loss: 8944.5352\n",
            "Epoch [34/100], Step [901/929], Loss: 8821.8838\n",
            "Epoch [35/100], Step [1/929], Loss: 8649.9209\n",
            "Epoch [35/100], Step [101/929], Loss: 9171.5605\n",
            "Epoch [35/100], Step [201/929], Loss: 9240.7305\n",
            "Epoch [35/100], Step [301/929], Loss: 9371.8906\n",
            "Epoch [35/100], Step [401/929], Loss: 8932.8086\n",
            "Epoch [35/100], Step [501/929], Loss: 9267.7949\n",
            "Epoch [35/100], Step [601/929], Loss: 8824.9141\n",
            "Epoch [35/100], Step [701/929], Loss: 9040.3516\n",
            "Epoch [35/100], Step [801/929], Loss: 8657.3887\n",
            "Epoch [35/100], Step [901/929], Loss: 8588.2119\n",
            "Epoch [36/100], Step [1/929], Loss: 8389.5361\n",
            "Epoch [36/100], Step [101/929], Loss: 8695.0352\n",
            "Epoch [36/100], Step [201/929], Loss: 9134.0537\n",
            "Epoch [36/100], Step [301/929], Loss: 9004.5000\n",
            "Epoch [36/100], Step [401/929], Loss: 8861.8398\n",
            "Epoch [36/100], Step [501/929], Loss: 9036.0488\n",
            "Epoch [36/100], Step [601/929], Loss: 8846.4229\n",
            "Epoch [36/100], Step [701/929], Loss: 8646.0215\n",
            "Epoch [36/100], Step [801/929], Loss: 9240.7754\n",
            "Epoch [36/100], Step [901/929], Loss: 8596.4131\n",
            "Epoch [37/100], Step [1/929], Loss: 9062.7188\n",
            "Epoch [37/100], Step [101/929], Loss: 8426.8223\n",
            "Epoch [37/100], Step [201/929], Loss: 9134.6367\n",
            "Epoch [37/100], Step [301/929], Loss: 9289.4570\n",
            "Epoch [37/100], Step [401/929], Loss: 8537.2324\n",
            "Epoch [37/100], Step [501/929], Loss: 9132.0850\n",
            "Epoch [37/100], Step [601/929], Loss: 8988.3535\n",
            "Epoch [37/100], Step [701/929], Loss: 9205.6240\n",
            "Epoch [37/100], Step [801/929], Loss: 8464.7715\n",
            "Epoch [37/100], Step [901/929], Loss: 9285.6299\n",
            "Epoch [38/100], Step [1/929], Loss: 8776.4658\n",
            "Epoch [38/100], Step [101/929], Loss: 8842.4756\n",
            "Epoch [38/100], Step [201/929], Loss: 8429.3848\n",
            "Epoch [38/100], Step [301/929], Loss: 8348.4551\n",
            "Epoch [38/100], Step [401/929], Loss: 9483.5586\n",
            "Epoch [38/100], Step [501/929], Loss: 9268.1426\n",
            "Epoch [38/100], Step [601/929], Loss: 8756.2930\n",
            "Epoch [38/100], Step [701/929], Loss: 8485.5469\n",
            "Epoch [38/100], Step [801/929], Loss: 9352.5625\n",
            "Epoch [38/100], Step [901/929], Loss: 8812.5752\n",
            "Epoch [39/100], Step [1/929], Loss: 8605.8398\n",
            "Epoch [39/100], Step [101/929], Loss: 9684.0889\n",
            "Epoch [39/100], Step [201/929], Loss: 8933.1406\n",
            "Epoch [39/100], Step [301/929], Loss: 8935.7305\n",
            "Epoch [39/100], Step [401/929], Loss: 9295.3896\n",
            "Epoch [39/100], Step [501/929], Loss: 9178.4766\n",
            "Epoch [39/100], Step [601/929], Loss: 9113.7051\n",
            "Epoch [39/100], Step [701/929], Loss: 8403.2197\n",
            "Epoch [39/100], Step [801/929], Loss: 8795.6211\n",
            "Epoch [39/100], Step [901/929], Loss: 8836.1475\n",
            "Epoch [40/100], Step [1/929], Loss: 8901.2324\n",
            "Epoch [40/100], Step [101/929], Loss: 9186.0391\n",
            "Epoch [40/100], Step [201/929], Loss: 9125.8418\n",
            "Epoch [40/100], Step [301/929], Loss: 8962.4648\n",
            "Epoch [40/100], Step [401/929], Loss: 8832.9121\n",
            "Epoch [40/100], Step [501/929], Loss: 8715.3047\n",
            "Epoch [40/100], Step [601/929], Loss: 8754.0459\n",
            "Epoch [40/100], Step [701/929], Loss: 8849.8555\n",
            "Epoch [40/100], Step [801/929], Loss: 8554.8477\n",
            "Epoch [40/100], Step [901/929], Loss: 8909.6279\n",
            "Epoch [41/100], Step [1/929], Loss: 8613.3447\n",
            "Epoch [41/100], Step [101/929], Loss: 9121.3047\n",
            "Epoch [41/100], Step [201/929], Loss: 8810.6719\n",
            "Epoch [41/100], Step [301/929], Loss: 8530.8809\n",
            "Epoch [41/100], Step [401/929], Loss: 9147.2939\n",
            "Epoch [41/100], Step [501/929], Loss: 9009.4219\n",
            "Epoch [41/100], Step [601/929], Loss: 9117.4941\n",
            "Epoch [41/100], Step [701/929], Loss: 8753.5459\n",
            "Epoch [41/100], Step [801/929], Loss: 9132.2158\n",
            "Epoch [41/100], Step [901/929], Loss: 8877.4277\n",
            "Epoch [42/100], Step [1/929], Loss: 8655.5771\n",
            "Epoch [42/100], Step [101/929], Loss: 8868.0762\n",
            "Epoch [42/100], Step [201/929], Loss: 9508.7002\n",
            "Epoch [42/100], Step [301/929], Loss: 8419.5000\n",
            "Epoch [42/100], Step [401/929], Loss: 8976.7861\n",
            "Epoch [42/100], Step [501/929], Loss: 9095.3916\n",
            "Epoch [42/100], Step [601/929], Loss: 8650.7666\n",
            "Epoch [42/100], Step [701/929], Loss: 8567.0361\n",
            "Epoch [42/100], Step [801/929], Loss: 8934.9141\n",
            "Epoch [42/100], Step [901/929], Loss: 8425.8145\n",
            "Epoch [43/100], Step [1/929], Loss: 8963.1582\n",
            "Epoch [43/100], Step [101/929], Loss: 8648.8281\n",
            "Epoch [43/100], Step [201/929], Loss: 8350.1982\n",
            "Epoch [43/100], Step [301/929], Loss: 8902.3682\n",
            "Epoch [43/100], Step [401/929], Loss: 8832.4121\n",
            "Epoch [43/100], Step [501/929], Loss: 9590.5625\n",
            "Epoch [43/100], Step [601/929], Loss: 8717.2773\n",
            "Epoch [43/100], Step [701/929], Loss: 9305.0312\n",
            "Epoch [43/100], Step [801/929], Loss: 8643.5254\n",
            "Epoch [43/100], Step [901/929], Loss: 8684.2891\n",
            "Epoch [44/100], Step [1/929], Loss: 8892.3174\n",
            "Epoch [44/100], Step [101/929], Loss: 8733.3848\n",
            "Epoch [44/100], Step [201/929], Loss: 8812.7080\n",
            "Epoch [44/100], Step [301/929], Loss: 9231.7158\n",
            "Epoch [44/100], Step [401/929], Loss: 8705.6553\n",
            "Epoch [44/100], Step [501/929], Loss: 8571.3047\n",
            "Epoch [44/100], Step [601/929], Loss: 8872.4463\n",
            "Epoch [44/100], Step [701/929], Loss: 8907.8389\n",
            "Epoch [44/100], Step [801/929], Loss: 8691.8008\n",
            "Epoch [44/100], Step [901/929], Loss: 9197.8555\n",
            "Epoch [45/100], Step [1/929], Loss: 8478.6250\n",
            "Epoch [45/100], Step [101/929], Loss: 9079.4365\n",
            "Epoch [45/100], Step [201/929], Loss: 9281.8691\n",
            "Epoch [45/100], Step [301/929], Loss: 8930.3086\n",
            "Epoch [45/100], Step [401/929], Loss: 8943.8721\n",
            "Epoch [45/100], Step [501/929], Loss: 9124.1748\n",
            "Epoch [45/100], Step [601/929], Loss: 8899.0381\n",
            "Epoch [45/100], Step [701/929], Loss: 8555.6270\n",
            "Epoch [45/100], Step [801/929], Loss: 9178.0859\n",
            "Epoch [45/100], Step [901/929], Loss: 8802.8994\n",
            "Epoch [46/100], Step [1/929], Loss: 9261.0664\n",
            "Epoch [46/100], Step [101/929], Loss: 8625.9580\n",
            "Epoch [46/100], Step [201/929], Loss: 8649.6230\n",
            "Epoch [46/100], Step [301/929], Loss: 8731.5977\n",
            "Epoch [46/100], Step [401/929], Loss: 8838.0928\n",
            "Epoch [46/100], Step [501/929], Loss: 8949.7109\n",
            "Epoch [46/100], Step [601/929], Loss: 8867.7080\n",
            "Epoch [46/100], Step [701/929], Loss: 8776.7363\n",
            "Epoch [46/100], Step [801/929], Loss: 8650.0029\n",
            "Epoch [46/100], Step [901/929], Loss: 8851.5986\n",
            "Epoch [47/100], Step [1/929], Loss: 8921.9150\n",
            "Epoch [47/100], Step [101/929], Loss: 8843.7461\n",
            "Epoch [47/100], Step [201/929], Loss: 9490.5908\n",
            "Epoch [47/100], Step [301/929], Loss: 9156.2188\n",
            "Epoch [47/100], Step [401/929], Loss: 9311.8428\n",
            "Epoch [47/100], Step [501/929], Loss: 8619.1602\n",
            "Epoch [47/100], Step [601/929], Loss: 8803.9238\n",
            "Epoch [47/100], Step [701/929], Loss: 9126.6816\n",
            "Epoch [47/100], Step [801/929], Loss: 9229.8633\n",
            "Epoch [47/100], Step [901/929], Loss: 8626.4570\n",
            "Epoch [48/100], Step [1/929], Loss: 8683.8906\n",
            "Epoch [48/100], Step [101/929], Loss: 8750.6914\n",
            "Epoch [48/100], Step [201/929], Loss: 9064.4941\n",
            "Epoch [48/100], Step [301/929], Loss: 8747.9609\n",
            "Epoch [48/100], Step [401/929], Loss: 8610.7900\n",
            "Epoch [48/100], Step [501/929], Loss: 8985.1514\n",
            "Epoch [48/100], Step [601/929], Loss: 8432.0195\n",
            "Epoch [48/100], Step [701/929], Loss: 8624.9316\n",
            "Epoch [48/100], Step [801/929], Loss: 9007.9238\n",
            "Epoch [48/100], Step [901/929], Loss: 9063.0811\n",
            "Epoch [49/100], Step [1/929], Loss: 9342.1602\n",
            "Epoch [49/100], Step [101/929], Loss: 8272.6777\n",
            "Epoch [49/100], Step [201/929], Loss: 9035.4355\n",
            "Epoch [49/100], Step [301/929], Loss: 8529.7441\n",
            "Epoch [49/100], Step [401/929], Loss: 8836.5205\n",
            "Epoch [49/100], Step [501/929], Loss: 8997.8516\n",
            "Epoch [49/100], Step [601/929], Loss: 8596.7266\n",
            "Epoch [49/100], Step [701/929], Loss: 8954.4775\n",
            "Epoch [49/100], Step [801/929], Loss: 9197.6963\n",
            "Epoch [49/100], Step [901/929], Loss: 8589.2969\n",
            "Epoch [50/100], Step [1/929], Loss: 9137.3994\n",
            "Epoch [50/100], Step [101/929], Loss: 9362.9922\n",
            "Epoch [50/100], Step [201/929], Loss: 8807.3877\n",
            "Epoch [50/100], Step [301/929], Loss: 8742.9297\n",
            "Epoch [50/100], Step [401/929], Loss: 9182.8057\n",
            "Epoch [50/100], Step [501/929], Loss: 7992.7480\n",
            "Epoch [50/100], Step [601/929], Loss: 8566.0156\n",
            "Epoch [50/100], Step [701/929], Loss: 8705.3701\n",
            "Epoch [50/100], Step [801/929], Loss: 9103.0010\n",
            "Epoch [50/100], Step [901/929], Loss: 8680.5596\n",
            "Epoch [51/100], Step [1/929], Loss: 8916.9180\n",
            "Epoch [51/100], Step [101/929], Loss: 8420.3281\n",
            "Epoch [51/100], Step [201/929], Loss: 8919.5371\n",
            "Epoch [51/100], Step [301/929], Loss: 9053.8350\n",
            "Epoch [51/100], Step [401/929], Loss: 8942.8438\n",
            "Epoch [51/100], Step [501/929], Loss: 8792.7812\n",
            "Epoch [51/100], Step [601/929], Loss: 8781.7305\n",
            "Epoch [51/100], Step [701/929], Loss: 8702.5645\n",
            "Epoch [51/100], Step [801/929], Loss: 9338.6846\n",
            "Epoch [51/100], Step [901/929], Loss: 8693.1426\n",
            "Epoch [52/100], Step [1/929], Loss: 8980.0195\n",
            "Epoch [52/100], Step [101/929], Loss: 8760.5498\n",
            "Epoch [52/100], Step [201/929], Loss: 9275.3408\n",
            "Epoch [52/100], Step [301/929], Loss: 8714.0479\n",
            "Epoch [52/100], Step [401/929], Loss: 8678.3262\n",
            "Epoch [52/100], Step [501/929], Loss: 8645.6338\n",
            "Epoch [52/100], Step [601/929], Loss: 8947.4395\n",
            "Epoch [52/100], Step [701/929], Loss: 9172.2217\n",
            "Epoch [52/100], Step [801/929], Loss: 8750.0820\n",
            "Epoch [52/100], Step [901/929], Loss: 8978.9922\n",
            "Epoch [53/100], Step [1/929], Loss: 8073.8896\n",
            "Epoch [53/100], Step [101/929], Loss: 8202.0703\n",
            "Epoch [53/100], Step [201/929], Loss: 9211.0957\n",
            "Epoch [53/100], Step [301/929], Loss: 8372.7402\n",
            "Epoch [53/100], Step [401/929], Loss: 8831.2031\n",
            "Epoch [53/100], Step [501/929], Loss: 9543.0430\n",
            "Epoch [53/100], Step [601/929], Loss: 8442.3447\n",
            "Epoch [53/100], Step [701/929], Loss: 8757.2314\n",
            "Epoch [53/100], Step [801/929], Loss: 8561.0586\n",
            "Epoch [53/100], Step [901/929], Loss: 8473.1904\n",
            "Epoch [54/100], Step [1/929], Loss: 8703.7861\n",
            "Epoch [54/100], Step [101/929], Loss: 8998.3750\n",
            "Epoch [54/100], Step [201/929], Loss: 8844.3389\n",
            "Epoch [54/100], Step [301/929], Loss: 8983.4395\n",
            "Epoch [54/100], Step [401/929], Loss: 9038.8369\n",
            "Epoch [54/100], Step [501/929], Loss: 9032.4678\n",
            "Epoch [54/100], Step [601/929], Loss: 8851.0830\n",
            "Epoch [54/100], Step [701/929], Loss: 9026.2314\n",
            "Epoch [54/100], Step [801/929], Loss: 9243.1826\n",
            "Epoch [54/100], Step [901/929], Loss: 8611.4785\n",
            "Epoch [55/100], Step [1/929], Loss: 8776.1152\n",
            "Epoch [55/100], Step [101/929], Loss: 8405.5361\n",
            "Epoch [55/100], Step [201/929], Loss: 9312.6309\n",
            "Epoch [55/100], Step [301/929], Loss: 8771.0928\n",
            "Epoch [55/100], Step [401/929], Loss: 8854.9590\n",
            "Epoch [55/100], Step [501/929], Loss: 8920.3711\n",
            "Epoch [55/100], Step [601/929], Loss: 8802.3555\n",
            "Epoch [55/100], Step [701/929], Loss: 8661.4102\n",
            "Epoch [55/100], Step [801/929], Loss: 8627.7275\n",
            "Epoch [55/100], Step [901/929], Loss: 9135.8438\n",
            "Epoch [56/100], Step [1/929], Loss: 8720.3564\n",
            "Epoch [56/100], Step [101/929], Loss: 8817.8359\n",
            "Epoch [56/100], Step [201/929], Loss: 8910.1953\n",
            "Epoch [56/100], Step [301/929], Loss: 8678.9033\n",
            "Epoch [56/100], Step [401/929], Loss: 8437.0469\n",
            "Epoch [56/100], Step [501/929], Loss: 8935.3301\n",
            "Epoch [56/100], Step [601/929], Loss: 9202.3223\n",
            "Epoch [56/100], Step [701/929], Loss: 9251.5586\n",
            "Epoch [56/100], Step [801/929], Loss: 8824.2715\n",
            "Epoch [56/100], Step [901/929], Loss: 8422.4551\n",
            "Epoch [57/100], Step [1/929], Loss: 9408.2324\n",
            "Epoch [57/100], Step [101/929], Loss: 8603.2217\n",
            "Epoch [57/100], Step [201/929], Loss: 8781.5107\n",
            "Epoch [57/100], Step [301/929], Loss: 8548.3477\n",
            "Epoch [57/100], Step [401/929], Loss: 9116.5654\n",
            "Epoch [57/100], Step [501/929], Loss: 8559.1201\n",
            "Epoch [57/100], Step [601/929], Loss: 8686.7402\n",
            "Epoch [57/100], Step [701/929], Loss: 8596.4551\n",
            "Epoch [57/100], Step [801/929], Loss: 8417.2607\n",
            "Epoch [57/100], Step [901/929], Loss: 8638.7910\n",
            "Epoch [58/100], Step [1/929], Loss: 8695.6855\n",
            "Epoch [58/100], Step [101/929], Loss: 8782.1836\n",
            "Epoch [58/100], Step [201/929], Loss: 8924.0820\n",
            "Epoch [58/100], Step [301/929], Loss: 9275.2363\n",
            "Epoch [58/100], Step [401/929], Loss: 8883.0195\n",
            "Epoch [58/100], Step [501/929], Loss: 8790.0186\n",
            "Epoch [58/100], Step [601/929], Loss: 8974.1396\n",
            "Epoch [58/100], Step [701/929], Loss: 8901.6523\n",
            "Epoch [58/100], Step [801/929], Loss: 9173.7461\n",
            "Epoch [58/100], Step [901/929], Loss: 8828.4160\n",
            "Epoch [59/100], Step [1/929], Loss: 8627.7490\n",
            "Epoch [59/100], Step [101/929], Loss: 8809.3672\n",
            "Epoch [59/100], Step [201/929], Loss: 8808.4492\n",
            "Epoch [59/100], Step [301/929], Loss: 8740.5811\n",
            "Epoch [59/100], Step [401/929], Loss: 8568.5498\n",
            "Epoch [59/100], Step [501/929], Loss: 8750.3730\n",
            "Epoch [59/100], Step [601/929], Loss: 8428.8340\n",
            "Epoch [59/100], Step [701/929], Loss: 9033.4951\n",
            "Epoch [59/100], Step [801/929], Loss: 9164.8301\n",
            "Epoch [59/100], Step [901/929], Loss: 8750.4531\n",
            "Epoch [60/100], Step [1/929], Loss: 9111.0430\n",
            "Epoch [60/100], Step [101/929], Loss: 9062.1680\n",
            "Epoch [60/100], Step [201/929], Loss: 8252.4805\n",
            "Epoch [60/100], Step [301/929], Loss: 9167.2539\n",
            "Epoch [60/100], Step [401/929], Loss: 9082.8789\n",
            "Epoch [60/100], Step [501/929], Loss: 8875.8975\n",
            "Epoch [60/100], Step [601/929], Loss: 8517.8232\n",
            "Epoch [60/100], Step [701/929], Loss: 8997.2695\n",
            "Epoch [60/100], Step [801/929], Loss: 8651.8701\n",
            "Epoch [60/100], Step [901/929], Loss: 8786.7900\n",
            "Epoch [61/100], Step [1/929], Loss: 8624.9004\n",
            "Epoch [61/100], Step [101/929], Loss: 8581.8506\n",
            "Epoch [61/100], Step [201/929], Loss: 8801.6055\n",
            "Epoch [61/100], Step [301/929], Loss: 8772.3398\n",
            "Epoch [61/100], Step [401/929], Loss: 9073.3467\n",
            "Epoch [61/100], Step [501/929], Loss: 8717.2109\n",
            "Epoch [61/100], Step [601/929], Loss: 9443.3750\n",
            "Epoch [61/100], Step [701/929], Loss: 8807.1943\n",
            "Epoch [61/100], Step [801/929], Loss: 8462.9336\n",
            "Epoch [61/100], Step [901/929], Loss: 8583.9629\n",
            "Epoch [62/100], Step [1/929], Loss: 8890.3799\n",
            "Epoch [62/100], Step [101/929], Loss: 8747.5693\n",
            "Epoch [62/100], Step [201/929], Loss: 8587.7773\n",
            "Epoch [62/100], Step [301/929], Loss: 9023.5391\n",
            "Epoch [62/100], Step [401/929], Loss: 8430.0625\n",
            "Epoch [62/100], Step [501/929], Loss: 8481.7988\n",
            "Epoch [62/100], Step [601/929], Loss: 8804.9512\n",
            "Epoch [62/100], Step [701/929], Loss: 8816.3926\n",
            "Epoch [62/100], Step [801/929], Loss: 9177.0137\n",
            "Epoch [62/100], Step [901/929], Loss: 8625.3154\n",
            "Epoch [63/100], Step [1/929], Loss: 8842.1162\n",
            "Epoch [63/100], Step [101/929], Loss: 9191.0234\n",
            "Epoch [63/100], Step [201/929], Loss: 9021.0176\n",
            "Epoch [63/100], Step [301/929], Loss: 8986.5645\n",
            "Epoch [63/100], Step [401/929], Loss: 8449.8623\n",
            "Epoch [63/100], Step [501/929], Loss: 9310.3457\n",
            "Epoch [63/100], Step [601/929], Loss: 9145.4160\n",
            "Epoch [63/100], Step [701/929], Loss: 9157.7939\n",
            "Epoch [63/100], Step [801/929], Loss: 8797.6465\n",
            "Epoch [63/100], Step [901/929], Loss: 9143.5635\n",
            "Epoch [64/100], Step [1/929], Loss: 8888.1621\n",
            "Epoch [64/100], Step [101/929], Loss: 8903.7383\n",
            "Epoch [64/100], Step [201/929], Loss: 8944.9395\n",
            "Epoch [64/100], Step [301/929], Loss: 8723.1934\n",
            "Epoch [64/100], Step [401/929], Loss: 8354.6670\n",
            "Epoch [64/100], Step [501/929], Loss: 8524.6309\n",
            "Epoch [64/100], Step [601/929], Loss: 8616.0508\n",
            "Epoch [64/100], Step [701/929], Loss: 9381.3438\n",
            "Epoch [64/100], Step [801/929], Loss: 8582.1074\n",
            "Epoch [64/100], Step [901/929], Loss: 8655.1221\n",
            "Epoch [65/100], Step [1/929], Loss: 8971.7344\n",
            "Epoch [65/100], Step [101/929], Loss: 8903.8193\n",
            "Epoch [65/100], Step [201/929], Loss: 8895.3770\n",
            "Epoch [65/100], Step [301/929], Loss: 7901.3257\n",
            "Epoch [65/100], Step [401/929], Loss: 8887.7412\n",
            "Epoch [65/100], Step [501/929], Loss: 8257.9307\n",
            "Epoch [65/100], Step [601/929], Loss: 8350.7607\n",
            "Epoch [65/100], Step [701/929], Loss: 8903.4570\n",
            "Epoch [65/100], Step [801/929], Loss: 8602.2344\n",
            "Epoch [65/100], Step [901/929], Loss: 8676.3330\n",
            "Epoch [66/100], Step [1/929], Loss: 8942.4609\n",
            "Epoch [66/100], Step [101/929], Loss: 8983.2344\n",
            "Epoch [66/100], Step [201/929], Loss: 8824.5605\n",
            "Epoch [66/100], Step [301/929], Loss: 8783.1045\n",
            "Epoch [66/100], Step [401/929], Loss: 8826.1875\n",
            "Epoch [66/100], Step [501/929], Loss: 8411.6348\n",
            "Epoch [66/100], Step [601/929], Loss: 8385.0391\n",
            "Epoch [66/100], Step [701/929], Loss: 8867.5117\n",
            "Epoch [66/100], Step [801/929], Loss: 8332.6660\n",
            "Epoch [66/100], Step [901/929], Loss: 8257.8145\n",
            "Epoch [67/100], Step [1/929], Loss: 8843.2676\n",
            "Epoch [67/100], Step [101/929], Loss: 8577.2461\n",
            "Epoch [67/100], Step [201/929], Loss: 8666.6904\n",
            "Epoch [67/100], Step [301/929], Loss: 8875.0781\n",
            "Epoch [67/100], Step [401/929], Loss: 9046.4453\n",
            "Epoch [67/100], Step [501/929], Loss: 8880.5244\n",
            "Epoch [67/100], Step [601/929], Loss: 8990.5293\n",
            "Epoch [67/100], Step [701/929], Loss: 8601.2861\n",
            "Epoch [67/100], Step [801/929], Loss: 8951.3418\n",
            "Epoch [67/100], Step [901/929], Loss: 8671.6582\n",
            "Epoch [68/100], Step [1/929], Loss: 8109.0537\n",
            "Epoch [68/100], Step [101/929], Loss: 8615.8574\n",
            "Epoch [68/100], Step [201/929], Loss: 8350.6230\n",
            "Epoch [68/100], Step [301/929], Loss: 8875.5713\n",
            "Epoch [68/100], Step [401/929], Loss: 9051.0439\n",
            "Epoch [68/100], Step [501/929], Loss: 8321.5576\n",
            "Epoch [68/100], Step [601/929], Loss: 9060.3623\n",
            "Epoch [68/100], Step [701/929], Loss: 8747.7070\n",
            "Epoch [68/100], Step [801/929], Loss: 8674.7949\n",
            "Epoch [68/100], Step [901/929], Loss: 8580.2432\n",
            "Epoch [69/100], Step [1/929], Loss: 8072.9312\n",
            "Epoch [69/100], Step [101/929], Loss: 8640.6895\n",
            "Epoch [69/100], Step [201/929], Loss: 9108.4375\n",
            "Epoch [69/100], Step [301/929], Loss: 8608.0830\n",
            "Epoch [69/100], Step [401/929], Loss: 8630.0879\n",
            "Epoch [69/100], Step [501/929], Loss: 8611.3467\n",
            "Epoch [69/100], Step [601/929], Loss: 9288.0674\n",
            "Epoch [69/100], Step [701/929], Loss: 9254.9199\n",
            "Epoch [69/100], Step [801/929], Loss: 9481.2139\n",
            "Epoch [69/100], Step [901/929], Loss: 8971.8789\n",
            "Epoch [70/100], Step [1/929], Loss: 8850.1094\n",
            "Epoch [70/100], Step [101/929], Loss: 8545.2578\n",
            "Epoch [70/100], Step [201/929], Loss: 8855.5537\n",
            "Epoch [70/100], Step [301/929], Loss: 8834.4160\n",
            "Epoch [70/100], Step [401/929], Loss: 8870.7129\n",
            "Epoch [70/100], Step [501/929], Loss: 8722.7656\n",
            "Epoch [70/100], Step [601/929], Loss: 9209.8408\n",
            "Epoch [70/100], Step [701/929], Loss: 8575.5605\n",
            "Epoch [70/100], Step [801/929], Loss: 9115.3203\n",
            "Epoch [70/100], Step [901/929], Loss: 8763.2354\n",
            "Epoch [71/100], Step [1/929], Loss: 9267.7109\n",
            "Epoch [71/100], Step [101/929], Loss: 9167.9111\n",
            "Epoch [71/100], Step [201/929], Loss: 8647.8740\n",
            "Epoch [71/100], Step [301/929], Loss: 8010.8555\n",
            "Epoch [71/100], Step [401/929], Loss: 9022.8750\n",
            "Epoch [71/100], Step [501/929], Loss: 8241.7305\n",
            "Epoch [71/100], Step [601/929], Loss: 8902.0986\n",
            "Epoch [71/100], Step [701/929], Loss: 8770.3984\n",
            "Epoch [71/100], Step [801/929], Loss: 8887.5879\n",
            "Epoch [71/100], Step [901/929], Loss: 9149.0576\n",
            "Epoch [72/100], Step [1/929], Loss: 8280.9746\n",
            "Epoch [72/100], Step [101/929], Loss: 8730.0225\n",
            "Epoch [72/100], Step [201/929], Loss: 8703.6504\n",
            "Epoch [72/100], Step [301/929], Loss: 8511.9102\n",
            "Epoch [72/100], Step [401/929], Loss: 8999.8125\n",
            "Epoch [72/100], Step [501/929], Loss: 8806.9512\n",
            "Epoch [72/100], Step [601/929], Loss: 9274.1514\n",
            "Epoch [72/100], Step [701/929], Loss: 8677.0596\n",
            "Epoch [72/100], Step [801/929], Loss: 7902.2510\n",
            "Epoch [72/100], Step [901/929], Loss: 8490.6221\n",
            "Epoch [73/100], Step [1/929], Loss: 8873.1914\n",
            "Epoch [73/100], Step [101/929], Loss: 9027.3896\n",
            "Epoch [73/100], Step [201/929], Loss: 8402.5049\n",
            "Epoch [73/100], Step [301/929], Loss: 8575.4170\n",
            "Epoch [73/100], Step [401/929], Loss: 8569.9795\n",
            "Epoch [73/100], Step [501/929], Loss: 8892.0977\n",
            "Epoch [73/100], Step [601/929], Loss: 8805.0449\n",
            "Epoch [73/100], Step [701/929], Loss: 9030.4727\n",
            "Epoch [73/100], Step [801/929], Loss: 8685.7334\n",
            "Epoch [73/100], Step [901/929], Loss: 8872.2080\n",
            "Epoch [74/100], Step [1/929], Loss: 8917.0293\n",
            "Epoch [74/100], Step [101/929], Loss: 8841.9590\n",
            "Epoch [74/100], Step [201/929], Loss: 8956.4707\n",
            "Epoch [74/100], Step [301/929], Loss: 8498.6924\n",
            "Epoch [74/100], Step [401/929], Loss: 9432.3369\n",
            "Epoch [74/100], Step [501/929], Loss: 8332.0215\n",
            "Epoch [74/100], Step [601/929], Loss: 8764.5469\n",
            "Epoch [74/100], Step [701/929], Loss: 8316.5400\n",
            "Epoch [74/100], Step [801/929], Loss: 8875.0566\n",
            "Epoch [74/100], Step [901/929], Loss: 8739.8643\n",
            "Epoch [75/100], Step [1/929], Loss: 8477.2725\n",
            "Epoch [75/100], Step [101/929], Loss: 8425.8633\n",
            "Epoch [75/100], Step [201/929], Loss: 8056.2764\n",
            "Epoch [75/100], Step [301/929], Loss: 8713.1348\n",
            "Epoch [75/100], Step [401/929], Loss: 8579.8457\n",
            "Epoch [75/100], Step [501/929], Loss: 8314.3398\n",
            "Epoch [75/100], Step [601/929], Loss: 8787.2656\n",
            "Epoch [75/100], Step [701/929], Loss: 9057.2607\n",
            "Epoch [75/100], Step [801/929], Loss: 8643.8965\n",
            "Epoch [75/100], Step [901/929], Loss: 8960.0723\n",
            "Epoch [76/100], Step [1/929], Loss: 8422.8477\n",
            "Epoch [76/100], Step [101/929], Loss: 8150.2666\n",
            "Epoch [76/100], Step [201/929], Loss: 9150.0693\n",
            "Epoch [76/100], Step [301/929], Loss: 8980.6699\n",
            "Epoch [76/100], Step [401/929], Loss: 8554.5654\n",
            "Epoch [76/100], Step [501/929], Loss: 9165.0762\n",
            "Epoch [76/100], Step [601/929], Loss: 8578.7598\n",
            "Epoch [76/100], Step [701/929], Loss: 9205.0068\n",
            "Epoch [76/100], Step [801/929], Loss: 8581.2773\n",
            "Epoch [76/100], Step [901/929], Loss: 9001.6240\n",
            "Epoch [77/100], Step [1/929], Loss: 8736.1992\n",
            "Epoch [77/100], Step [101/929], Loss: 8154.1606\n",
            "Epoch [77/100], Step [201/929], Loss: 8484.9883\n",
            "Epoch [77/100], Step [301/929], Loss: 9033.4385\n",
            "Epoch [77/100], Step [401/929], Loss: 8752.3730\n",
            "Epoch [77/100], Step [501/929], Loss: 9145.8965\n",
            "Epoch [77/100], Step [601/929], Loss: 8973.9980\n",
            "Epoch [77/100], Step [701/929], Loss: 9080.4111\n",
            "Epoch [77/100], Step [801/929], Loss: 8665.5986\n",
            "Epoch [77/100], Step [901/929], Loss: 8789.0127\n",
            "Epoch [78/100], Step [1/929], Loss: 8549.8320\n",
            "Epoch [78/100], Step [101/929], Loss: 8533.2217\n",
            "Epoch [78/100], Step [201/929], Loss: 8929.8896\n",
            "Epoch [78/100], Step [301/929], Loss: 8756.0215\n",
            "Epoch [78/100], Step [401/929], Loss: 8764.0674\n",
            "Epoch [78/100], Step [501/929], Loss: 8374.5439\n",
            "Epoch [78/100], Step [601/929], Loss: 9110.4092\n",
            "Epoch [78/100], Step [701/929], Loss: 9044.6758\n",
            "Epoch [78/100], Step [801/929], Loss: 8917.2168\n",
            "Epoch [78/100], Step [901/929], Loss: 8799.0830\n",
            "Epoch [79/100], Step [1/929], Loss: 9145.5039\n",
            "Epoch [79/100], Step [101/929], Loss: 8394.3203\n",
            "Epoch [79/100], Step [201/929], Loss: 8787.9346\n",
            "Epoch [79/100], Step [301/929], Loss: 8844.3770\n",
            "Epoch [79/100], Step [401/929], Loss: 8890.8223\n",
            "Epoch [79/100], Step [501/929], Loss: 8941.2988\n",
            "Epoch [79/100], Step [601/929], Loss: 9306.5137\n",
            "Epoch [79/100], Step [701/929], Loss: 8754.3086\n",
            "Epoch [79/100], Step [801/929], Loss: 8603.9199\n",
            "Epoch [79/100], Step [901/929], Loss: 8421.7188\n",
            "Epoch [80/100], Step [1/929], Loss: 9038.3008\n",
            "Epoch [80/100], Step [101/929], Loss: 8537.2900\n",
            "Epoch [80/100], Step [201/929], Loss: 8442.0234\n",
            "Epoch [80/100], Step [301/929], Loss: 8234.8232\n",
            "Epoch [80/100], Step [401/929], Loss: 8884.1602\n",
            "Epoch [80/100], Step [501/929], Loss: 8619.2402\n",
            "Epoch [80/100], Step [601/929], Loss: 8870.2930\n",
            "Epoch [80/100], Step [701/929], Loss: 8703.5576\n",
            "Epoch [80/100], Step [801/929], Loss: 8571.9043\n",
            "Epoch [80/100], Step [901/929], Loss: 9408.4482\n",
            "Epoch [81/100], Step [1/929], Loss: 8880.6621\n",
            "Epoch [81/100], Step [101/929], Loss: 8777.9893\n",
            "Epoch [81/100], Step [201/929], Loss: 8512.9023\n",
            "Epoch [81/100], Step [301/929], Loss: 8785.1230\n",
            "Epoch [81/100], Step [401/929], Loss: 8656.1719\n",
            "Epoch [81/100], Step [501/929], Loss: 8968.5029\n",
            "Epoch [81/100], Step [601/929], Loss: 8667.7529\n",
            "Epoch [81/100], Step [701/929], Loss: 8759.5820\n",
            "Epoch [81/100], Step [801/929], Loss: 8531.1445\n",
            "Epoch [81/100], Step [901/929], Loss: 8205.1211\n",
            "Epoch [82/100], Step [1/929], Loss: 8108.2520\n",
            "Epoch [82/100], Step [101/929], Loss: 8542.5752\n",
            "Epoch [82/100], Step [201/929], Loss: 8924.1621\n",
            "Epoch [82/100], Step [301/929], Loss: 9346.4238\n",
            "Epoch [82/100], Step [401/929], Loss: 8647.1621\n",
            "Epoch [82/100], Step [501/929], Loss: 8408.9248\n",
            "Epoch [82/100], Step [601/929], Loss: 9022.4717\n",
            "Epoch [82/100], Step [701/929], Loss: 8723.0859\n",
            "Epoch [82/100], Step [801/929], Loss: 8684.7910\n",
            "Epoch [82/100], Step [901/929], Loss: 9008.3047\n",
            "Epoch [83/100], Step [1/929], Loss: 8950.1660\n",
            "Epoch [83/100], Step [101/929], Loss: 8503.1348\n",
            "Epoch [83/100], Step [201/929], Loss: 9163.2666\n",
            "Epoch [83/100], Step [301/929], Loss: 8826.3848\n",
            "Epoch [83/100], Step [401/929], Loss: 8791.9229\n",
            "Epoch [83/100], Step [501/929], Loss: 8612.0400\n",
            "Epoch [83/100], Step [601/929], Loss: 8708.4121\n",
            "Epoch [83/100], Step [701/929], Loss: 8760.6074\n",
            "Epoch [83/100], Step [801/929], Loss: 8612.6357\n",
            "Epoch [83/100], Step [901/929], Loss: 8971.5859\n",
            "Epoch [84/100], Step [1/929], Loss: 8455.9531\n",
            "Epoch [84/100], Step [101/929], Loss: 8847.5840\n",
            "Epoch [84/100], Step [201/929], Loss: 8421.0723\n",
            "Epoch [84/100], Step [301/929], Loss: 8731.2725\n",
            "Epoch [84/100], Step [401/929], Loss: 9370.5703\n",
            "Epoch [84/100], Step [501/929], Loss: 8829.3584\n",
            "Epoch [84/100], Step [601/929], Loss: 9232.7520\n",
            "Epoch [84/100], Step [701/929], Loss: 9029.0293\n",
            "Epoch [84/100], Step [801/929], Loss: 8803.7051\n",
            "Epoch [84/100], Step [901/929], Loss: 8888.6045\n",
            "Epoch [85/100], Step [1/929], Loss: 9264.8164\n",
            "Epoch [85/100], Step [101/929], Loss: 9092.0430\n",
            "Epoch [85/100], Step [201/929], Loss: 9110.9609\n",
            "Epoch [85/100], Step [301/929], Loss: 7815.7285\n",
            "Epoch [85/100], Step [401/929], Loss: 8590.9043\n",
            "Epoch [85/100], Step [501/929], Loss: 8805.3896\n",
            "Epoch [85/100], Step [601/929], Loss: 8945.5498\n",
            "Epoch [85/100], Step [701/929], Loss: 8653.3984\n",
            "Epoch [85/100], Step [801/929], Loss: 8735.3994\n",
            "Epoch [85/100], Step [901/929], Loss: 8516.5186\n",
            "Epoch [86/100], Step [1/929], Loss: 8645.6807\n",
            "Epoch [86/100], Step [101/929], Loss: 7944.6924\n",
            "Epoch [86/100], Step [201/929], Loss: 8843.1143\n",
            "Epoch [86/100], Step [301/929], Loss: 8751.0742\n",
            "Epoch [86/100], Step [401/929], Loss: 9109.5195\n",
            "Epoch [86/100], Step [501/929], Loss: 9033.3125\n",
            "Epoch [86/100], Step [601/929], Loss: 8852.0977\n",
            "Epoch [86/100], Step [701/929], Loss: 8734.8477\n",
            "Epoch [86/100], Step [801/929], Loss: 8589.8086\n",
            "Epoch [86/100], Step [901/929], Loss: 9217.7910\n",
            "Epoch [87/100], Step [1/929], Loss: 8955.3047\n",
            "Epoch [87/100], Step [101/929], Loss: 8740.8975\n",
            "Epoch [87/100], Step [201/929], Loss: 8474.8115\n",
            "Epoch [87/100], Step [301/929], Loss: 8526.9844\n",
            "Epoch [87/100], Step [401/929], Loss: 9122.4844\n",
            "Epoch [87/100], Step [501/929], Loss: 8006.4395\n",
            "Epoch [87/100], Step [601/929], Loss: 8858.0879\n",
            "Epoch [87/100], Step [701/929], Loss: 8797.2207\n",
            "Epoch [87/100], Step [801/929], Loss: 9079.7900\n",
            "Epoch [87/100], Step [901/929], Loss: 8579.9316\n",
            "Epoch [88/100], Step [1/929], Loss: 8502.5117\n",
            "Epoch [88/100], Step [101/929], Loss: 8450.1719\n",
            "Epoch [88/100], Step [201/929], Loss: 9108.1299\n",
            "Epoch [88/100], Step [301/929], Loss: 8857.7549\n",
            "Epoch [88/100], Step [401/929], Loss: 9235.5801\n",
            "Epoch [88/100], Step [501/929], Loss: 8933.6631\n",
            "Epoch [88/100], Step [601/929], Loss: 9007.8145\n",
            "Epoch [88/100], Step [701/929], Loss: 8255.7539\n",
            "Epoch [88/100], Step [801/929], Loss: 8732.5488\n",
            "Epoch [88/100], Step [901/929], Loss: 8493.5684\n",
            "Epoch [89/100], Step [1/929], Loss: 9149.3145\n",
            "Epoch [89/100], Step [101/929], Loss: 9005.2578\n",
            "Epoch [89/100], Step [201/929], Loss: 8365.7305\n",
            "Epoch [89/100], Step [301/929], Loss: 8064.4434\n",
            "Epoch [89/100], Step [401/929], Loss: 9311.0498\n",
            "Epoch [89/100], Step [501/929], Loss: 8707.2314\n",
            "Epoch [89/100], Step [601/929], Loss: 8990.4707\n",
            "Epoch [89/100], Step [701/929], Loss: 8518.9355\n",
            "Epoch [89/100], Step [801/929], Loss: 8965.9492\n",
            "Epoch [89/100], Step [901/929], Loss: 8615.2568\n",
            "Epoch [90/100], Step [1/929], Loss: 8650.5869\n",
            "Epoch [90/100], Step [101/929], Loss: 9011.1055\n",
            "Epoch [90/100], Step [201/929], Loss: 8249.1055\n",
            "Epoch [90/100], Step [301/929], Loss: 8881.7412\n",
            "Epoch [90/100], Step [401/929], Loss: 8562.3887\n",
            "Epoch [90/100], Step [501/929], Loss: 8735.6699\n",
            "Epoch [90/100], Step [601/929], Loss: 8968.1055\n",
            "Epoch [90/100], Step [701/929], Loss: 9092.1719\n",
            "Epoch [90/100], Step [801/929], Loss: 8832.5264\n",
            "Epoch [90/100], Step [901/929], Loss: 8515.6748\n",
            "Epoch [91/100], Step [1/929], Loss: 8465.5498\n",
            "Epoch [91/100], Step [101/929], Loss: 9080.7930\n",
            "Epoch [91/100], Step [201/929], Loss: 9135.9414\n",
            "Epoch [91/100], Step [301/929], Loss: 8707.7021\n",
            "Epoch [91/100], Step [401/929], Loss: 8459.4023\n",
            "Epoch [91/100], Step [501/929], Loss: 9139.1348\n",
            "Epoch [91/100], Step [601/929], Loss: 8784.2920\n",
            "Epoch [91/100], Step [701/929], Loss: 8717.9355\n",
            "Epoch [91/100], Step [801/929], Loss: 8329.0635\n",
            "Epoch [91/100], Step [901/929], Loss: 9391.3809\n",
            "Epoch [92/100], Step [1/929], Loss: 8678.3594\n",
            "Epoch [92/100], Step [101/929], Loss: 8647.9160\n",
            "Epoch [92/100], Step [201/929], Loss: 9168.8262\n",
            "Epoch [92/100], Step [301/929], Loss: 8444.2275\n",
            "Epoch [92/100], Step [401/929], Loss: 8848.2930\n",
            "Epoch [92/100], Step [501/929], Loss: 8505.0039\n",
            "Epoch [92/100], Step [601/929], Loss: 8680.3037\n",
            "Epoch [92/100], Step [701/929], Loss: 9214.3408\n",
            "Epoch [92/100], Step [801/929], Loss: 8510.1953\n",
            "Epoch [92/100], Step [901/929], Loss: 8636.9580\n",
            "Epoch [93/100], Step [1/929], Loss: 9065.8506\n",
            "Epoch [93/100], Step [101/929], Loss: 8697.3203\n",
            "Epoch [93/100], Step [201/929], Loss: 8425.1084\n",
            "Epoch [93/100], Step [301/929], Loss: 8879.3115\n",
            "Epoch [93/100], Step [401/929], Loss: 8544.0879\n",
            "Epoch [93/100], Step [501/929], Loss: 8733.2939\n",
            "Epoch [93/100], Step [601/929], Loss: 8988.1895\n",
            "Epoch [93/100], Step [701/929], Loss: 8022.4224\n",
            "Epoch [93/100], Step [801/929], Loss: 8833.7041\n",
            "Epoch [93/100], Step [901/929], Loss: 8647.7793\n",
            "Epoch [94/100], Step [1/929], Loss: 8712.2637\n",
            "Epoch [94/100], Step [101/929], Loss: 9119.1895\n",
            "Epoch [94/100], Step [201/929], Loss: 9106.6357\n",
            "Epoch [94/100], Step [301/929], Loss: 8376.9971\n",
            "Epoch [94/100], Step [401/929], Loss: 8878.3857\n",
            "Epoch [94/100], Step [501/929], Loss: 8877.3213\n",
            "Epoch [94/100], Step [601/929], Loss: 8699.8887\n",
            "Epoch [94/100], Step [701/929], Loss: 8755.7871\n",
            "Epoch [94/100], Step [801/929], Loss: 9255.2998\n",
            "Epoch [94/100], Step [901/929], Loss: 9113.8896\n",
            "Epoch [95/100], Step [1/929], Loss: 8899.1895\n",
            "Epoch [95/100], Step [101/929], Loss: 8953.4297\n",
            "Epoch [95/100], Step [201/929], Loss: 8877.4609\n",
            "Epoch [95/100], Step [301/929], Loss: 8400.7998\n",
            "Epoch [95/100], Step [401/929], Loss: 8880.6309\n",
            "Epoch [95/100], Step [501/929], Loss: 8634.3232\n",
            "Epoch [95/100], Step [601/929], Loss: 9038.8105\n",
            "Epoch [95/100], Step [701/929], Loss: 9062.7627\n",
            "Epoch [95/100], Step [801/929], Loss: 9104.6494\n",
            "Epoch [95/100], Step [901/929], Loss: 8844.4482\n",
            "Epoch [96/100], Step [1/929], Loss: 8806.3291\n",
            "Epoch [96/100], Step [101/929], Loss: 9269.0000\n",
            "Epoch [96/100], Step [201/929], Loss: 9179.2666\n",
            "Epoch [96/100], Step [301/929], Loss: 7993.7383\n",
            "Epoch [96/100], Step [401/929], Loss: 9305.5273\n",
            "Epoch [96/100], Step [501/929], Loss: 8902.1133\n",
            "Epoch [96/100], Step [601/929], Loss: 8185.6660\n",
            "Epoch [96/100], Step [701/929], Loss: 8959.8389\n",
            "Epoch [96/100], Step [801/929], Loss: 8727.5742\n",
            "Epoch [96/100], Step [901/929], Loss: 8913.6641\n",
            "Epoch [97/100], Step [1/929], Loss: 8805.2539\n",
            "Epoch [97/100], Step [101/929], Loss: 8913.1611\n",
            "Epoch [97/100], Step [201/929], Loss: 8667.5820\n",
            "Epoch [97/100], Step [301/929], Loss: 8688.2900\n",
            "Epoch [97/100], Step [401/929], Loss: 8464.5146\n",
            "Epoch [97/100], Step [501/929], Loss: 8829.0713\n",
            "Epoch [97/100], Step [601/929], Loss: 9138.2744\n",
            "Epoch [97/100], Step [701/929], Loss: 8732.0312\n",
            "Epoch [97/100], Step [801/929], Loss: 8351.9561\n",
            "Epoch [97/100], Step [901/929], Loss: 8532.7988\n",
            "Epoch [98/100], Step [1/929], Loss: 8494.5322\n",
            "Epoch [98/100], Step [101/929], Loss: 8971.1729\n",
            "Epoch [98/100], Step [201/929], Loss: 9372.2773\n",
            "Epoch [98/100], Step [301/929], Loss: 8657.8936\n",
            "Epoch [98/100], Step [401/929], Loss: 9020.1982\n",
            "Epoch [98/100], Step [501/929], Loss: 8793.8447\n",
            "Epoch [98/100], Step [601/929], Loss: 8891.7256\n",
            "Epoch [98/100], Step [701/929], Loss: 9092.8711\n",
            "Epoch [98/100], Step [801/929], Loss: 9351.0166\n",
            "Epoch [98/100], Step [901/929], Loss: 8823.7559\n",
            "Epoch [99/100], Step [1/929], Loss: 8883.3623\n",
            "Epoch [99/100], Step [101/929], Loss: 8582.6992\n",
            "Epoch [99/100], Step [201/929], Loss: 8983.6963\n",
            "Epoch [99/100], Step [301/929], Loss: 8839.6152\n",
            "Epoch [99/100], Step [401/929], Loss: 8901.7451\n",
            "Epoch [99/100], Step [501/929], Loss: 9265.3711\n",
            "Epoch [99/100], Step [601/929], Loss: 8932.2959\n",
            "Epoch [99/100], Step [701/929], Loss: 8773.2871\n",
            "Epoch [99/100], Step [801/929], Loss: 8563.9727\n",
            "Epoch [99/100], Step [901/929], Loss: 8656.0879\n",
            "Epoch [100/100], Step [1/929], Loss: 8326.5938\n",
            "Epoch [100/100], Step [101/929], Loss: 8753.1768\n",
            "Epoch [100/100], Step [201/929], Loss: 8534.3633\n",
            "Epoch [100/100], Step [301/929], Loss: 9129.5078\n",
            "Epoch [100/100], Step [401/929], Loss: 9041.7441\n",
            "Epoch [100/100], Step [501/929], Loss: 8597.2500\n",
            "Epoch [100/100], Step [601/929], Loss: 8704.0537\n",
            "Epoch [100/100], Step [701/929], Loss: 9403.8018\n",
            "Epoch [100/100], Step [801/929], Loss: 8613.9805\n",
            "Epoch [100/100], Step [901/929], Loss: 8717.4922\n",
            "Accuracy of the SVM on the test images with 600 labeled images: 92.69%\n",
            "Epoch [1/100], Step [1/922], Loss: 37710.9805\n",
            "Epoch [1/100], Step [101/922], Loss: 13446.9121\n",
            "Epoch [1/100], Step [201/922], Loss: 12288.5801\n",
            "Epoch [1/100], Step [301/922], Loss: 12619.9854\n",
            "Epoch [1/100], Step [401/922], Loss: 12019.1816\n",
            "Epoch [1/100], Step [501/922], Loss: 11244.8496\n",
            "Epoch [1/100], Step [601/922], Loss: 11442.8896\n",
            "Epoch [1/100], Step [701/922], Loss: 12143.3135\n",
            "Epoch [1/100], Step [801/922], Loss: 11652.3379\n",
            "Epoch [1/100], Step [901/922], Loss: 11717.9443\n",
            "Epoch [2/100], Step [1/922], Loss: 11380.9844\n",
            "Epoch [2/100], Step [101/922], Loss: 10767.6865\n",
            "Epoch [2/100], Step [201/922], Loss: 10839.2949\n",
            "Epoch [2/100], Step [301/922], Loss: 10942.9160\n",
            "Epoch [2/100], Step [401/922], Loss: 10937.1123\n",
            "Epoch [2/100], Step [501/922], Loss: 10570.5293\n",
            "Epoch [2/100], Step [601/922], Loss: 10627.0830\n",
            "Epoch [2/100], Step [701/922], Loss: 10816.3418\n",
            "Epoch [2/100], Step [801/922], Loss: 10810.9287\n",
            "Epoch [2/100], Step [901/922], Loss: 10681.4678\n",
            "Epoch [3/100], Step [1/922], Loss: 10107.5488\n",
            "Epoch [3/100], Step [101/922], Loss: 10946.7119\n",
            "Epoch [3/100], Step [201/922], Loss: 10466.5156\n",
            "Epoch [3/100], Step [301/922], Loss: 9757.8652\n",
            "Epoch [3/100], Step [401/922], Loss: 9919.6895\n",
            "Epoch [3/100], Step [501/922], Loss: 10682.8271\n",
            "Epoch [3/100], Step [601/922], Loss: 10284.3828\n",
            "Epoch [3/100], Step [701/922], Loss: 10168.4883\n",
            "Epoch [3/100], Step [801/922], Loss: 10191.7402\n",
            "Epoch [3/100], Step [901/922], Loss: 9921.2852\n",
            "Epoch [4/100], Step [1/922], Loss: 9360.5381\n",
            "Epoch [4/100], Step [101/922], Loss: 10254.7979\n",
            "Epoch [4/100], Step [201/922], Loss: 10148.6230\n",
            "Epoch [4/100], Step [301/922], Loss: 10336.8916\n",
            "Epoch [4/100], Step [401/922], Loss: 9738.0928\n",
            "Epoch [4/100], Step [501/922], Loss: 10051.7988\n",
            "Epoch [4/100], Step [601/922], Loss: 9788.2939\n",
            "Epoch [4/100], Step [701/922], Loss: 10074.5781\n",
            "Epoch [4/100], Step [801/922], Loss: 9989.5605\n",
            "Epoch [4/100], Step [901/922], Loss: 10231.5859\n",
            "Epoch [5/100], Step [1/922], Loss: 9473.4141\n",
            "Epoch [5/100], Step [101/922], Loss: 9901.0391\n",
            "Epoch [5/100], Step [201/922], Loss: 10386.4395\n",
            "Epoch [5/100], Step [301/922], Loss: 10107.5361\n",
            "Epoch [5/100], Step [401/922], Loss: 9761.9736\n",
            "Epoch [5/100], Step [501/922], Loss: 10113.9258\n",
            "Epoch [5/100], Step [601/922], Loss: 10298.0137\n",
            "Epoch [5/100], Step [701/922], Loss: 10009.6719\n",
            "Epoch [5/100], Step [801/922], Loss: 9912.3789\n",
            "Epoch [5/100], Step [901/922], Loss: 9276.3809\n",
            "Epoch [6/100], Step [1/922], Loss: 9480.2354\n",
            "Epoch [6/100], Step [101/922], Loss: 9763.5605\n",
            "Epoch [6/100], Step [201/922], Loss: 9218.8730\n",
            "Epoch [6/100], Step [301/922], Loss: 9929.5166\n",
            "Epoch [6/100], Step [401/922], Loss: 9758.1953\n",
            "Epoch [6/100], Step [501/922], Loss: 9412.3848\n",
            "Epoch [6/100], Step [601/922], Loss: 9346.1494\n",
            "Epoch [6/100], Step [701/922], Loss: 10021.7012\n",
            "Epoch [6/100], Step [801/922], Loss: 10251.6953\n",
            "Epoch [6/100], Step [901/922], Loss: 9863.7559\n",
            "Epoch [7/100], Step [1/922], Loss: 9830.4111\n",
            "Epoch [7/100], Step [101/922], Loss: 10119.5693\n",
            "Epoch [7/100], Step [201/922], Loss: 10328.0146\n",
            "Epoch [7/100], Step [301/922], Loss: 10068.7539\n",
            "Epoch [7/100], Step [401/922], Loss: 9255.5771\n",
            "Epoch [7/100], Step [501/922], Loss: 9944.0039\n",
            "Epoch [7/100], Step [601/922], Loss: 9273.1426\n",
            "Epoch [7/100], Step [701/922], Loss: 9162.9512\n",
            "Epoch [7/100], Step [801/922], Loss: 9911.1475\n",
            "Epoch [7/100], Step [901/922], Loss: 10015.0986\n",
            "Epoch [8/100], Step [1/922], Loss: 9820.5215\n",
            "Epoch [8/100], Step [101/922], Loss: 9591.2881\n",
            "Epoch [8/100], Step [201/922], Loss: 9453.2822\n",
            "Epoch [8/100], Step [301/922], Loss: 9203.8057\n",
            "Epoch [8/100], Step [401/922], Loss: 9581.4131\n",
            "Epoch [8/100], Step [501/922], Loss: 9127.5859\n",
            "Epoch [8/100], Step [601/922], Loss: 9648.5771\n",
            "Epoch [8/100], Step [701/922], Loss: 9520.2988\n",
            "Epoch [8/100], Step [801/922], Loss: 8812.2041\n",
            "Epoch [8/100], Step [901/922], Loss: 9075.7422\n",
            "Epoch [9/100], Step [1/922], Loss: 10010.2285\n",
            "Epoch [9/100], Step [101/922], Loss: 9140.2041\n",
            "Epoch [9/100], Step [201/922], Loss: 10121.7461\n",
            "Epoch [9/100], Step [301/922], Loss: 9510.6396\n",
            "Epoch [9/100], Step [401/922], Loss: 9504.5684\n",
            "Epoch [9/100], Step [501/922], Loss: 9449.3047\n",
            "Epoch [9/100], Step [601/922], Loss: 9817.8486\n",
            "Epoch [9/100], Step [701/922], Loss: 8932.9893\n",
            "Epoch [9/100], Step [801/922], Loss: 9982.9590\n",
            "Epoch [9/100], Step [901/922], Loss: 9744.2383\n",
            "Epoch [10/100], Step [1/922], Loss: 9520.5410\n",
            "Epoch [10/100], Step [101/922], Loss: 9160.0605\n",
            "Epoch [10/100], Step [201/922], Loss: 9344.5049\n",
            "Epoch [10/100], Step [301/922], Loss: 9693.6748\n",
            "Epoch [10/100], Step [401/922], Loss: 9157.2139\n",
            "Epoch [10/100], Step [501/922], Loss: 8822.5176\n",
            "Epoch [10/100], Step [601/922], Loss: 9153.0879\n",
            "Epoch [10/100], Step [701/922], Loss: 9457.3340\n",
            "Epoch [10/100], Step [801/922], Loss: 9066.6943\n",
            "Epoch [10/100], Step [901/922], Loss: 9330.5576\n",
            "Epoch [11/100], Step [1/922], Loss: 9247.3311\n",
            "Epoch [11/100], Step [101/922], Loss: 9709.4512\n",
            "Epoch [11/100], Step [201/922], Loss: 9391.9521\n",
            "Epoch [11/100], Step [301/922], Loss: 9755.6387\n",
            "Epoch [11/100], Step [401/922], Loss: 9670.4424\n",
            "Epoch [11/100], Step [501/922], Loss: 9251.6982\n",
            "Epoch [11/100], Step [601/922], Loss: 9441.9785\n",
            "Epoch [11/100], Step [701/922], Loss: 9576.1221\n",
            "Epoch [11/100], Step [801/922], Loss: 9503.7861\n",
            "Epoch [11/100], Step [901/922], Loss: 9809.2148\n",
            "Epoch [12/100], Step [1/922], Loss: 9365.9521\n",
            "Epoch [12/100], Step [101/922], Loss: 9906.3770\n",
            "Epoch [12/100], Step [201/922], Loss: 9255.2129\n",
            "Epoch [12/100], Step [301/922], Loss: 9690.0645\n",
            "Epoch [12/100], Step [401/922], Loss: 9451.2500\n",
            "Epoch [12/100], Step [501/922], Loss: 9413.2705\n",
            "Epoch [12/100], Step [601/922], Loss: 9667.6152\n",
            "Epoch [12/100], Step [701/922], Loss: 9308.9180\n",
            "Epoch [12/100], Step [801/922], Loss: 8785.8887\n",
            "Epoch [12/100], Step [901/922], Loss: 9247.7539\n",
            "Epoch [13/100], Step [1/922], Loss: 8774.4590\n",
            "Epoch [13/100], Step [101/922], Loss: 9524.7168\n",
            "Epoch [13/100], Step [201/922], Loss: 8981.0352\n",
            "Epoch [13/100], Step [301/922], Loss: 9621.6709\n",
            "Epoch [13/100], Step [401/922], Loss: 8572.5781\n",
            "Epoch [13/100], Step [501/922], Loss: 8766.9141\n",
            "Epoch [13/100], Step [601/922], Loss: 9391.1992\n",
            "Epoch [13/100], Step [701/922], Loss: 8922.1113\n",
            "Epoch [13/100], Step [801/922], Loss: 9635.2227\n",
            "Epoch [13/100], Step [901/922], Loss: 9214.8896\n",
            "Epoch [14/100], Step [1/922], Loss: 9417.1074\n",
            "Epoch [14/100], Step [101/922], Loss: 9283.3330\n",
            "Epoch [14/100], Step [201/922], Loss: 9086.2861\n",
            "Epoch [14/100], Step [301/922], Loss: 8993.5039\n",
            "Epoch [14/100], Step [401/922], Loss: 9183.6035\n",
            "Epoch [14/100], Step [501/922], Loss: 9543.0889\n",
            "Epoch [14/100], Step [601/922], Loss: 9236.3662\n",
            "Epoch [14/100], Step [701/922], Loss: 9676.3271\n",
            "Epoch [14/100], Step [801/922], Loss: 9570.8398\n",
            "Epoch [14/100], Step [901/922], Loss: 9420.2969\n",
            "Epoch [15/100], Step [1/922], Loss: 9103.5010\n",
            "Epoch [15/100], Step [101/922], Loss: 9525.8066\n",
            "Epoch [15/100], Step [201/922], Loss: 9434.0117\n",
            "Epoch [15/100], Step [301/922], Loss: 8775.2754\n",
            "Epoch [15/100], Step [401/922], Loss: 8939.1133\n",
            "Epoch [15/100], Step [501/922], Loss: 9696.9814\n",
            "Epoch [15/100], Step [601/922], Loss: 9207.7783\n",
            "Epoch [15/100], Step [701/922], Loss: 9367.3066\n",
            "Epoch [15/100], Step [801/922], Loss: 9746.3125\n",
            "Epoch [15/100], Step [901/922], Loss: 9072.5967\n",
            "Epoch [16/100], Step [1/922], Loss: 9439.5498\n",
            "Epoch [16/100], Step [101/922], Loss: 9327.6328\n",
            "Epoch [16/100], Step [201/922], Loss: 9436.9854\n",
            "Epoch [16/100], Step [301/922], Loss: 8964.5088\n",
            "Epoch [16/100], Step [401/922], Loss: 9729.2607\n",
            "Epoch [16/100], Step [501/922], Loss: 8803.2900\n",
            "Epoch [16/100], Step [601/922], Loss: 9386.9785\n",
            "Epoch [16/100], Step [701/922], Loss: 8867.6523\n",
            "Epoch [16/100], Step [801/922], Loss: 9567.8076\n",
            "Epoch [16/100], Step [901/922], Loss: 9088.1006\n",
            "Epoch [17/100], Step [1/922], Loss: 9694.0342\n",
            "Epoch [17/100], Step [101/922], Loss: 9494.2861\n",
            "Epoch [17/100], Step [201/922], Loss: 9233.7402\n",
            "Epoch [17/100], Step [301/922], Loss: 8938.9580\n",
            "Epoch [17/100], Step [401/922], Loss: 9035.2734\n",
            "Epoch [17/100], Step [501/922], Loss: 8919.0713\n",
            "Epoch [17/100], Step [601/922], Loss: 8957.3535\n",
            "Epoch [17/100], Step [701/922], Loss: 9022.2305\n",
            "Epoch [17/100], Step [801/922], Loss: 8933.4717\n",
            "Epoch [17/100], Step [901/922], Loss: 8835.6777\n",
            "Epoch [18/100], Step [1/922], Loss: 9082.8154\n",
            "Epoch [18/100], Step [101/922], Loss: 8879.2930\n",
            "Epoch [18/100], Step [201/922], Loss: 8773.6562\n",
            "Epoch [18/100], Step [301/922], Loss: 9287.2441\n",
            "Epoch [18/100], Step [401/922], Loss: 8886.1777\n",
            "Epoch [18/100], Step [501/922], Loss: 9311.7461\n",
            "Epoch [18/100], Step [601/922], Loss: 8353.0166\n",
            "Epoch [18/100], Step [701/922], Loss: 9336.1895\n",
            "Epoch [18/100], Step [801/922], Loss: 8850.6377\n",
            "Epoch [18/100], Step [901/922], Loss: 8782.8828\n",
            "Epoch [19/100], Step [1/922], Loss: 9134.8848\n",
            "Epoch [19/100], Step [101/922], Loss: 8999.3857\n",
            "Epoch [19/100], Step [201/922], Loss: 9113.2920\n",
            "Epoch [19/100], Step [301/922], Loss: 9086.5156\n",
            "Epoch [19/100], Step [401/922], Loss: 8778.7324\n",
            "Epoch [19/100], Step [501/922], Loss: 8954.6025\n",
            "Epoch [19/100], Step [601/922], Loss: 8809.5078\n",
            "Epoch [19/100], Step [701/922], Loss: 8811.2803\n",
            "Epoch [19/100], Step [801/922], Loss: 8338.7422\n",
            "Epoch [19/100], Step [901/922], Loss: 9556.2070\n",
            "Epoch [20/100], Step [1/922], Loss: 9391.8662\n",
            "Epoch [20/100], Step [101/922], Loss: 8856.7803\n",
            "Epoch [20/100], Step [201/922], Loss: 9042.4932\n",
            "Epoch [20/100], Step [301/922], Loss: 9165.9883\n",
            "Epoch [20/100], Step [401/922], Loss: 9673.9883\n",
            "Epoch [20/100], Step [501/922], Loss: 8650.8340\n",
            "Epoch [20/100], Step [601/922], Loss: 9334.6699\n",
            "Epoch [20/100], Step [701/922], Loss: 8944.3369\n",
            "Epoch [20/100], Step [801/922], Loss: 9211.7578\n",
            "Epoch [20/100], Step [901/922], Loss: 8973.9023\n",
            "Epoch [21/100], Step [1/922], Loss: 8850.1904\n",
            "Epoch [21/100], Step [101/922], Loss: 9044.6719\n",
            "Epoch [21/100], Step [201/922], Loss: 8553.6904\n",
            "Epoch [21/100], Step [301/922], Loss: 8785.3818\n",
            "Epoch [21/100], Step [401/922], Loss: 9300.0439\n",
            "Epoch [21/100], Step [501/922], Loss: 9587.1367\n",
            "Epoch [21/100], Step [601/922], Loss: 9133.3936\n",
            "Epoch [21/100], Step [701/922], Loss: 9127.0479\n",
            "Epoch [21/100], Step [801/922], Loss: 9291.5918\n",
            "Epoch [21/100], Step [901/922], Loss: 8999.6230\n",
            "Epoch [22/100], Step [1/922], Loss: 9723.5635\n",
            "Epoch [22/100], Step [101/922], Loss: 9042.0850\n",
            "Epoch [22/100], Step [201/922], Loss: 8988.8242\n",
            "Epoch [22/100], Step [301/922], Loss: 9376.5479\n",
            "Epoch [22/100], Step [401/922], Loss: 9009.0430\n",
            "Epoch [22/100], Step [501/922], Loss: 8477.5732\n",
            "Epoch [22/100], Step [601/922], Loss: 9060.7598\n",
            "Epoch [22/100], Step [701/922], Loss: 8758.5098\n",
            "Epoch [22/100], Step [801/922], Loss: 8247.5537\n",
            "Epoch [22/100], Step [901/922], Loss: 8818.6709\n",
            "Epoch [23/100], Step [1/922], Loss: 9030.6094\n",
            "Epoch [23/100], Step [101/922], Loss: 8705.0742\n",
            "Epoch [23/100], Step [201/922], Loss: 9143.6182\n",
            "Epoch [23/100], Step [301/922], Loss: 9047.7783\n",
            "Epoch [23/100], Step [401/922], Loss: 8876.0859\n",
            "Epoch [23/100], Step [501/922], Loss: 9287.9561\n",
            "Epoch [23/100], Step [601/922], Loss: 8567.2119\n",
            "Epoch [23/100], Step [701/922], Loss: 8952.6582\n",
            "Epoch [23/100], Step [801/922], Loss: 9202.8213\n",
            "Epoch [23/100], Step [901/922], Loss: 9371.4307\n",
            "Epoch [24/100], Step [1/922], Loss: 8975.6191\n",
            "Epoch [24/100], Step [101/922], Loss: 9220.5977\n",
            "Epoch [24/100], Step [201/922], Loss: 9394.8633\n",
            "Epoch [24/100], Step [301/922], Loss: 8759.3389\n",
            "Epoch [24/100], Step [401/922], Loss: 8685.9277\n",
            "Epoch [24/100], Step [501/922], Loss: 8942.6689\n",
            "Epoch [24/100], Step [601/922], Loss: 8620.0557\n",
            "Epoch [24/100], Step [701/922], Loss: 8972.1816\n",
            "Epoch [24/100], Step [801/922], Loss: 9369.8584\n",
            "Epoch [24/100], Step [901/922], Loss: 9506.0322\n",
            "Epoch [25/100], Step [1/922], Loss: 8538.5449\n",
            "Epoch [25/100], Step [101/922], Loss: 8904.1484\n",
            "Epoch [25/100], Step [201/922], Loss: 9014.0547\n",
            "Epoch [25/100], Step [301/922], Loss: 8682.6143\n",
            "Epoch [25/100], Step [401/922], Loss: 8850.2617\n",
            "Epoch [25/100], Step [501/922], Loss: 9418.5098\n",
            "Epoch [25/100], Step [601/922], Loss: 9346.6963\n",
            "Epoch [25/100], Step [701/922], Loss: 8455.6875\n",
            "Epoch [25/100], Step [801/922], Loss: 9035.2432\n",
            "Epoch [25/100], Step [901/922], Loss: 8914.6719\n",
            "Epoch [26/100], Step [1/922], Loss: 9077.3330\n",
            "Epoch [26/100], Step [101/922], Loss: 8939.2617\n",
            "Epoch [26/100], Step [201/922], Loss: 8744.7793\n",
            "Epoch [26/100], Step [301/922], Loss: 9057.3467\n",
            "Epoch [26/100], Step [401/922], Loss: 9259.2539\n",
            "Epoch [26/100], Step [501/922], Loss: 9180.2646\n",
            "Epoch [26/100], Step [601/922], Loss: 8846.6113\n",
            "Epoch [26/100], Step [701/922], Loss: 8808.5264\n",
            "Epoch [26/100], Step [801/922], Loss: 8596.4170\n",
            "Epoch [26/100], Step [901/922], Loss: 9074.2412\n",
            "Epoch [27/100], Step [1/922], Loss: 9203.3975\n",
            "Epoch [27/100], Step [101/922], Loss: 9222.7236\n",
            "Epoch [27/100], Step [201/922], Loss: 8738.5078\n",
            "Epoch [27/100], Step [301/922], Loss: 9070.9531\n",
            "Epoch [27/100], Step [401/922], Loss: 8700.8848\n",
            "Epoch [27/100], Step [501/922], Loss: 8780.8789\n",
            "Epoch [27/100], Step [601/922], Loss: 8593.7695\n",
            "Epoch [27/100], Step [701/922], Loss: 8848.2539\n",
            "Epoch [27/100], Step [801/922], Loss: 8857.6436\n",
            "Epoch [27/100], Step [901/922], Loss: 9022.2070\n",
            "Epoch [28/100], Step [1/922], Loss: 9705.5703\n",
            "Epoch [28/100], Step [101/922], Loss: 8992.3906\n",
            "Epoch [28/100], Step [201/922], Loss: 8762.2754\n",
            "Epoch [28/100], Step [301/922], Loss: 9169.5830\n",
            "Epoch [28/100], Step [401/922], Loss: 9157.7871\n",
            "Epoch [28/100], Step [501/922], Loss: 8370.3828\n",
            "Epoch [28/100], Step [601/922], Loss: 8984.1973\n",
            "Epoch [28/100], Step [701/922], Loss: 9269.4434\n",
            "Epoch [28/100], Step [801/922], Loss: 9158.3721\n",
            "Epoch [28/100], Step [901/922], Loss: 9395.7188\n",
            "Epoch [29/100], Step [1/922], Loss: 8458.9268\n",
            "Epoch [29/100], Step [101/922], Loss: 9063.2285\n",
            "Epoch [29/100], Step [201/922], Loss: 9016.5508\n",
            "Epoch [29/100], Step [301/922], Loss: 8536.2529\n",
            "Epoch [29/100], Step [401/922], Loss: 8883.6289\n",
            "Epoch [29/100], Step [501/922], Loss: 9053.0449\n",
            "Epoch [29/100], Step [601/922], Loss: 8714.6973\n",
            "Epoch [29/100], Step [701/922], Loss: 8621.7402\n",
            "Epoch [29/100], Step [801/922], Loss: 8738.9971\n",
            "Epoch [29/100], Step [901/922], Loss: 9154.1885\n",
            "Epoch [30/100], Step [1/922], Loss: 8882.5479\n",
            "Epoch [30/100], Step [101/922], Loss: 8842.9766\n",
            "Epoch [30/100], Step [201/922], Loss: 9103.9775\n",
            "Epoch [30/100], Step [301/922], Loss: 9234.5811\n",
            "Epoch [30/100], Step [401/922], Loss: 9043.6250\n",
            "Epoch [30/100], Step [501/922], Loss: 8775.7510\n",
            "Epoch [30/100], Step [601/922], Loss: 8903.2334\n",
            "Epoch [30/100], Step [701/922], Loss: 8816.7656\n",
            "Epoch [30/100], Step [801/922], Loss: 8821.8789\n",
            "Epoch [30/100], Step [901/922], Loss: 8784.5000\n",
            "Epoch [31/100], Step [1/922], Loss: 8871.1123\n",
            "Epoch [31/100], Step [101/922], Loss: 8855.4336\n",
            "Epoch [31/100], Step [201/922], Loss: 8748.5078\n",
            "Epoch [31/100], Step [301/922], Loss: 9278.6885\n",
            "Epoch [31/100], Step [401/922], Loss: 9162.7549\n",
            "Epoch [31/100], Step [501/922], Loss: 8944.7705\n",
            "Epoch [31/100], Step [601/922], Loss: 9057.1250\n",
            "Epoch [31/100], Step [701/922], Loss: 9012.5439\n",
            "Epoch [31/100], Step [801/922], Loss: 8837.8301\n",
            "Epoch [31/100], Step [901/922], Loss: 8931.0889\n",
            "Epoch [32/100], Step [1/922], Loss: 9253.5039\n",
            "Epoch [32/100], Step [101/922], Loss: 9523.7842\n",
            "Epoch [32/100], Step [201/922], Loss: 8739.5234\n",
            "Epoch [32/100], Step [301/922], Loss: 8696.2539\n",
            "Epoch [32/100], Step [401/922], Loss: 8589.6191\n",
            "Epoch [32/100], Step [501/922], Loss: 8934.0439\n",
            "Epoch [32/100], Step [601/922], Loss: 8634.6719\n",
            "Epoch [32/100], Step [701/922], Loss: 9047.0625\n",
            "Epoch [32/100], Step [801/922], Loss: 8956.1367\n",
            "Epoch [32/100], Step [901/922], Loss: 9310.7715\n",
            "Epoch [33/100], Step [1/922], Loss: 8619.9141\n",
            "Epoch [33/100], Step [101/922], Loss: 8907.1602\n",
            "Epoch [33/100], Step [201/922], Loss: 9331.5859\n",
            "Epoch [33/100], Step [301/922], Loss: 9397.9756\n",
            "Epoch [33/100], Step [401/922], Loss: 9233.7568\n",
            "Epoch [33/100], Step [501/922], Loss: 8814.3652\n",
            "Epoch [33/100], Step [601/922], Loss: 9071.4326\n",
            "Epoch [33/100], Step [701/922], Loss: 9033.8984\n",
            "Epoch [33/100], Step [801/922], Loss: 9319.4502\n",
            "Epoch [33/100], Step [901/922], Loss: 9072.0508\n",
            "Epoch [34/100], Step [1/922], Loss: 8924.8975\n",
            "Epoch [34/100], Step [101/922], Loss: 9152.0596\n",
            "Epoch [34/100], Step [201/922], Loss: 9328.5938\n",
            "Epoch [34/100], Step [301/922], Loss: 8481.4512\n",
            "Epoch [34/100], Step [401/922], Loss: 8447.6787\n",
            "Epoch [34/100], Step [501/922], Loss: 9177.6113\n",
            "Epoch [34/100], Step [601/922], Loss: 9308.4219\n",
            "Epoch [34/100], Step [701/922], Loss: 8673.7256\n",
            "Epoch [34/100], Step [801/922], Loss: 8768.1738\n",
            "Epoch [34/100], Step [901/922], Loss: 8734.5049\n",
            "Epoch [35/100], Step [1/922], Loss: 9070.1992\n",
            "Epoch [35/100], Step [101/922], Loss: 9026.3750\n",
            "Epoch [35/100], Step [201/922], Loss: 8606.2969\n",
            "Epoch [35/100], Step [301/922], Loss: 9466.3555\n",
            "Epoch [35/100], Step [401/922], Loss: 8471.3047\n",
            "Epoch [35/100], Step [501/922], Loss: 9716.2441\n",
            "Epoch [35/100], Step [601/922], Loss: 9493.1113\n",
            "Epoch [35/100], Step [701/922], Loss: 9069.5664\n",
            "Epoch [35/100], Step [801/922], Loss: 9141.6045\n",
            "Epoch [35/100], Step [901/922], Loss: 8612.3887\n",
            "Epoch [36/100], Step [1/922], Loss: 8839.4268\n",
            "Epoch [36/100], Step [101/922], Loss: 8673.1504\n",
            "Epoch [36/100], Step [201/922], Loss: 8914.2324\n",
            "Epoch [36/100], Step [301/922], Loss: 8856.5186\n",
            "Epoch [36/100], Step [401/922], Loss: 8758.9434\n",
            "Epoch [36/100], Step [501/922], Loss: 9044.4219\n",
            "Epoch [36/100], Step [601/922], Loss: 9203.2793\n",
            "Epoch [36/100], Step [701/922], Loss: 8925.0879\n",
            "Epoch [36/100], Step [801/922], Loss: 8906.0205\n",
            "Epoch [36/100], Step [901/922], Loss: 8796.7188\n",
            "Epoch [37/100], Step [1/922], Loss: 8787.8838\n",
            "Epoch [37/100], Step [101/922], Loss: 9016.2588\n",
            "Epoch [37/100], Step [201/922], Loss: 8915.2246\n",
            "Epoch [37/100], Step [301/922], Loss: 9462.0527\n",
            "Epoch [37/100], Step [401/922], Loss: 9052.3584\n",
            "Epoch [37/100], Step [501/922], Loss: 9352.9316\n",
            "Epoch [37/100], Step [601/922], Loss: 8975.4844\n",
            "Epoch [37/100], Step [701/922], Loss: 9110.1777\n",
            "Epoch [37/100], Step [801/922], Loss: 9109.4746\n",
            "Epoch [37/100], Step [901/922], Loss: 9156.5576\n",
            "Epoch [38/100], Step [1/922], Loss: 8941.4697\n",
            "Epoch [38/100], Step [101/922], Loss: 8971.0859\n",
            "Epoch [38/100], Step [201/922], Loss: 9160.9492\n",
            "Epoch [38/100], Step [301/922], Loss: 9354.6162\n",
            "Epoch [38/100], Step [401/922], Loss: 9116.3301\n",
            "Epoch [38/100], Step [501/922], Loss: 9461.3057\n",
            "Epoch [38/100], Step [601/922], Loss: 9237.8076\n",
            "Epoch [38/100], Step [701/922], Loss: 9038.2930\n",
            "Epoch [38/100], Step [801/922], Loss: 8719.1855\n",
            "Epoch [38/100], Step [901/922], Loss: 8683.3359\n",
            "Epoch [39/100], Step [1/922], Loss: 8372.3955\n",
            "Epoch [39/100], Step [101/922], Loss: 9171.8350\n",
            "Epoch [39/100], Step [201/922], Loss: 8772.8545\n",
            "Epoch [39/100], Step [301/922], Loss: 8824.6699\n",
            "Epoch [39/100], Step [401/922], Loss: 8834.2354\n",
            "Epoch [39/100], Step [501/922], Loss: 8493.9648\n",
            "Epoch [39/100], Step [601/922], Loss: 8882.0977\n",
            "Epoch [39/100], Step [701/922], Loss: 8939.7969\n",
            "Epoch [39/100], Step [801/922], Loss: 9128.1611\n",
            "Epoch [39/100], Step [901/922], Loss: 9323.5615\n",
            "Epoch [40/100], Step [1/922], Loss: 8604.9111\n",
            "Epoch [40/100], Step [101/922], Loss: 9069.1279\n",
            "Epoch [40/100], Step [201/922], Loss: 9049.8652\n",
            "Epoch [40/100], Step [301/922], Loss: 8900.5947\n",
            "Epoch [40/100], Step [401/922], Loss: 8722.0938\n",
            "Epoch [40/100], Step [501/922], Loss: 9805.6211\n",
            "Epoch [40/100], Step [601/922], Loss: 8931.0498\n",
            "Epoch [40/100], Step [701/922], Loss: 8976.6416\n",
            "Epoch [40/100], Step [801/922], Loss: 8889.2305\n",
            "Epoch [40/100], Step [901/922], Loss: 9142.4727\n",
            "Epoch [41/100], Step [1/922], Loss: 8794.5645\n",
            "Epoch [41/100], Step [101/922], Loss: 8373.0859\n",
            "Epoch [41/100], Step [201/922], Loss: 8740.7090\n",
            "Epoch [41/100], Step [301/922], Loss: 9388.9961\n",
            "Epoch [41/100], Step [401/922], Loss: 9097.6582\n",
            "Epoch [41/100], Step [501/922], Loss: 8774.7285\n",
            "Epoch [41/100], Step [601/922], Loss: 8901.6748\n",
            "Epoch [41/100], Step [701/922], Loss: 8953.8086\n",
            "Epoch [41/100], Step [801/922], Loss: 9251.9102\n",
            "Epoch [41/100], Step [901/922], Loss: 8940.7236\n",
            "Epoch [42/100], Step [1/922], Loss: 8737.0049\n",
            "Epoch [42/100], Step [101/922], Loss: 8861.5449\n",
            "Epoch [42/100], Step [201/922], Loss: 8680.4531\n",
            "Epoch [42/100], Step [301/922], Loss: 8735.9512\n",
            "Epoch [42/100], Step [401/922], Loss: 8744.3213\n",
            "Epoch [42/100], Step [501/922], Loss: 8326.3555\n",
            "Epoch [42/100], Step [601/922], Loss: 9312.4473\n",
            "Epoch [42/100], Step [701/922], Loss: 8784.1367\n",
            "Epoch [42/100], Step [801/922], Loss: 9044.7812\n",
            "Epoch [42/100], Step [901/922], Loss: 8419.0049\n",
            "Epoch [43/100], Step [1/922], Loss: 9029.0254\n",
            "Epoch [43/100], Step [101/922], Loss: 8667.9658\n",
            "Epoch [43/100], Step [201/922], Loss: 8829.0312\n",
            "Epoch [43/100], Step [301/922], Loss: 9437.4941\n",
            "Epoch [43/100], Step [401/922], Loss: 8919.2529\n",
            "Epoch [43/100], Step [501/922], Loss: 9209.9092\n",
            "Epoch [43/100], Step [601/922], Loss: 8985.0508\n",
            "Epoch [43/100], Step [701/922], Loss: 8823.5020\n",
            "Epoch [43/100], Step [801/922], Loss: 8958.4414\n",
            "Epoch [43/100], Step [901/922], Loss: 9072.4697\n",
            "Epoch [44/100], Step [1/922], Loss: 8696.4717\n",
            "Epoch [44/100], Step [101/922], Loss: 9050.9814\n",
            "Epoch [44/100], Step [201/922], Loss: 8581.5420\n",
            "Epoch [44/100], Step [301/922], Loss: 8947.6152\n",
            "Epoch [44/100], Step [401/922], Loss: 9010.5889\n",
            "Epoch [44/100], Step [501/922], Loss: 8997.6064\n",
            "Epoch [44/100], Step [601/922], Loss: 8734.7598\n",
            "Epoch [44/100], Step [701/922], Loss: 8951.2402\n",
            "Epoch [44/100], Step [801/922], Loss: 9318.5117\n",
            "Epoch [44/100], Step [901/922], Loss: 8921.5137\n",
            "Epoch [45/100], Step [1/922], Loss: 8780.3691\n",
            "Epoch [45/100], Step [101/922], Loss: 8851.1641\n",
            "Epoch [45/100], Step [201/922], Loss: 8544.8232\n",
            "Epoch [45/100], Step [301/922], Loss: 8608.1836\n",
            "Epoch [45/100], Step [401/922], Loss: 8957.3779\n",
            "Epoch [45/100], Step [501/922], Loss: 9068.4873\n",
            "Epoch [45/100], Step [601/922], Loss: 8571.8418\n",
            "Epoch [45/100], Step [701/922], Loss: 8650.1963\n",
            "Epoch [45/100], Step [801/922], Loss: 9005.0908\n",
            "Epoch [45/100], Step [901/922], Loss: 9036.1602\n",
            "Epoch [46/100], Step [1/922], Loss: 8504.8154\n",
            "Epoch [46/100], Step [101/922], Loss: 8905.2090\n",
            "Epoch [46/100], Step [201/922], Loss: 9035.3828\n",
            "Epoch [46/100], Step [301/922], Loss: 8809.8193\n",
            "Epoch [46/100], Step [401/922], Loss: 9016.6162\n",
            "Epoch [46/100], Step [501/922], Loss: 8722.2461\n",
            "Epoch [46/100], Step [601/922], Loss: 8810.9678\n",
            "Epoch [46/100], Step [701/922], Loss: 8944.0020\n",
            "Epoch [46/100], Step [801/922], Loss: 9190.5830\n",
            "Epoch [46/100], Step [901/922], Loss: 8698.3105\n",
            "Epoch [47/100], Step [1/922], Loss: 8742.6562\n",
            "Epoch [47/100], Step [101/922], Loss: 8705.4014\n",
            "Epoch [47/100], Step [201/922], Loss: 8869.7002\n",
            "Epoch [47/100], Step [301/922], Loss: 8595.2715\n",
            "Epoch [47/100], Step [401/922], Loss: 8897.1260\n",
            "Epoch [47/100], Step [501/922], Loss: 9065.3770\n",
            "Epoch [47/100], Step [601/922], Loss: 8564.0518\n",
            "Epoch [47/100], Step [701/922], Loss: 8793.1055\n",
            "Epoch [47/100], Step [801/922], Loss: 9602.9609\n",
            "Epoch [47/100], Step [901/922], Loss: 9144.1973\n",
            "Epoch [48/100], Step [1/922], Loss: 8753.3477\n",
            "Epoch [48/100], Step [101/922], Loss: 8768.9629\n",
            "Epoch [48/100], Step [201/922], Loss: 9388.5273\n",
            "Epoch [48/100], Step [301/922], Loss: 8970.7422\n",
            "Epoch [48/100], Step [401/922], Loss: 8967.8535\n",
            "Epoch [48/100], Step [501/922], Loss: 8684.7939\n",
            "Epoch [48/100], Step [601/922], Loss: 8681.7119\n",
            "Epoch [48/100], Step [701/922], Loss: 8518.1104\n",
            "Epoch [48/100], Step [801/922], Loss: 9112.6455\n",
            "Epoch [48/100], Step [901/922], Loss: 8538.5283\n",
            "Epoch [49/100], Step [1/922], Loss: 8948.4863\n",
            "Epoch [49/100], Step [101/922], Loss: 8887.1289\n",
            "Epoch [49/100], Step [201/922], Loss: 8669.9561\n",
            "Epoch [49/100], Step [301/922], Loss: 8912.2969\n",
            "Epoch [49/100], Step [401/922], Loss: 8354.1387\n",
            "Epoch [49/100], Step [501/922], Loss: 8976.0898\n",
            "Epoch [49/100], Step [601/922], Loss: 8717.1709\n",
            "Epoch [49/100], Step [701/922], Loss: 8626.4492\n",
            "Epoch [49/100], Step [801/922], Loss: 8849.9395\n",
            "Epoch [49/100], Step [901/922], Loss: 8648.7959\n",
            "Epoch [50/100], Step [1/922], Loss: 9150.0352\n",
            "Epoch [50/100], Step [101/922], Loss: 8823.4092\n",
            "Epoch [50/100], Step [201/922], Loss: 9272.6436\n",
            "Epoch [50/100], Step [301/922], Loss: 8673.3389\n",
            "Epoch [50/100], Step [401/922], Loss: 8810.2705\n",
            "Epoch [50/100], Step [501/922], Loss: 8990.8408\n",
            "Epoch [50/100], Step [601/922], Loss: 8741.0283\n",
            "Epoch [50/100], Step [701/922], Loss: 8885.9766\n",
            "Epoch [50/100], Step [801/922], Loss: 8601.4785\n",
            "Epoch [50/100], Step [901/922], Loss: 9360.0049\n",
            "Epoch [51/100], Step [1/922], Loss: 9100.7471\n",
            "Epoch [51/100], Step [101/922], Loss: 9086.4043\n",
            "Epoch [51/100], Step [201/922], Loss: 9074.9102\n",
            "Epoch [51/100], Step [301/922], Loss: 8350.6855\n",
            "Epoch [51/100], Step [401/922], Loss: 8671.3096\n",
            "Epoch [51/100], Step [501/922], Loss: 9080.4648\n",
            "Epoch [51/100], Step [601/922], Loss: 8261.1240\n",
            "Epoch [51/100], Step [701/922], Loss: 8624.3447\n",
            "Epoch [51/100], Step [801/922], Loss: 8981.0312\n",
            "Epoch [51/100], Step [901/922], Loss: 8867.5156\n",
            "Epoch [52/100], Step [1/922], Loss: 8727.0664\n",
            "Epoch [52/100], Step [101/922], Loss: 8954.4805\n",
            "Epoch [52/100], Step [201/922], Loss: 8798.3145\n",
            "Epoch [52/100], Step [301/922], Loss: 9181.7744\n",
            "Epoch [52/100], Step [401/922], Loss: 8054.0679\n",
            "Epoch [52/100], Step [501/922], Loss: 8975.0547\n",
            "Epoch [52/100], Step [601/922], Loss: 8942.1572\n",
            "Epoch [52/100], Step [701/922], Loss: 8770.0010\n",
            "Epoch [52/100], Step [801/922], Loss: 9136.7207\n",
            "Epoch [52/100], Step [901/922], Loss: 8984.1729\n",
            "Epoch [53/100], Step [1/922], Loss: 8759.0781\n",
            "Epoch [53/100], Step [101/922], Loss: 8744.7764\n",
            "Epoch [53/100], Step [201/922], Loss: 9401.1035\n",
            "Epoch [53/100], Step [301/922], Loss: 8714.4990\n",
            "Epoch [53/100], Step [401/922], Loss: 8900.8398\n",
            "Epoch [53/100], Step [501/922], Loss: 8688.1484\n",
            "Epoch [53/100], Step [601/922], Loss: 8593.9355\n",
            "Epoch [53/100], Step [701/922], Loss: 8734.0693\n",
            "Epoch [53/100], Step [801/922], Loss: 8705.3926\n",
            "Epoch [53/100], Step [901/922], Loss: 8810.4805\n",
            "Epoch [54/100], Step [1/922], Loss: 7907.8931\n",
            "Epoch [54/100], Step [101/922], Loss: 8562.1689\n",
            "Epoch [54/100], Step [201/922], Loss: 9091.6455\n",
            "Epoch [54/100], Step [301/922], Loss: 9321.1260\n",
            "Epoch [54/100], Step [401/922], Loss: 9286.5840\n",
            "Epoch [54/100], Step [501/922], Loss: 8768.3633\n",
            "Epoch [54/100], Step [601/922], Loss: 8975.2363\n",
            "Epoch [54/100], Step [701/922], Loss: 8541.1406\n",
            "Epoch [54/100], Step [801/922], Loss: 8913.4336\n",
            "Epoch [54/100], Step [901/922], Loss: 8805.2578\n",
            "Epoch [55/100], Step [1/922], Loss: 8659.2188\n",
            "Epoch [55/100], Step [101/922], Loss: 8577.4805\n",
            "Epoch [55/100], Step [201/922], Loss: 8456.7793\n",
            "Epoch [55/100], Step [301/922], Loss: 9106.0986\n",
            "Epoch [55/100], Step [401/922], Loss: 8780.0918\n",
            "Epoch [55/100], Step [501/922], Loss: 8791.1445\n",
            "Epoch [55/100], Step [601/922], Loss: 8767.0547\n",
            "Epoch [55/100], Step [701/922], Loss: 9229.8418\n",
            "Epoch [55/100], Step [801/922], Loss: 8876.2607\n",
            "Epoch [55/100], Step [901/922], Loss: 8558.7422\n",
            "Epoch [56/100], Step [1/922], Loss: 9280.6113\n",
            "Epoch [56/100], Step [101/922], Loss: 9336.8574\n",
            "Epoch [56/100], Step [201/922], Loss: 8848.3398\n",
            "Epoch [56/100], Step [301/922], Loss: 8388.0635\n",
            "Epoch [56/100], Step [401/922], Loss: 9439.8203\n",
            "Epoch [56/100], Step [501/922], Loss: 8741.2529\n",
            "Epoch [56/100], Step [601/922], Loss: 9221.5703\n",
            "Epoch [56/100], Step [701/922], Loss: 9195.9248\n",
            "Epoch [56/100], Step [801/922], Loss: 9182.2070\n",
            "Epoch [56/100], Step [901/922], Loss: 8959.4961\n",
            "Epoch [57/100], Step [1/922], Loss: 9299.5977\n",
            "Epoch [57/100], Step [101/922], Loss: 8987.9414\n",
            "Epoch [57/100], Step [201/922], Loss: 9441.3252\n",
            "Epoch [57/100], Step [301/922], Loss: 9246.2178\n",
            "Epoch [57/100], Step [401/922], Loss: 9103.8066\n",
            "Epoch [57/100], Step [501/922], Loss: 8888.2705\n",
            "Epoch [57/100], Step [601/922], Loss: 9042.4902\n",
            "Epoch [57/100], Step [701/922], Loss: 8990.7178\n",
            "Epoch [57/100], Step [801/922], Loss: 9035.8682\n",
            "Epoch [57/100], Step [901/922], Loss: 8912.7793\n",
            "Epoch [58/100], Step [1/922], Loss: 8970.8379\n",
            "Epoch [58/100], Step [101/922], Loss: 8884.7549\n",
            "Epoch [58/100], Step [201/922], Loss: 8820.9375\n",
            "Epoch [58/100], Step [301/922], Loss: 8833.7812\n",
            "Epoch [58/100], Step [401/922], Loss: 8653.7988\n",
            "Epoch [58/100], Step [501/922], Loss: 9290.3184\n",
            "Epoch [58/100], Step [601/922], Loss: 9338.2344\n",
            "Epoch [58/100], Step [701/922], Loss: 8760.0986\n",
            "Epoch [58/100], Step [801/922], Loss: 8918.8291\n",
            "Epoch [58/100], Step [901/922], Loss: 8627.8994\n",
            "Epoch [59/100], Step [1/922], Loss: 9004.4668\n",
            "Epoch [59/100], Step [101/922], Loss: 8476.9248\n",
            "Epoch [59/100], Step [201/922], Loss: 8901.8066\n",
            "Epoch [59/100], Step [301/922], Loss: 8852.5713\n",
            "Epoch [59/100], Step [401/922], Loss: 8674.8193\n",
            "Epoch [59/100], Step [501/922], Loss: 9165.2373\n",
            "Epoch [59/100], Step [601/922], Loss: 8304.8848\n",
            "Epoch [59/100], Step [701/922], Loss: 8572.2520\n",
            "Epoch [59/100], Step [801/922], Loss: 8881.2041\n",
            "Epoch [59/100], Step [901/922], Loss: 8823.3096\n",
            "Epoch [60/100], Step [1/922], Loss: 8583.3662\n",
            "Epoch [60/100], Step [101/922], Loss: 8848.3242\n",
            "Epoch [60/100], Step [201/922], Loss: 8479.8340\n",
            "Epoch [60/100], Step [301/922], Loss: 8899.9512\n",
            "Epoch [60/100], Step [401/922], Loss: 8746.7930\n",
            "Epoch [60/100], Step [501/922], Loss: 8961.0156\n",
            "Epoch [60/100], Step [601/922], Loss: 9050.7344\n",
            "Epoch [60/100], Step [701/922], Loss: 8741.3330\n",
            "Epoch [60/100], Step [801/922], Loss: 9044.1123\n",
            "Epoch [60/100], Step [901/922], Loss: 8749.5732\n",
            "Epoch [61/100], Step [1/922], Loss: 9395.1309\n",
            "Epoch [61/100], Step [101/922], Loss: 9080.1885\n",
            "Epoch [61/100], Step [201/922], Loss: 9206.6924\n",
            "Epoch [61/100], Step [301/922], Loss: 8978.0703\n",
            "Epoch [61/100], Step [401/922], Loss: 9008.8193\n",
            "Epoch [61/100], Step [501/922], Loss: 9103.3135\n",
            "Epoch [61/100], Step [601/922], Loss: 9198.5605\n",
            "Epoch [61/100], Step [701/922], Loss: 8805.1777\n",
            "Epoch [61/100], Step [801/922], Loss: 8546.0928\n",
            "Epoch [61/100], Step [901/922], Loss: 8619.9004\n",
            "Epoch [62/100], Step [1/922], Loss: 8768.3223\n",
            "Epoch [62/100], Step [101/922], Loss: 8704.5684\n",
            "Epoch [62/100], Step [201/922], Loss: 8817.7266\n",
            "Epoch [62/100], Step [301/922], Loss: 8634.5342\n",
            "Epoch [62/100], Step [401/922], Loss: 8784.8535\n",
            "Epoch [62/100], Step [501/922], Loss: 9044.8721\n",
            "Epoch [62/100], Step [601/922], Loss: 8361.3330\n",
            "Epoch [62/100], Step [701/922], Loss: 9128.2080\n",
            "Epoch [62/100], Step [801/922], Loss: 8668.5869\n",
            "Epoch [62/100], Step [901/922], Loss: 8933.9473\n",
            "Epoch [63/100], Step [1/922], Loss: 8822.1094\n",
            "Epoch [63/100], Step [101/922], Loss: 8934.2441\n",
            "Epoch [63/100], Step [201/922], Loss: 8603.1758\n",
            "Epoch [63/100], Step [301/922], Loss: 8591.0586\n",
            "Epoch [63/100], Step [401/922], Loss: 8489.1055\n",
            "Epoch [63/100], Step [501/922], Loss: 8997.9092\n",
            "Epoch [63/100], Step [601/922], Loss: 8818.0811\n",
            "Epoch [63/100], Step [701/922], Loss: 9154.0918\n",
            "Epoch [63/100], Step [801/922], Loss: 8573.3281\n",
            "Epoch [63/100], Step [901/922], Loss: 9031.3447\n",
            "Epoch [64/100], Step [1/922], Loss: 8953.6162\n",
            "Epoch [64/100], Step [101/922], Loss: 8671.9521\n",
            "Epoch [64/100], Step [201/922], Loss: 9305.7598\n",
            "Epoch [64/100], Step [301/922], Loss: 8167.8467\n",
            "Epoch [64/100], Step [401/922], Loss: 9125.9141\n",
            "Epoch [64/100], Step [501/922], Loss: 8730.8633\n",
            "Epoch [64/100], Step [601/922], Loss: 9039.2246\n",
            "Epoch [64/100], Step [701/922], Loss: 8689.6299\n",
            "Epoch [64/100], Step [801/922], Loss: 9057.4248\n",
            "Epoch [64/100], Step [901/922], Loss: 9031.9346\n",
            "Epoch [65/100], Step [1/922], Loss: 9167.3467\n",
            "Epoch [65/100], Step [101/922], Loss: 8107.9434\n",
            "Epoch [65/100], Step [201/922], Loss: 8778.4727\n",
            "Epoch [65/100], Step [301/922], Loss: 8479.0693\n",
            "Epoch [65/100], Step [401/922], Loss: 9069.6807\n",
            "Epoch [65/100], Step [501/922], Loss: 7849.8730\n",
            "Epoch [65/100], Step [601/922], Loss: 8679.4766\n",
            "Epoch [65/100], Step [701/922], Loss: 8474.4473\n",
            "Epoch [65/100], Step [801/922], Loss: 8712.3857\n",
            "Epoch [65/100], Step [901/922], Loss: 9114.0225\n",
            "Epoch [66/100], Step [1/922], Loss: 9052.7627\n",
            "Epoch [66/100], Step [101/922], Loss: 9209.9785\n",
            "Epoch [66/100], Step [201/922], Loss: 8703.9922\n",
            "Epoch [66/100], Step [301/922], Loss: 8757.0459\n",
            "Epoch [66/100], Step [401/922], Loss: 8935.2734\n",
            "Epoch [66/100], Step [501/922], Loss: 9155.8760\n",
            "Epoch [66/100], Step [601/922], Loss: 8590.4199\n",
            "Epoch [66/100], Step [701/922], Loss: 8563.5635\n",
            "Epoch [66/100], Step [801/922], Loss: 8509.5059\n",
            "Epoch [66/100], Step [901/922], Loss: 8784.0596\n",
            "Epoch [67/100], Step [1/922], Loss: 8818.6855\n",
            "Epoch [67/100], Step [101/922], Loss: 8720.4492\n",
            "Epoch [67/100], Step [201/922], Loss: 8444.3760\n",
            "Epoch [67/100], Step [301/922], Loss: 8242.4189\n",
            "Epoch [67/100], Step [401/922], Loss: 8007.0112\n",
            "Epoch [67/100], Step [501/922], Loss: 8633.9990\n",
            "Epoch [67/100], Step [601/922], Loss: 8464.2930\n",
            "Epoch [67/100], Step [701/922], Loss: 8506.0664\n",
            "Epoch [67/100], Step [801/922], Loss: 8268.9238\n",
            "Epoch [67/100], Step [901/922], Loss: 8838.0830\n",
            "Epoch [68/100], Step [1/922], Loss: 8930.0635\n",
            "Epoch [68/100], Step [101/922], Loss: 8440.7383\n",
            "Epoch [68/100], Step [201/922], Loss: 8568.6660\n",
            "Epoch [68/100], Step [301/922], Loss: 8998.7148\n",
            "Epoch [68/100], Step [401/922], Loss: 9166.8486\n",
            "Epoch [68/100], Step [501/922], Loss: 8959.5283\n",
            "Epoch [68/100], Step [601/922], Loss: 8714.5918\n",
            "Epoch [68/100], Step [701/922], Loss: 8505.7510\n",
            "Epoch [68/100], Step [801/922], Loss: 8292.1055\n",
            "Epoch [68/100], Step [901/922], Loss: 9244.2402\n",
            "Epoch [69/100], Step [1/922], Loss: 8272.1631\n",
            "Epoch [69/100], Step [101/922], Loss: 8540.5244\n",
            "Epoch [69/100], Step [201/922], Loss: 8926.5713\n",
            "Epoch [69/100], Step [301/922], Loss: 8723.0547\n",
            "Epoch [69/100], Step [401/922], Loss: 8744.4629\n",
            "Epoch [69/100], Step [501/922], Loss: 8597.5469\n",
            "Epoch [69/100], Step [601/922], Loss: 8861.9053\n",
            "Epoch [69/100], Step [701/922], Loss: 8745.4043\n",
            "Epoch [69/100], Step [801/922], Loss: 8466.8613\n",
            "Epoch [69/100], Step [901/922], Loss: 9039.3896\n",
            "Epoch [70/100], Step [1/922], Loss: 8867.7148\n",
            "Epoch [70/100], Step [101/922], Loss: 9495.7900\n",
            "Epoch [70/100], Step [201/922], Loss: 9026.9697\n",
            "Epoch [70/100], Step [301/922], Loss: 9657.6758\n",
            "Epoch [70/100], Step [401/922], Loss: 8983.8418\n",
            "Epoch [70/100], Step [501/922], Loss: 8732.9326\n",
            "Epoch [70/100], Step [601/922], Loss: 8894.2070\n",
            "Epoch [70/100], Step [701/922], Loss: 8778.8418\n",
            "Epoch [70/100], Step [801/922], Loss: 8419.8896\n",
            "Epoch [70/100], Step [901/922], Loss: 8910.5557\n",
            "Epoch [71/100], Step [1/922], Loss: 8884.3555\n",
            "Epoch [71/100], Step [101/922], Loss: 9098.3867\n",
            "Epoch [71/100], Step [201/922], Loss: 8111.6523\n",
            "Epoch [71/100], Step [301/922], Loss: 9130.2002\n",
            "Epoch [71/100], Step [401/922], Loss: 8635.1807\n",
            "Epoch [71/100], Step [501/922], Loss: 9261.2949\n",
            "Epoch [71/100], Step [601/922], Loss: 9500.2041\n",
            "Epoch [71/100], Step [701/922], Loss: 9066.6250\n",
            "Epoch [71/100], Step [801/922], Loss: 9437.7676\n",
            "Epoch [71/100], Step [901/922], Loss: 8823.9912\n",
            "Epoch [72/100], Step [1/922], Loss: 8614.2256\n",
            "Epoch [72/100], Step [101/922], Loss: 8746.3242\n",
            "Epoch [72/100], Step [201/922], Loss: 8949.1758\n",
            "Epoch [72/100], Step [301/922], Loss: 8692.9619\n",
            "Epoch [72/100], Step [401/922], Loss: 8402.7930\n",
            "Epoch [72/100], Step [501/922], Loss: 8424.1064\n",
            "Epoch [72/100], Step [601/922], Loss: 8476.4326\n",
            "Epoch [72/100], Step [701/922], Loss: 8648.6074\n",
            "Epoch [72/100], Step [801/922], Loss: 8678.2471\n",
            "Epoch [72/100], Step [901/922], Loss: 8890.3369\n",
            "Epoch [73/100], Step [1/922], Loss: 8566.0996\n",
            "Epoch [73/100], Step [101/922], Loss: 8601.3574\n",
            "Epoch [73/100], Step [201/922], Loss: 8828.8613\n",
            "Epoch [73/100], Step [301/922], Loss: 8991.0254\n",
            "Epoch [73/100], Step [401/922], Loss: 8689.1543\n",
            "Epoch [73/100], Step [501/922], Loss: 8664.6104\n",
            "Epoch [73/100], Step [601/922], Loss: 9491.3809\n",
            "Epoch [73/100], Step [701/922], Loss: 8763.1914\n",
            "Epoch [73/100], Step [801/922], Loss: 9231.3418\n",
            "Epoch [73/100], Step [901/922], Loss: 9244.5400\n",
            "Epoch [74/100], Step [1/922], Loss: 9226.1885\n",
            "Epoch [74/100], Step [101/922], Loss: 8833.7305\n",
            "Epoch [74/100], Step [201/922], Loss: 9075.5977\n",
            "Epoch [74/100], Step [301/922], Loss: 9452.5107\n",
            "Epoch [74/100], Step [401/922], Loss: 8652.7344\n",
            "Epoch [74/100], Step [501/922], Loss: 8651.7764\n",
            "Epoch [74/100], Step [601/922], Loss: 8783.3975\n",
            "Epoch [74/100], Step [701/922], Loss: 8461.6162\n",
            "Epoch [74/100], Step [801/922], Loss: 8462.1162\n",
            "Epoch [74/100], Step [901/922], Loss: 8891.1992\n",
            "Epoch [75/100], Step [1/922], Loss: 8615.7363\n",
            "Epoch [75/100], Step [101/922], Loss: 9305.9160\n",
            "Epoch [75/100], Step [201/922], Loss: 8608.2744\n",
            "Epoch [75/100], Step [301/922], Loss: 8862.3652\n",
            "Epoch [75/100], Step [401/922], Loss: 8873.2598\n",
            "Epoch [75/100], Step [501/922], Loss: 8411.9229\n",
            "Epoch [75/100], Step [601/922], Loss: 9001.3389\n",
            "Epoch [75/100], Step [701/922], Loss: 8560.1260\n",
            "Epoch [75/100], Step [801/922], Loss: 8151.7046\n",
            "Epoch [75/100], Step [901/922], Loss: 8703.9893\n",
            "Epoch [76/100], Step [1/922], Loss: 9037.8242\n",
            "Epoch [76/100], Step [101/922], Loss: 9121.3750\n",
            "Epoch [76/100], Step [201/922], Loss: 8980.7676\n",
            "Epoch [76/100], Step [301/922], Loss: 9035.0801\n",
            "Epoch [76/100], Step [401/922], Loss: 9017.4492\n",
            "Epoch [76/100], Step [501/922], Loss: 9088.9727\n",
            "Epoch [76/100], Step [601/922], Loss: 9311.5020\n",
            "Epoch [76/100], Step [701/922], Loss: 8868.9600\n",
            "Epoch [76/100], Step [801/922], Loss: 9054.3896\n",
            "Epoch [76/100], Step [901/922], Loss: 8933.3516\n",
            "Epoch [77/100], Step [1/922], Loss: 9281.2510\n",
            "Epoch [77/100], Step [101/922], Loss: 8228.1279\n",
            "Epoch [77/100], Step [201/922], Loss: 8498.1035\n",
            "Epoch [77/100], Step [301/922], Loss: 8970.2725\n",
            "Epoch [77/100], Step [401/922], Loss: 8956.4307\n",
            "Epoch [77/100], Step [501/922], Loss: 8450.8789\n",
            "Epoch [77/100], Step [601/922], Loss: 8324.9619\n",
            "Epoch [77/100], Step [701/922], Loss: 8849.3311\n",
            "Epoch [77/100], Step [801/922], Loss: 8699.4121\n",
            "Epoch [77/100], Step [901/922], Loss: 8663.0898\n",
            "Epoch [78/100], Step [1/922], Loss: 8670.7314\n",
            "Epoch [78/100], Step [101/922], Loss: 8953.4336\n",
            "Epoch [78/100], Step [201/922], Loss: 8610.4492\n",
            "Epoch [78/100], Step [301/922], Loss: 8034.3540\n",
            "Epoch [78/100], Step [401/922], Loss: 8420.6855\n",
            "Epoch [78/100], Step [501/922], Loss: 8803.4756\n",
            "Epoch [78/100], Step [601/922], Loss: 8885.7920\n",
            "Epoch [78/100], Step [701/922], Loss: 8604.8184\n",
            "Epoch [78/100], Step [801/922], Loss: 8290.7881\n",
            "Epoch [78/100], Step [901/922], Loss: 8575.8516\n",
            "Epoch [79/100], Step [1/922], Loss: 9008.2969\n",
            "Epoch [79/100], Step [101/922], Loss: 8635.3389\n",
            "Epoch [79/100], Step [201/922], Loss: 8908.8916\n",
            "Epoch [79/100], Step [301/922], Loss: 9039.0771\n",
            "Epoch [79/100], Step [401/922], Loss: 8816.9170\n",
            "Epoch [79/100], Step [501/922], Loss: 9088.2080\n",
            "Epoch [79/100], Step [601/922], Loss: 9537.0879\n",
            "Epoch [79/100], Step [701/922], Loss: 8455.4014\n",
            "Epoch [79/100], Step [801/922], Loss: 8665.5137\n",
            "Epoch [79/100], Step [901/922], Loss: 8734.6816\n",
            "Epoch [80/100], Step [1/922], Loss: 8606.6250\n",
            "Epoch [80/100], Step [101/922], Loss: 9196.2783\n",
            "Epoch [80/100], Step [201/922], Loss: 8428.1104\n",
            "Epoch [80/100], Step [301/922], Loss: 8684.5215\n",
            "Epoch [80/100], Step [401/922], Loss: 8778.8457\n",
            "Epoch [80/100], Step [501/922], Loss: 8974.3008\n",
            "Epoch [80/100], Step [601/922], Loss: 8875.7988\n",
            "Epoch [80/100], Step [701/922], Loss: 8819.0420\n",
            "Epoch [80/100], Step [801/922], Loss: 9087.1826\n",
            "Epoch [80/100], Step [901/922], Loss: 9087.3369\n",
            "Epoch [81/100], Step [1/922], Loss: 8575.9033\n",
            "Epoch [81/100], Step [101/922], Loss: 8362.0820\n",
            "Epoch [81/100], Step [201/922], Loss: 9138.0527\n",
            "Epoch [81/100], Step [301/922], Loss: 8765.7432\n",
            "Epoch [81/100], Step [401/922], Loss: 8734.2988\n",
            "Epoch [81/100], Step [501/922], Loss: 8822.2988\n",
            "Epoch [81/100], Step [601/922], Loss: 8268.1396\n",
            "Epoch [81/100], Step [701/922], Loss: 8448.4492\n",
            "Epoch [81/100], Step [801/922], Loss: 8741.5488\n",
            "Epoch [81/100], Step [901/922], Loss: 8852.3555\n",
            "Epoch [82/100], Step [1/922], Loss: 8939.3701\n",
            "Epoch [82/100], Step [101/922], Loss: 8641.0020\n",
            "Epoch [82/100], Step [201/922], Loss: 8940.4824\n",
            "Epoch [82/100], Step [301/922], Loss: 8334.5244\n",
            "Epoch [82/100], Step [401/922], Loss: 9050.7715\n",
            "Epoch [82/100], Step [501/922], Loss: 8675.9365\n",
            "Epoch [82/100], Step [601/922], Loss: 9032.2461\n",
            "Epoch [82/100], Step [701/922], Loss: 8900.7158\n",
            "Epoch [82/100], Step [801/922], Loss: 8519.2520\n",
            "Epoch [82/100], Step [901/922], Loss: 8340.7061\n",
            "Epoch [83/100], Step [1/922], Loss: 8940.3418\n",
            "Epoch [83/100], Step [101/922], Loss: 8919.1689\n",
            "Epoch [83/100], Step [201/922], Loss: 9261.1816\n",
            "Epoch [83/100], Step [301/922], Loss: 8939.6133\n",
            "Epoch [83/100], Step [401/922], Loss: 9353.3262\n",
            "Epoch [83/100], Step [501/922], Loss: 8706.8857\n",
            "Epoch [83/100], Step [601/922], Loss: 9172.4873\n",
            "Epoch [83/100], Step [701/922], Loss: 8933.5615\n",
            "Epoch [83/100], Step [801/922], Loss: 8882.2949\n",
            "Epoch [83/100], Step [901/922], Loss: 9254.2314\n",
            "Epoch [84/100], Step [1/922], Loss: 8743.1338\n",
            "Epoch [84/100], Step [101/922], Loss: 8688.3984\n",
            "Epoch [84/100], Step [201/922], Loss: 8429.3506\n",
            "Epoch [84/100], Step [301/922], Loss: 9169.8672\n",
            "Epoch [84/100], Step [401/922], Loss: 9021.4668\n",
            "Epoch [84/100], Step [501/922], Loss: 8574.1191\n",
            "Epoch [84/100], Step [601/922], Loss: 8810.7617\n",
            "Epoch [84/100], Step [701/922], Loss: 9094.5244\n",
            "Epoch [84/100], Step [801/922], Loss: 8780.6094\n",
            "Epoch [84/100], Step [901/922], Loss: 9094.9189\n",
            "Epoch [85/100], Step [1/922], Loss: 9130.7217\n",
            "Epoch [85/100], Step [101/922], Loss: 8599.5830\n",
            "Epoch [85/100], Step [201/922], Loss: 8575.5078\n",
            "Epoch [85/100], Step [301/922], Loss: 8597.0410\n",
            "Epoch [85/100], Step [401/922], Loss: 9132.1719\n",
            "Epoch [85/100], Step [501/922], Loss: 8787.2930\n",
            "Epoch [85/100], Step [601/922], Loss: 9089.7773\n",
            "Epoch [85/100], Step [701/922], Loss: 9156.2344\n",
            "Epoch [85/100], Step [801/922], Loss: 8981.4795\n",
            "Epoch [85/100], Step [901/922], Loss: 8869.3867\n",
            "Epoch [86/100], Step [1/922], Loss: 9130.0332\n",
            "Epoch [86/100], Step [101/922], Loss: 9117.4512\n",
            "Epoch [86/100], Step [201/922], Loss: 8923.4141\n",
            "Epoch [86/100], Step [301/922], Loss: 8317.1836\n",
            "Epoch [86/100], Step [401/922], Loss: 9032.8457\n",
            "Epoch [86/100], Step [501/922], Loss: 8807.1494\n",
            "Epoch [86/100], Step [601/922], Loss: 8681.8213\n",
            "Epoch [86/100], Step [701/922], Loss: 9004.1006\n",
            "Epoch [86/100], Step [801/922], Loss: 9236.8486\n",
            "Epoch [86/100], Step [901/922], Loss: 9053.9697\n",
            "Epoch [87/100], Step [1/922], Loss: 9148.6553\n",
            "Epoch [87/100], Step [101/922], Loss: 8793.5254\n",
            "Epoch [87/100], Step [201/922], Loss: 8536.7686\n",
            "Epoch [87/100], Step [301/922], Loss: 8791.4111\n",
            "Epoch [87/100], Step [401/922], Loss: 8630.9170\n",
            "Epoch [87/100], Step [501/922], Loss: 8603.6855\n",
            "Epoch [87/100], Step [601/922], Loss: 8572.7354\n",
            "Epoch [87/100], Step [701/922], Loss: 9236.5996\n",
            "Epoch [87/100], Step [801/922], Loss: 8670.7598\n",
            "Epoch [87/100], Step [901/922], Loss: 8387.1201\n",
            "Epoch [88/100], Step [1/922], Loss: 8890.8252\n",
            "Epoch [88/100], Step [101/922], Loss: 8835.1875\n",
            "Epoch [88/100], Step [201/922], Loss: 8582.7119\n",
            "Epoch [88/100], Step [301/922], Loss: 8665.4785\n",
            "Epoch [88/100], Step [401/922], Loss: 8654.1240\n",
            "Epoch [88/100], Step [501/922], Loss: 8819.2861\n",
            "Epoch [88/100], Step [601/922], Loss: 8826.8486\n",
            "Epoch [88/100], Step [701/922], Loss: 9043.9600\n",
            "Epoch [88/100], Step [801/922], Loss: 9063.7832\n",
            "Epoch [88/100], Step [901/922], Loss: 8554.9814\n",
            "Epoch [89/100], Step [1/922], Loss: 8503.2568\n",
            "Epoch [89/100], Step [101/922], Loss: 8828.2656\n",
            "Epoch [89/100], Step [201/922], Loss: 8495.7812\n",
            "Epoch [89/100], Step [301/922], Loss: 8851.9980\n",
            "Epoch [89/100], Step [401/922], Loss: 8709.4980\n",
            "Epoch [89/100], Step [501/922], Loss: 8237.5391\n",
            "Epoch [89/100], Step [601/922], Loss: 9022.2080\n",
            "Epoch [89/100], Step [701/922], Loss: 9064.9561\n",
            "Epoch [89/100], Step [801/922], Loss: 8754.9668\n",
            "Epoch [89/100], Step [901/922], Loss: 8757.6162\n",
            "Epoch [90/100], Step [1/922], Loss: 8845.1514\n",
            "Epoch [90/100], Step [101/922], Loss: 8643.0527\n",
            "Epoch [90/100], Step [201/922], Loss: 8400.0938\n",
            "Epoch [90/100], Step [301/922], Loss: 8757.2051\n",
            "Epoch [90/100], Step [401/922], Loss: 8610.5322\n",
            "Epoch [90/100], Step [501/922], Loss: 9048.5586\n",
            "Epoch [90/100], Step [601/922], Loss: 8988.3428\n",
            "Epoch [90/100], Step [701/922], Loss: 8441.7656\n",
            "Epoch [90/100], Step [801/922], Loss: 8410.3018\n",
            "Epoch [90/100], Step [901/922], Loss: 8623.2793\n",
            "Epoch [91/100], Step [1/922], Loss: 9088.0752\n",
            "Epoch [91/100], Step [101/922], Loss: 8792.0342\n",
            "Epoch [91/100], Step [201/922], Loss: 8774.1035\n",
            "Epoch [91/100], Step [301/922], Loss: 9222.3701\n",
            "Epoch [91/100], Step [401/922], Loss: 8325.2080\n",
            "Epoch [91/100], Step [501/922], Loss: 8740.7197\n",
            "Epoch [91/100], Step [601/922], Loss: 8789.3438\n",
            "Epoch [91/100], Step [701/922], Loss: 8828.1318\n",
            "Epoch [91/100], Step [801/922], Loss: 9053.1328\n",
            "Epoch [91/100], Step [901/922], Loss: 8889.7471\n",
            "Epoch [92/100], Step [1/922], Loss: 8682.8125\n",
            "Epoch [92/100], Step [101/922], Loss: 8865.7598\n",
            "Epoch [92/100], Step [201/922], Loss: 8845.6289\n",
            "Epoch [92/100], Step [301/922], Loss: 8544.7539\n",
            "Epoch [92/100], Step [401/922], Loss: 8508.3730\n",
            "Epoch [92/100], Step [501/922], Loss: 8841.7168\n",
            "Epoch [92/100], Step [601/922], Loss: 8396.8105\n",
            "Epoch [92/100], Step [701/922], Loss: 9011.9570\n",
            "Epoch [92/100], Step [801/922], Loss: 9170.8564\n",
            "Epoch [92/100], Step [901/922], Loss: 8421.3496\n",
            "Epoch [93/100], Step [1/922], Loss: 8404.4893\n",
            "Epoch [93/100], Step [101/922], Loss: 8419.6377\n",
            "Epoch [93/100], Step [201/922], Loss: 8844.7510\n",
            "Epoch [93/100], Step [301/922], Loss: 8818.3789\n",
            "Epoch [93/100], Step [401/922], Loss: 8931.3008\n",
            "Epoch [93/100], Step [501/922], Loss: 8408.0195\n",
            "Epoch [93/100], Step [601/922], Loss: 8594.3057\n",
            "Epoch [93/100], Step [701/922], Loss: 8936.9082\n",
            "Epoch [93/100], Step [801/922], Loss: 9299.4004\n",
            "Epoch [93/100], Step [901/922], Loss: 9497.6973\n",
            "Epoch [94/100], Step [1/922], Loss: 8965.6689\n",
            "Epoch [94/100], Step [101/922], Loss: 8771.9951\n",
            "Epoch [94/100], Step [201/922], Loss: 8909.7207\n",
            "Epoch [94/100], Step [301/922], Loss: 8864.5557\n",
            "Epoch [94/100], Step [401/922], Loss: 8211.8008\n",
            "Epoch [94/100], Step [501/922], Loss: 8151.1343\n",
            "Epoch [94/100], Step [601/922], Loss: 8290.3535\n",
            "Epoch [94/100], Step [701/922], Loss: 8818.4629\n",
            "Epoch [94/100], Step [801/922], Loss: 8753.0215\n",
            "Epoch [94/100], Step [901/922], Loss: 9180.3105\n",
            "Epoch [95/100], Step [1/922], Loss: 8673.7012\n",
            "Epoch [95/100], Step [101/922], Loss: 8462.9512\n",
            "Epoch [95/100], Step [201/922], Loss: 8107.3086\n",
            "Epoch [95/100], Step [301/922], Loss: 8787.7295\n",
            "Epoch [95/100], Step [401/922], Loss: 8995.6973\n",
            "Epoch [95/100], Step [501/922], Loss: 8787.7129\n",
            "Epoch [95/100], Step [601/922], Loss: 8989.1445\n",
            "Epoch [95/100], Step [701/922], Loss: 8833.9150\n",
            "Epoch [95/100], Step [801/922], Loss: 8768.1738\n",
            "Epoch [95/100], Step [901/922], Loss: 8787.2295\n",
            "Epoch [96/100], Step [1/922], Loss: 9205.7422\n",
            "Epoch [96/100], Step [101/922], Loss: 8795.1338\n",
            "Epoch [96/100], Step [201/922], Loss: 8944.4346\n",
            "Epoch [96/100], Step [301/922], Loss: 8621.3799\n",
            "Epoch [96/100], Step [401/922], Loss: 8781.3379\n",
            "Epoch [96/100], Step [501/922], Loss: 8069.3408\n",
            "Epoch [96/100], Step [601/922], Loss: 8624.5801\n",
            "Epoch [96/100], Step [701/922], Loss: 8922.3779\n",
            "Epoch [96/100], Step [801/922], Loss: 8241.0254\n",
            "Epoch [96/100], Step [901/922], Loss: 9243.5371\n",
            "Epoch [97/100], Step [1/922], Loss: 8514.6650\n",
            "Epoch [97/100], Step [101/922], Loss: 8605.4141\n",
            "Epoch [97/100], Step [201/922], Loss: 8859.3506\n",
            "Epoch [97/100], Step [301/922], Loss: 8813.0088\n",
            "Epoch [97/100], Step [401/922], Loss: 8753.4277\n",
            "Epoch [97/100], Step [501/922], Loss: 8817.1221\n",
            "Epoch [97/100], Step [601/922], Loss: 8181.2114\n",
            "Epoch [97/100], Step [701/922], Loss: 8751.2422\n",
            "Epoch [97/100], Step [801/922], Loss: 8633.0049\n",
            "Epoch [97/100], Step [901/922], Loss: 8899.0879\n",
            "Epoch [98/100], Step [1/922], Loss: 8527.6426\n",
            "Epoch [98/100], Step [101/922], Loss: 8851.7910\n",
            "Epoch [98/100], Step [201/922], Loss: 8872.1074\n",
            "Epoch [98/100], Step [301/922], Loss: 8681.7402\n",
            "Epoch [98/100], Step [401/922], Loss: 8972.9512\n",
            "Epoch [98/100], Step [501/922], Loss: 8875.3896\n",
            "Epoch [98/100], Step [601/922], Loss: 8745.7812\n",
            "Epoch [98/100], Step [701/922], Loss: 9072.5762\n",
            "Epoch [98/100], Step [801/922], Loss: 8974.8818\n",
            "Epoch [98/100], Step [901/922], Loss: 9148.3643\n",
            "Epoch [99/100], Step [1/922], Loss: 8639.2373\n",
            "Epoch [99/100], Step [101/922], Loss: 9280.2910\n",
            "Epoch [99/100], Step [201/922], Loss: 8913.9199\n",
            "Epoch [99/100], Step [301/922], Loss: 8970.2705\n",
            "Epoch [99/100], Step [401/922], Loss: 9076.3564\n",
            "Epoch [99/100], Step [501/922], Loss: 8616.0508\n",
            "Epoch [99/100], Step [601/922], Loss: 8464.0859\n",
            "Epoch [99/100], Step [701/922], Loss: 8709.4043\n",
            "Epoch [99/100], Step [801/922], Loss: 9053.9902\n",
            "Epoch [99/100], Step [901/922], Loss: 9017.5840\n",
            "Epoch [100/100], Step [1/922], Loss: 9063.6406\n",
            "Epoch [100/100], Step [101/922], Loss: 8989.2637\n",
            "Epoch [100/100], Step [201/922], Loss: 8931.9141\n",
            "Epoch [100/100], Step [301/922], Loss: 8590.6406\n",
            "Epoch [100/100], Step [401/922], Loss: 8567.3770\n",
            "Epoch [100/100], Step [501/922], Loss: 8945.1191\n",
            "Epoch [100/100], Step [601/922], Loss: 8743.3594\n",
            "Epoch [100/100], Step [701/922], Loss: 8765.7314\n",
            "Epoch [100/100], Step [801/922], Loss: 8731.1416\n",
            "Epoch [100/100], Step [901/922], Loss: 8729.9082\n",
            "Accuracy of the SVM on the test images with 1000 labeled images: 93.59%\n",
            "Epoch [1/100], Step [1/891], Loss: 38327.0469\n",
            "Epoch [1/100], Step [101/891], Loss: 12840.4277\n",
            "Epoch [1/100], Step [201/891], Loss: 12341.5107\n",
            "Epoch [1/100], Step [301/891], Loss: 12292.5850\n",
            "Epoch [1/100], Step [401/891], Loss: 12417.4502\n",
            "Epoch [1/100], Step [501/891], Loss: 12509.0010\n",
            "Epoch [1/100], Step [601/891], Loss: 11902.6191\n",
            "Epoch [1/100], Step [701/891], Loss: 11442.4121\n",
            "Epoch [1/100], Step [801/891], Loss: 11066.3486\n",
            "Epoch [2/100], Step [1/891], Loss: 11376.7842\n",
            "Epoch [2/100], Step [101/891], Loss: 10580.9834\n",
            "Epoch [2/100], Step [201/891], Loss: 11669.5098\n",
            "Epoch [2/100], Step [301/891], Loss: 10389.7803\n",
            "Epoch [2/100], Step [401/891], Loss: 10211.1143\n",
            "Epoch [2/100], Step [501/891], Loss: 11128.2744\n",
            "Epoch [2/100], Step [601/891], Loss: 10662.0625\n",
            "Epoch [2/100], Step [701/891], Loss: 10787.4443\n",
            "Epoch [2/100], Step [801/891], Loss: 10017.8682\n",
            "Epoch [3/100], Step [1/891], Loss: 11082.2148\n",
            "Epoch [3/100], Step [101/891], Loss: 10384.7812\n",
            "Epoch [3/100], Step [201/891], Loss: 9939.2363\n",
            "Epoch [3/100], Step [301/891], Loss: 10541.0254\n",
            "Epoch [3/100], Step [401/891], Loss: 10491.3350\n",
            "Epoch [3/100], Step [501/891], Loss: 10028.7939\n",
            "Epoch [3/100], Step [601/891], Loss: 10533.4502\n",
            "Epoch [3/100], Step [701/891], Loss: 10343.8379\n",
            "Epoch [3/100], Step [801/891], Loss: 9655.3604\n",
            "Epoch [4/100], Step [1/891], Loss: 9381.9551\n",
            "Epoch [4/100], Step [101/891], Loss: 10491.1406\n",
            "Epoch [4/100], Step [201/891], Loss: 10218.6943\n",
            "Epoch [4/100], Step [301/891], Loss: 9967.9785\n",
            "Epoch [4/100], Step [401/891], Loss: 10017.4229\n",
            "Epoch [4/100], Step [501/891], Loss: 10420.0020\n",
            "Epoch [4/100], Step [601/891], Loss: 10037.0859\n",
            "Epoch [4/100], Step [701/891], Loss: 9556.9619\n",
            "Epoch [4/100], Step [801/891], Loss: 10500.6641\n",
            "Epoch [5/100], Step [1/891], Loss: 9294.7031\n",
            "Epoch [5/100], Step [101/891], Loss: 9472.0596\n",
            "Epoch [5/100], Step [201/891], Loss: 10046.0088\n",
            "Epoch [5/100], Step [301/891], Loss: 10088.2715\n",
            "Epoch [5/100], Step [401/891], Loss: 9825.9424\n",
            "Epoch [5/100], Step [501/891], Loss: 10110.5674\n",
            "Epoch [5/100], Step [601/891], Loss: 9734.1436\n",
            "Epoch [5/100], Step [701/891], Loss: 9255.5879\n",
            "Epoch [5/100], Step [801/891], Loss: 9476.8047\n",
            "Epoch [6/100], Step [1/891], Loss: 9463.7588\n",
            "Epoch [6/100], Step [101/891], Loss: 10186.8398\n",
            "Epoch [6/100], Step [201/891], Loss: 9793.6602\n",
            "Epoch [6/100], Step [301/891], Loss: 9906.8271\n",
            "Epoch [6/100], Step [401/891], Loss: 9169.7090\n",
            "Epoch [6/100], Step [501/891], Loss: 9636.6348\n",
            "Epoch [6/100], Step [601/891], Loss: 9266.0176\n",
            "Epoch [6/100], Step [701/891], Loss: 9509.4932\n",
            "Epoch [6/100], Step [801/891], Loss: 9101.2754\n",
            "Epoch [7/100], Step [1/891], Loss: 10123.5156\n",
            "Epoch [7/100], Step [101/891], Loss: 9943.4912\n",
            "Epoch [7/100], Step [201/891], Loss: 9583.7119\n",
            "Epoch [7/100], Step [301/891], Loss: 9353.7979\n",
            "Epoch [7/100], Step [401/891], Loss: 9649.7109\n",
            "Epoch [7/100], Step [501/891], Loss: 9317.5332\n",
            "Epoch [7/100], Step [601/891], Loss: 9841.5703\n",
            "Epoch [7/100], Step [701/891], Loss: 9365.6855\n",
            "Epoch [7/100], Step [801/891], Loss: 9605.9473\n",
            "Epoch [8/100], Step [1/891], Loss: 9972.0908\n",
            "Epoch [8/100], Step [101/891], Loss: 9157.5293\n",
            "Epoch [8/100], Step [201/891], Loss: 9557.7256\n",
            "Epoch [8/100], Step [301/891], Loss: 9708.7158\n",
            "Epoch [8/100], Step [401/891], Loss: 9562.7158\n",
            "Epoch [8/100], Step [501/891], Loss: 9153.3770\n",
            "Epoch [8/100], Step [601/891], Loss: 9387.8066\n",
            "Epoch [8/100], Step [701/891], Loss: 9016.1846\n",
            "Epoch [8/100], Step [801/891], Loss: 9819.4805\n",
            "Epoch [9/100], Step [1/891], Loss: 9288.7412\n",
            "Epoch [9/100], Step [101/891], Loss: 9736.2109\n",
            "Epoch [9/100], Step [201/891], Loss: 9674.7910\n",
            "Epoch [9/100], Step [301/891], Loss: 9278.1709\n",
            "Epoch [9/100], Step [401/891], Loss: 8968.5469\n",
            "Epoch [9/100], Step [501/891], Loss: 9706.1514\n",
            "Epoch [9/100], Step [601/891], Loss: 9761.6748\n",
            "Epoch [9/100], Step [701/891], Loss: 8720.1836\n",
            "Epoch [9/100], Step [801/891], Loss: 9619.4961\n",
            "Epoch [10/100], Step [1/891], Loss: 9314.7529\n",
            "Epoch [10/100], Step [101/891], Loss: 9599.8457\n",
            "Epoch [10/100], Step [201/891], Loss: 9142.7119\n",
            "Epoch [10/100], Step [301/891], Loss: 8860.6016\n",
            "Epoch [10/100], Step [401/891], Loss: 9225.2539\n",
            "Epoch [10/100], Step [501/891], Loss: 9232.0967\n",
            "Epoch [10/100], Step [601/891], Loss: 9331.1006\n",
            "Epoch [10/100], Step [701/891], Loss: 9647.4131\n",
            "Epoch [10/100], Step [801/891], Loss: 9566.2656\n",
            "Epoch [11/100], Step [1/891], Loss: 9464.8350\n",
            "Epoch [11/100], Step [101/891], Loss: 9251.6963\n",
            "Epoch [11/100], Step [201/891], Loss: 9621.3730\n",
            "Epoch [11/100], Step [301/891], Loss: 9370.0986\n",
            "Epoch [11/100], Step [401/891], Loss: 9886.7969\n",
            "Epoch [11/100], Step [501/891], Loss: 9098.5498\n",
            "Epoch [11/100], Step [601/891], Loss: 9355.8418\n",
            "Epoch [11/100], Step [701/891], Loss: 9109.8730\n",
            "Epoch [11/100], Step [801/891], Loss: 9297.4619\n",
            "Epoch [12/100], Step [1/891], Loss: 9511.5176\n",
            "Epoch [12/100], Step [101/891], Loss: 9425.7979\n",
            "Epoch [12/100], Step [201/891], Loss: 8882.5000\n",
            "Epoch [12/100], Step [301/891], Loss: 9504.3613\n",
            "Epoch [12/100], Step [401/891], Loss: 9357.1201\n",
            "Epoch [12/100], Step [501/891], Loss: 9276.3037\n",
            "Epoch [12/100], Step [601/891], Loss: 9468.6152\n",
            "Epoch [12/100], Step [701/891], Loss: 9323.4785\n",
            "Epoch [12/100], Step [801/891], Loss: 9177.7754\n",
            "Epoch [13/100], Step [1/891], Loss: 9446.7041\n",
            "Epoch [13/100], Step [101/891], Loss: 8588.3018\n",
            "Epoch [13/100], Step [201/891], Loss: 9220.6641\n",
            "Epoch [13/100], Step [301/891], Loss: 9036.3965\n",
            "Epoch [13/100], Step [401/891], Loss: 9387.8008\n",
            "Epoch [13/100], Step [501/891], Loss: 8750.7285\n",
            "Epoch [13/100], Step [601/891], Loss: 8870.1641\n",
            "Epoch [13/100], Step [701/891], Loss: 9173.0254\n",
            "Epoch [13/100], Step [801/891], Loss: 9408.2578\n",
            "Epoch [14/100], Step [1/891], Loss: 9185.3516\n",
            "Epoch [14/100], Step [101/891], Loss: 9836.7666\n",
            "Epoch [14/100], Step [201/891], Loss: 9304.0732\n",
            "Epoch [14/100], Step [301/891], Loss: 9778.9072\n",
            "Epoch [14/100], Step [401/891], Loss: 9427.2871\n",
            "Epoch [14/100], Step [501/891], Loss: 9291.7158\n",
            "Epoch [14/100], Step [601/891], Loss: 9195.9258\n",
            "Epoch [14/100], Step [701/891], Loss: 9246.4922\n",
            "Epoch [14/100], Step [801/891], Loss: 9179.5430\n",
            "Epoch [15/100], Step [1/891], Loss: 9305.8291\n",
            "Epoch [15/100], Step [101/891], Loss: 9014.3623\n",
            "Epoch [15/100], Step [201/891], Loss: 9046.8721\n",
            "Epoch [15/100], Step [301/891], Loss: 9498.5674\n",
            "Epoch [15/100], Step [401/891], Loss: 8950.8037\n",
            "Epoch [15/100], Step [501/891], Loss: 9080.0732\n",
            "Epoch [15/100], Step [601/891], Loss: 9028.3730\n",
            "Epoch [15/100], Step [701/891], Loss: 9592.1680\n",
            "Epoch [15/100], Step [801/891], Loss: 9272.8066\n",
            "Epoch [16/100], Step [1/891], Loss: 9936.4209\n",
            "Epoch [16/100], Step [101/891], Loss: 8668.5703\n",
            "Epoch [16/100], Step [201/891], Loss: 9390.5967\n",
            "Epoch [16/100], Step [301/891], Loss: 9236.5225\n",
            "Epoch [16/100], Step [401/891], Loss: 9591.6328\n",
            "Epoch [16/100], Step [501/891], Loss: 9497.1387\n",
            "Epoch [16/100], Step [601/891], Loss: 9361.2588\n",
            "Epoch [16/100], Step [701/891], Loss: 8986.3633\n",
            "Epoch [16/100], Step [801/891], Loss: 9047.5391\n",
            "Epoch [17/100], Step [1/891], Loss: 8937.4883\n",
            "Epoch [17/100], Step [101/891], Loss: 9470.9570\n",
            "Epoch [17/100], Step [201/891], Loss: 9323.5508\n",
            "Epoch [17/100], Step [301/891], Loss: 9060.6592\n",
            "Epoch [17/100], Step [401/891], Loss: 9402.5000\n",
            "Epoch [17/100], Step [501/891], Loss: 8692.6133\n",
            "Epoch [17/100], Step [601/891], Loss: 8779.6318\n",
            "Epoch [17/100], Step [701/891], Loss: 9671.3809\n",
            "Epoch [17/100], Step [801/891], Loss: 9165.0176\n",
            "Epoch [18/100], Step [1/891], Loss: 9017.5176\n",
            "Epoch [18/100], Step [101/891], Loss: 9481.2363\n",
            "Epoch [18/100], Step [201/891], Loss: 9287.3711\n",
            "Epoch [18/100], Step [301/891], Loss: 8917.1895\n",
            "Epoch [18/100], Step [401/891], Loss: 9368.9131\n",
            "Epoch [18/100], Step [501/891], Loss: 8846.3867\n",
            "Epoch [18/100], Step [601/891], Loss: 8877.7842\n",
            "Epoch [18/100], Step [701/891], Loss: 8893.1875\n",
            "Epoch [18/100], Step [801/891], Loss: 9281.7734\n",
            "Epoch [19/100], Step [1/891], Loss: 9123.8330\n",
            "Epoch [19/100], Step [101/891], Loss: 9493.5547\n",
            "Epoch [19/100], Step [201/891], Loss: 8937.2559\n",
            "Epoch [19/100], Step [301/891], Loss: 9129.3564\n",
            "Epoch [19/100], Step [401/891], Loss: 9081.2266\n",
            "Epoch [19/100], Step [501/891], Loss: 8872.0771\n",
            "Epoch [19/100], Step [601/891], Loss: 8817.3379\n",
            "Epoch [19/100], Step [701/891], Loss: 8963.2051\n",
            "Epoch [19/100], Step [801/891], Loss: 9120.8330\n",
            "Epoch [20/100], Step [1/891], Loss: 8661.0557\n",
            "Epoch [20/100], Step [101/891], Loss: 8733.0312\n",
            "Epoch [20/100], Step [201/891], Loss: 8928.8594\n",
            "Epoch [20/100], Step [301/891], Loss: 9200.3877\n",
            "Epoch [20/100], Step [401/891], Loss: 9281.8926\n",
            "Epoch [20/100], Step [501/891], Loss: 8505.7363\n",
            "Epoch [20/100], Step [601/891], Loss: 8633.8193\n",
            "Epoch [20/100], Step [701/891], Loss: 8992.6865\n",
            "Epoch [20/100], Step [801/891], Loss: 9355.6133\n",
            "Epoch [21/100], Step [1/891], Loss: 9170.5352\n",
            "Epoch [21/100], Step [101/891], Loss: 9374.5898\n",
            "Epoch [21/100], Step [201/891], Loss: 9339.5811\n",
            "Epoch [21/100], Step [301/891], Loss: 8854.4629\n",
            "Epoch [21/100], Step [401/891], Loss: 8332.2354\n",
            "Epoch [21/100], Step [501/891], Loss: 8823.3438\n",
            "Epoch [21/100], Step [601/891], Loss: 9252.7500\n",
            "Epoch [21/100], Step [701/891], Loss: 8905.8389\n",
            "Epoch [21/100], Step [801/891], Loss: 9158.5596\n",
            "Epoch [22/100], Step [1/891], Loss: 9460.7266\n",
            "Epoch [22/100], Step [101/891], Loss: 8904.3828\n",
            "Epoch [22/100], Step [201/891], Loss: 9321.6904\n",
            "Epoch [22/100], Step [301/891], Loss: 8805.4023\n",
            "Epoch [22/100], Step [401/891], Loss: 8952.8340\n",
            "Epoch [22/100], Step [501/891], Loss: 9339.8994\n",
            "Epoch [22/100], Step [601/891], Loss: 8892.2500\n",
            "Epoch [22/100], Step [701/891], Loss: 9199.3262\n",
            "Epoch [22/100], Step [801/891], Loss: 9088.8848\n",
            "Epoch [23/100], Step [1/891], Loss: 8984.1621\n",
            "Epoch [23/100], Step [101/891], Loss: 9055.6182\n",
            "Epoch [23/100], Step [201/891], Loss: 8675.0449\n",
            "Epoch [23/100], Step [301/891], Loss: 9219.8545\n",
            "Epoch [23/100], Step [401/891], Loss: 8621.8945\n",
            "Epoch [23/100], Step [501/891], Loss: 8798.5020\n",
            "Epoch [23/100], Step [601/891], Loss: 9365.7666\n",
            "Epoch [23/100], Step [701/891], Loss: 8721.2314\n",
            "Epoch [23/100], Step [801/891], Loss: 9224.1016\n",
            "Epoch [24/100], Step [1/891], Loss: 9224.6201\n",
            "Epoch [24/100], Step [101/891], Loss: 9169.2578\n",
            "Epoch [24/100], Step [201/891], Loss: 9234.1855\n",
            "Epoch [24/100], Step [301/891], Loss: 9309.5117\n",
            "Epoch [24/100], Step [401/891], Loss: 8918.6641\n",
            "Epoch [24/100], Step [501/891], Loss: 8847.7754\n",
            "Epoch [24/100], Step [601/891], Loss: 8908.8848\n",
            "Epoch [24/100], Step [701/891], Loss: 8907.3447\n",
            "Epoch [24/100], Step [801/891], Loss: 9391.4512\n",
            "Epoch [25/100], Step [1/891], Loss: 8816.4482\n",
            "Epoch [25/100], Step [101/891], Loss: 9162.5449\n",
            "Epoch [25/100], Step [201/891], Loss: 9252.6592\n",
            "Epoch [25/100], Step [301/891], Loss: 9222.5791\n",
            "Epoch [25/100], Step [401/891], Loss: 9170.0117\n",
            "Epoch [25/100], Step [501/891], Loss: 9288.9160\n",
            "Epoch [25/100], Step [601/891], Loss: 8519.6006\n",
            "Epoch [25/100], Step [701/891], Loss: 9150.6875\n",
            "Epoch [25/100], Step [801/891], Loss: 8735.1621\n",
            "Epoch [26/100], Step [1/891], Loss: 9137.2344\n",
            "Epoch [26/100], Step [101/891], Loss: 9206.6562\n",
            "Epoch [26/100], Step [201/891], Loss: 9710.1953\n",
            "Epoch [26/100], Step [301/891], Loss: 8486.7051\n",
            "Epoch [26/100], Step [401/891], Loss: 9180.9648\n",
            "Epoch [26/100], Step [501/891], Loss: 8566.3633\n",
            "Epoch [26/100], Step [601/891], Loss: 8398.8818\n",
            "Epoch [26/100], Step [701/891], Loss: 9053.5225\n",
            "Epoch [26/100], Step [801/891], Loss: 9600.9951\n",
            "Epoch [27/100], Step [1/891], Loss: 8911.3398\n",
            "Epoch [27/100], Step [101/891], Loss: 8821.1191\n",
            "Epoch [27/100], Step [201/891], Loss: 9169.2227\n",
            "Epoch [27/100], Step [301/891], Loss: 8530.7207\n",
            "Epoch [27/100], Step [401/891], Loss: 9285.6045\n",
            "Epoch [27/100], Step [501/891], Loss: 8842.0547\n",
            "Epoch [27/100], Step [601/891], Loss: 8278.8379\n",
            "Epoch [27/100], Step [701/891], Loss: 9152.0020\n",
            "Epoch [27/100], Step [801/891], Loss: 8870.3535\n",
            "Epoch [28/100], Step [1/891], Loss: 8607.9727\n",
            "Epoch [28/100], Step [101/891], Loss: 9351.0254\n",
            "Epoch [28/100], Step [201/891], Loss: 8802.2998\n",
            "Epoch [28/100], Step [301/891], Loss: 9482.3984\n",
            "Epoch [28/100], Step [401/891], Loss: 9028.2559\n",
            "Epoch [28/100], Step [501/891], Loss: 8484.7939\n",
            "Epoch [28/100], Step [601/891], Loss: 8947.4258\n",
            "Epoch [28/100], Step [701/891], Loss: 9383.9883\n",
            "Epoch [28/100], Step [801/891], Loss: 8962.7373\n",
            "Epoch [29/100], Step [1/891], Loss: 8830.7373\n",
            "Epoch [29/100], Step [101/891], Loss: 8852.7256\n",
            "Epoch [29/100], Step [201/891], Loss: 8696.8613\n",
            "Epoch [29/100], Step [301/891], Loss: 9793.7432\n",
            "Epoch [29/100], Step [401/891], Loss: 8509.3047\n",
            "Epoch [29/100], Step [501/891], Loss: 8752.8105\n",
            "Epoch [29/100], Step [601/891], Loss: 8618.0205\n",
            "Epoch [29/100], Step [701/891], Loss: 9132.1924\n",
            "Epoch [29/100], Step [801/891], Loss: 8699.7207\n",
            "Epoch [30/100], Step [1/891], Loss: 8912.0605\n",
            "Epoch [30/100], Step [101/891], Loss: 8976.3203\n",
            "Epoch [30/100], Step [201/891], Loss: 9334.7393\n",
            "Epoch [30/100], Step [301/891], Loss: 9418.5654\n",
            "Epoch [30/100], Step [401/891], Loss: 9129.7256\n",
            "Epoch [30/100], Step [501/891], Loss: 9362.4062\n",
            "Epoch [30/100], Step [601/891], Loss: 8966.0264\n",
            "Epoch [30/100], Step [701/891], Loss: 8968.9258\n",
            "Epoch [30/100], Step [801/891], Loss: 9001.9219\n",
            "Epoch [31/100], Step [1/891], Loss: 9007.3047\n",
            "Epoch [31/100], Step [101/891], Loss: 8710.8037\n",
            "Epoch [31/100], Step [201/891], Loss: 8484.4561\n",
            "Epoch [31/100], Step [301/891], Loss: 9266.2822\n",
            "Epoch [31/100], Step [401/891], Loss: 9280.2295\n",
            "Epoch [31/100], Step [501/891], Loss: 9078.1660\n",
            "Epoch [31/100], Step [601/891], Loss: 9405.8174\n",
            "Epoch [31/100], Step [701/891], Loss: 8859.4434\n",
            "Epoch [31/100], Step [801/891], Loss: 9344.1426\n",
            "Epoch [32/100], Step [1/891], Loss: 8250.3574\n",
            "Epoch [32/100], Step [101/891], Loss: 9041.6904\n",
            "Epoch [32/100], Step [201/891], Loss: 9511.3330\n",
            "Epoch [32/100], Step [301/891], Loss: 8735.8945\n",
            "Epoch [32/100], Step [401/891], Loss: 9063.2119\n",
            "Epoch [32/100], Step [501/891], Loss: 9109.1416\n",
            "Epoch [32/100], Step [601/891], Loss: 9330.5635\n",
            "Epoch [32/100], Step [701/891], Loss: 9284.6475\n",
            "Epoch [32/100], Step [801/891], Loss: 9061.4287\n",
            "Epoch [33/100], Step [1/891], Loss: 8995.1895\n",
            "Epoch [33/100], Step [101/891], Loss: 8908.6699\n",
            "Epoch [33/100], Step [201/891], Loss: 9132.1064\n",
            "Epoch [33/100], Step [301/891], Loss: 8996.9863\n",
            "Epoch [33/100], Step [401/891], Loss: 9093.9160\n",
            "Epoch [33/100], Step [501/891], Loss: 9057.0752\n",
            "Epoch [33/100], Step [601/891], Loss: 8925.9766\n",
            "Epoch [33/100], Step [701/891], Loss: 9051.0195\n",
            "Epoch [33/100], Step [801/891], Loss: 8773.1865\n",
            "Epoch [34/100], Step [1/891], Loss: 9158.0889\n",
            "Epoch [34/100], Step [101/891], Loss: 9294.2881\n",
            "Epoch [34/100], Step [201/891], Loss: 9263.5771\n",
            "Epoch [34/100], Step [301/891], Loss: 9051.6797\n",
            "Epoch [34/100], Step [401/891], Loss: 9153.1650\n",
            "Epoch [34/100], Step [501/891], Loss: 8662.8594\n",
            "Epoch [34/100], Step [601/891], Loss: 8994.4824\n",
            "Epoch [34/100], Step [701/891], Loss: 8702.0986\n",
            "Epoch [34/100], Step [801/891], Loss: 8687.8555\n",
            "Epoch [35/100], Step [1/891], Loss: 9472.3994\n",
            "Epoch [35/100], Step [101/891], Loss: 9550.0645\n",
            "Epoch [35/100], Step [201/891], Loss: 9192.7646\n",
            "Epoch [35/100], Step [301/891], Loss: 8626.3691\n",
            "Epoch [35/100], Step [401/891], Loss: 8710.4434\n",
            "Epoch [35/100], Step [501/891], Loss: 8991.3408\n",
            "Epoch [35/100], Step [601/891], Loss: 8606.5273\n",
            "Epoch [35/100], Step [701/891], Loss: 8740.5059\n",
            "Epoch [35/100], Step [801/891], Loss: 8922.6943\n",
            "Epoch [36/100], Step [1/891], Loss: 9032.1562\n",
            "Epoch [36/100], Step [101/891], Loss: 9246.3027\n",
            "Epoch [36/100], Step [201/891], Loss: 9120.9023\n",
            "Epoch [36/100], Step [301/891], Loss: 9007.2217\n",
            "Epoch [36/100], Step [401/891], Loss: 9116.8379\n",
            "Epoch [36/100], Step [501/891], Loss: 9120.0166\n",
            "Epoch [36/100], Step [601/891], Loss: 9395.5088\n",
            "Epoch [36/100], Step [701/891], Loss: 9280.9727\n",
            "Epoch [36/100], Step [801/891], Loss: 9155.2891\n",
            "Epoch [37/100], Step [1/891], Loss: 8814.4717\n",
            "Epoch [37/100], Step [101/891], Loss: 8419.9922\n",
            "Epoch [37/100], Step [201/891], Loss: 9315.1357\n",
            "Epoch [37/100], Step [301/891], Loss: 9152.2070\n",
            "Epoch [37/100], Step [401/891], Loss: 8792.6982\n",
            "Epoch [37/100], Step [501/891], Loss: 8639.3047\n",
            "Epoch [37/100], Step [601/891], Loss: 8756.8604\n",
            "Epoch [37/100], Step [701/891], Loss: 8890.7031\n",
            "Epoch [37/100], Step [801/891], Loss: 8846.8623\n",
            "Epoch [38/100], Step [1/891], Loss: 8527.6035\n",
            "Epoch [38/100], Step [101/891], Loss: 8785.8691\n",
            "Epoch [38/100], Step [201/891], Loss: 8768.4922\n",
            "Epoch [38/100], Step [301/891], Loss: 8935.2070\n",
            "Epoch [38/100], Step [401/891], Loss: 9014.9551\n",
            "Epoch [38/100], Step [501/891], Loss: 8523.0225\n",
            "Epoch [38/100], Step [601/891], Loss: 8607.0762\n",
            "Epoch [38/100], Step [701/891], Loss: 9229.2188\n",
            "Epoch [38/100], Step [801/891], Loss: 8734.4824\n",
            "Epoch [39/100], Step [1/891], Loss: 8760.7832\n",
            "Epoch [39/100], Step [101/891], Loss: 8602.2178\n",
            "Epoch [39/100], Step [201/891], Loss: 8479.3877\n",
            "Epoch [39/100], Step [301/891], Loss: 9091.1377\n",
            "Epoch [39/100], Step [401/891], Loss: 8911.6709\n",
            "Epoch [39/100], Step [501/891], Loss: 8382.7959\n",
            "Epoch [39/100], Step [601/891], Loss: 9157.6982\n",
            "Epoch [39/100], Step [701/891], Loss: 8767.9844\n",
            "Epoch [39/100], Step [801/891], Loss: 8703.9111\n",
            "Epoch [40/100], Step [1/891], Loss: 8464.0156\n",
            "Epoch [40/100], Step [101/891], Loss: 9573.5693\n",
            "Epoch [40/100], Step [201/891], Loss: 9153.4727\n",
            "Epoch [40/100], Step [301/891], Loss: 8773.7324\n",
            "Epoch [40/100], Step [401/891], Loss: 8817.8301\n",
            "Epoch [40/100], Step [501/891], Loss: 9424.6270\n",
            "Epoch [40/100], Step [601/891], Loss: 9265.2373\n",
            "Epoch [40/100], Step [701/891], Loss: 9553.5176\n",
            "Epoch [40/100], Step [801/891], Loss: 8945.5244\n",
            "Epoch [41/100], Step [1/891], Loss: 9045.9258\n",
            "Epoch [41/100], Step [101/891], Loss: 8602.2207\n",
            "Epoch [41/100], Step [201/891], Loss: 8978.7129\n",
            "Epoch [41/100], Step [301/891], Loss: 8918.1152\n",
            "Epoch [41/100], Step [401/891], Loss: 9257.1650\n",
            "Epoch [41/100], Step [501/891], Loss: 9336.9141\n",
            "Epoch [41/100], Step [601/891], Loss: 8652.1152\n",
            "Epoch [41/100], Step [701/891], Loss: 8955.4365\n",
            "Epoch [41/100], Step [801/891], Loss: 8828.0283\n",
            "Epoch [42/100], Step [1/891], Loss: 9192.2227\n",
            "Epoch [42/100], Step [101/891], Loss: 8624.8740\n",
            "Epoch [42/100], Step [201/891], Loss: 8534.9043\n",
            "Epoch [42/100], Step [301/891], Loss: 9197.6699\n",
            "Epoch [42/100], Step [401/891], Loss: 8289.7051\n",
            "Epoch [42/100], Step [501/891], Loss: 8453.8115\n",
            "Epoch [42/100], Step [601/891], Loss: 9328.4678\n",
            "Epoch [42/100], Step [701/891], Loss: 8476.7266\n",
            "Epoch [42/100], Step [801/891], Loss: 9315.4092\n",
            "Epoch [43/100], Step [1/891], Loss: 9181.9062\n",
            "Epoch [43/100], Step [101/891], Loss: 8813.9941\n",
            "Epoch [43/100], Step [201/891], Loss: 8625.1543\n",
            "Epoch [43/100], Step [301/891], Loss: 9381.9346\n",
            "Epoch [43/100], Step [401/891], Loss: 8271.1504\n",
            "Epoch [43/100], Step [501/891], Loss: 8691.5859\n",
            "Epoch [43/100], Step [601/891], Loss: 8231.7832\n",
            "Epoch [43/100], Step [701/891], Loss: 9190.0225\n",
            "Epoch [43/100], Step [801/891], Loss: 8879.6299\n",
            "Epoch [44/100], Step [1/891], Loss: 9172.6855\n",
            "Epoch [44/100], Step [101/891], Loss: 8676.4131\n",
            "Epoch [44/100], Step [201/891], Loss: 9421.2539\n",
            "Epoch [44/100], Step [301/891], Loss: 9419.5664\n",
            "Epoch [44/100], Step [401/891], Loss: 8963.2832\n",
            "Epoch [44/100], Step [501/891], Loss: 8575.8555\n",
            "Epoch [44/100], Step [601/891], Loss: 8920.2383\n",
            "Epoch [44/100], Step [701/891], Loss: 8532.9727\n",
            "Epoch [44/100], Step [801/891], Loss: 8974.5293\n",
            "Epoch [45/100], Step [1/891], Loss: 8219.1240\n",
            "Epoch [45/100], Step [101/891], Loss: 9365.8438\n",
            "Epoch [45/100], Step [201/891], Loss: 8540.5586\n",
            "Epoch [45/100], Step [301/891], Loss: 9364.5537\n",
            "Epoch [45/100], Step [401/891], Loss: 8733.8027\n",
            "Epoch [45/100], Step [501/891], Loss: 8695.1328\n",
            "Epoch [45/100], Step [601/891], Loss: 9114.3936\n",
            "Epoch [45/100], Step [701/891], Loss: 8419.6152\n",
            "Epoch [45/100], Step [801/891], Loss: 8844.2314\n",
            "Epoch [46/100], Step [1/891], Loss: 8537.8604\n",
            "Epoch [46/100], Step [101/891], Loss: 8848.4062\n",
            "Epoch [46/100], Step [201/891], Loss: 8984.2256\n",
            "Epoch [46/100], Step [301/891], Loss: 8475.2676\n",
            "Epoch [46/100], Step [401/891], Loss: 9252.3701\n",
            "Epoch [46/100], Step [501/891], Loss: 8826.2559\n",
            "Epoch [46/100], Step [601/891], Loss: 9067.6514\n",
            "Epoch [46/100], Step [701/891], Loss: 8899.6934\n",
            "Epoch [46/100], Step [801/891], Loss: 8725.3379\n",
            "Epoch [47/100], Step [1/891], Loss: 9208.6035\n",
            "Epoch [47/100], Step [101/891], Loss: 8852.0176\n",
            "Epoch [47/100], Step [201/891], Loss: 9125.6318\n",
            "Epoch [47/100], Step [301/891], Loss: 8725.7285\n",
            "Epoch [47/100], Step [401/891], Loss: 8899.5186\n",
            "Epoch [47/100], Step [501/891], Loss: 8899.7959\n",
            "Epoch [47/100], Step [601/891], Loss: 8403.9414\n",
            "Epoch [47/100], Step [701/891], Loss: 9711.7715\n",
            "Epoch [47/100], Step [801/891], Loss: 8718.6592\n",
            "Epoch [48/100], Step [1/891], Loss: 8945.2324\n",
            "Epoch [48/100], Step [101/891], Loss: 9021.2812\n",
            "Epoch [48/100], Step [201/891], Loss: 8555.8271\n",
            "Epoch [48/100], Step [301/891], Loss: 8372.3545\n",
            "Epoch [48/100], Step [401/891], Loss: 8673.0996\n",
            "Epoch [48/100], Step [501/891], Loss: 9233.8223\n",
            "Epoch [48/100], Step [601/891], Loss: 8459.9287\n",
            "Epoch [48/100], Step [701/891], Loss: 9191.0879\n",
            "Epoch [48/100], Step [801/891], Loss: 8750.8779\n",
            "Epoch [49/100], Step [1/891], Loss: 9668.0283\n",
            "Epoch [49/100], Step [101/891], Loss: 8755.6836\n",
            "Epoch [49/100], Step [201/891], Loss: 8780.1631\n",
            "Epoch [49/100], Step [301/891], Loss: 8785.0693\n",
            "Epoch [49/100], Step [401/891], Loss: 8584.8135\n",
            "Epoch [49/100], Step [501/891], Loss: 9088.8691\n",
            "Epoch [49/100], Step [601/891], Loss: 8821.6084\n",
            "Epoch [49/100], Step [701/891], Loss: 9390.5791\n",
            "Epoch [49/100], Step [801/891], Loss: 9038.2080\n",
            "Epoch [50/100], Step [1/891], Loss: 9605.1934\n",
            "Epoch [50/100], Step [101/891], Loss: 8557.6846\n",
            "Epoch [50/100], Step [201/891], Loss: 8960.2051\n",
            "Epoch [50/100], Step [301/891], Loss: 8791.6543\n",
            "Epoch [50/100], Step [401/891], Loss: 8656.6133\n",
            "Epoch [50/100], Step [501/891], Loss: 8532.7598\n",
            "Epoch [50/100], Step [601/891], Loss: 8647.7510\n",
            "Epoch [50/100], Step [701/891], Loss: 9072.8125\n",
            "Epoch [50/100], Step [801/891], Loss: 9077.0449\n",
            "Epoch [51/100], Step [1/891], Loss: 9044.4551\n",
            "Epoch [51/100], Step [101/891], Loss: 8866.7168\n",
            "Epoch [51/100], Step [201/891], Loss: 8834.0361\n",
            "Epoch [51/100], Step [301/891], Loss: 8806.6396\n",
            "Epoch [51/100], Step [401/891], Loss: 9109.7988\n",
            "Epoch [51/100], Step [501/891], Loss: 8662.7559\n",
            "Epoch [51/100], Step [601/891], Loss: 9356.6709\n",
            "Epoch [51/100], Step [701/891], Loss: 8819.4023\n",
            "Epoch [51/100], Step [801/891], Loss: 8998.9756\n",
            "Epoch [52/100], Step [1/891], Loss: 9073.5146\n",
            "Epoch [52/100], Step [101/891], Loss: 8465.9482\n",
            "Epoch [52/100], Step [201/891], Loss: 8520.8359\n",
            "Epoch [52/100], Step [301/891], Loss: 8855.1543\n",
            "Epoch [52/100], Step [401/891], Loss: 9187.2930\n",
            "Epoch [52/100], Step [501/891], Loss: 8382.1006\n",
            "Epoch [52/100], Step [601/891], Loss: 9085.2021\n",
            "Epoch [52/100], Step [701/891], Loss: 8817.3516\n",
            "Epoch [52/100], Step [801/891], Loss: 8881.0127\n",
            "Epoch [53/100], Step [1/891], Loss: 8654.1729\n",
            "Epoch [53/100], Step [101/891], Loss: 8868.3477\n",
            "Epoch [53/100], Step [201/891], Loss: 9152.2666\n",
            "Epoch [53/100], Step [301/891], Loss: 9395.3174\n",
            "Epoch [53/100], Step [401/891], Loss: 8759.0049\n",
            "Epoch [53/100], Step [501/891], Loss: 8955.2217\n",
            "Epoch [53/100], Step [601/891], Loss: 8932.0293\n",
            "Epoch [53/100], Step [701/891], Loss: 8794.1484\n",
            "Epoch [53/100], Step [801/891], Loss: 8427.7715\n",
            "Epoch [54/100], Step [1/891], Loss: 8712.9736\n",
            "Epoch [54/100], Step [101/891], Loss: 9168.7988\n",
            "Epoch [54/100], Step [201/891], Loss: 8660.7881\n",
            "Epoch [54/100], Step [301/891], Loss: 8655.1162\n",
            "Epoch [54/100], Step [401/891], Loss: 9017.9268\n",
            "Epoch [54/100], Step [501/891], Loss: 9017.0664\n",
            "Epoch [54/100], Step [601/891], Loss: 8860.9219\n",
            "Epoch [54/100], Step [701/891], Loss: 8992.3223\n",
            "Epoch [54/100], Step [801/891], Loss: 8981.1953\n",
            "Epoch [55/100], Step [1/891], Loss: 8526.5996\n",
            "Epoch [55/100], Step [101/891], Loss: 9165.0605\n",
            "Epoch [55/100], Step [201/891], Loss: 8824.1670\n",
            "Epoch [55/100], Step [301/891], Loss: 8930.4375\n",
            "Epoch [55/100], Step [401/891], Loss: 8664.2588\n",
            "Epoch [55/100], Step [501/891], Loss: 8644.1660\n",
            "Epoch [55/100], Step [601/891], Loss: 9261.2715\n",
            "Epoch [55/100], Step [701/891], Loss: 8594.7773\n",
            "Epoch [55/100], Step [801/891], Loss: 8664.3047\n",
            "Epoch [56/100], Step [1/891], Loss: 8846.1475\n",
            "Epoch [56/100], Step [101/891], Loss: 8798.3477\n",
            "Epoch [56/100], Step [201/891], Loss: 8622.6250\n",
            "Epoch [56/100], Step [301/891], Loss: 8633.1309\n",
            "Epoch [56/100], Step [401/891], Loss: 8124.4844\n",
            "Epoch [56/100], Step [501/891], Loss: 8220.2959\n",
            "Epoch [56/100], Step [601/891], Loss: 8638.1836\n",
            "Epoch [56/100], Step [701/891], Loss: 8721.8438\n",
            "Epoch [56/100], Step [801/891], Loss: 8289.3145\n",
            "Epoch [57/100], Step [1/891], Loss: 8867.2207\n",
            "Epoch [57/100], Step [101/891], Loss: 9163.8906\n",
            "Epoch [57/100], Step [201/891], Loss: 8822.7822\n",
            "Epoch [57/100], Step [301/891], Loss: 9077.6025\n",
            "Epoch [57/100], Step [401/891], Loss: 8852.4512\n",
            "Epoch [57/100], Step [501/891], Loss: 8761.7402\n",
            "Epoch [57/100], Step [601/891], Loss: 8812.4268\n",
            "Epoch [57/100], Step [701/891], Loss: 8486.5078\n",
            "Epoch [57/100], Step [801/891], Loss: 8767.6348\n",
            "Epoch [58/100], Step [1/891], Loss: 8798.5977\n",
            "Epoch [58/100], Step [101/891], Loss: 9074.7129\n",
            "Epoch [58/100], Step [201/891], Loss: 8397.9922\n",
            "Epoch [58/100], Step [301/891], Loss: 8875.7842\n",
            "Epoch [58/100], Step [401/891], Loss: 8561.1113\n",
            "Epoch [58/100], Step [501/891], Loss: 8692.0000\n",
            "Epoch [58/100], Step [601/891], Loss: 9069.1045\n",
            "Epoch [58/100], Step [701/891], Loss: 8972.1338\n",
            "Epoch [58/100], Step [801/891], Loss: 9005.2754\n",
            "Epoch [59/100], Step [1/891], Loss: 8601.2539\n",
            "Epoch [59/100], Step [101/891], Loss: 8666.0117\n",
            "Epoch [59/100], Step [201/891], Loss: 8970.6865\n",
            "Epoch [59/100], Step [301/891], Loss: 9064.7793\n",
            "Epoch [59/100], Step [401/891], Loss: 8545.9189\n",
            "Epoch [59/100], Step [501/891], Loss: 9011.5693\n",
            "Epoch [59/100], Step [601/891], Loss: 8754.4922\n",
            "Epoch [59/100], Step [701/891], Loss: 9191.7217\n",
            "Epoch [59/100], Step [801/891], Loss: 8644.4873\n",
            "Epoch [60/100], Step [1/891], Loss: 9062.8594\n",
            "Epoch [60/100], Step [101/891], Loss: 8644.2090\n",
            "Epoch [60/100], Step [201/891], Loss: 8814.8477\n",
            "Epoch [60/100], Step [301/891], Loss: 8810.3379\n",
            "Epoch [60/100], Step [401/891], Loss: 8993.5674\n",
            "Epoch [60/100], Step [501/891], Loss: 8818.5205\n",
            "Epoch [60/100], Step [601/891], Loss: 8776.6738\n",
            "Epoch [60/100], Step [701/891], Loss: 8533.6377\n",
            "Epoch [60/100], Step [801/891], Loss: 8848.4570\n",
            "Epoch [61/100], Step [1/891], Loss: 8805.1699\n",
            "Epoch [61/100], Step [101/891], Loss: 8641.4717\n",
            "Epoch [61/100], Step [201/891], Loss: 8915.0010\n",
            "Epoch [61/100], Step [301/891], Loss: 9433.9727\n",
            "Epoch [61/100], Step [401/891], Loss: 8779.9609\n",
            "Epoch [61/100], Step [501/891], Loss: 8678.5430\n",
            "Epoch [61/100], Step [601/891], Loss: 8560.2168\n",
            "Epoch [61/100], Step [701/891], Loss: 8140.1777\n",
            "Epoch [61/100], Step [801/891], Loss: 8761.1982\n",
            "Epoch [62/100], Step [1/891], Loss: 9191.8467\n",
            "Epoch [62/100], Step [101/891], Loss: 8788.7998\n",
            "Epoch [62/100], Step [201/891], Loss: 9087.3047\n",
            "Epoch [62/100], Step [301/891], Loss: 9014.5762\n",
            "Epoch [62/100], Step [401/891], Loss: 8937.0332\n",
            "Epoch [62/100], Step [501/891], Loss: 8540.6016\n",
            "Epoch [62/100], Step [601/891], Loss: 8883.3359\n",
            "Epoch [62/100], Step [701/891], Loss: 9046.3301\n",
            "Epoch [62/100], Step [801/891], Loss: 8921.4199\n",
            "Epoch [63/100], Step [1/891], Loss: 9109.8936\n",
            "Epoch [63/100], Step [101/891], Loss: 8603.9131\n",
            "Epoch [63/100], Step [201/891], Loss: 9100.1572\n",
            "Epoch [63/100], Step [301/891], Loss: 8699.7725\n",
            "Epoch [63/100], Step [401/891], Loss: 9011.6270\n",
            "Epoch [63/100], Step [501/891], Loss: 8872.8809\n",
            "Epoch [63/100], Step [601/891], Loss: 8868.7920\n",
            "Epoch [63/100], Step [701/891], Loss: 8818.7080\n",
            "Epoch [63/100], Step [801/891], Loss: 9082.1719\n",
            "Epoch [64/100], Step [1/891], Loss: 8957.4844\n",
            "Epoch [64/100], Step [101/891], Loss: 8780.1445\n",
            "Epoch [64/100], Step [201/891], Loss: 8840.1484\n",
            "Epoch [64/100], Step [301/891], Loss: 9107.4414\n",
            "Epoch [64/100], Step [401/891], Loss: 9271.8213\n",
            "Epoch [64/100], Step [501/891], Loss: 9042.8213\n",
            "Epoch [64/100], Step [601/891], Loss: 8689.5527\n",
            "Epoch [64/100], Step [701/891], Loss: 8794.9023\n",
            "Epoch [64/100], Step [801/891], Loss: 8993.8564\n",
            "Epoch [65/100], Step [1/891], Loss: 8387.7109\n",
            "Epoch [65/100], Step [101/891], Loss: 9144.7461\n",
            "Epoch [65/100], Step [201/891], Loss: 7945.7520\n",
            "Epoch [65/100], Step [301/891], Loss: 8721.5371\n",
            "Epoch [65/100], Step [401/891], Loss: 8789.2637\n",
            "Epoch [65/100], Step [501/891], Loss: 8472.0352\n",
            "Epoch [65/100], Step [601/891], Loss: 8630.8721\n",
            "Epoch [65/100], Step [701/891], Loss: 9058.3252\n",
            "Epoch [65/100], Step [801/891], Loss: 9121.3701\n",
            "Epoch [66/100], Step [1/891], Loss: 9216.9590\n",
            "Epoch [66/100], Step [101/891], Loss: 9347.8633\n",
            "Epoch [66/100], Step [201/891], Loss: 9060.7842\n",
            "Epoch [66/100], Step [301/891], Loss: 8859.8281\n",
            "Epoch [66/100], Step [401/891], Loss: 8664.2852\n",
            "Epoch [66/100], Step [501/891], Loss: 8931.9844\n",
            "Epoch [66/100], Step [601/891], Loss: 9066.1992\n",
            "Epoch [66/100], Step [701/891], Loss: 8855.3916\n",
            "Epoch [66/100], Step [801/891], Loss: 8795.8740\n",
            "Epoch [67/100], Step [1/891], Loss: 8672.5166\n",
            "Epoch [67/100], Step [101/891], Loss: 8690.8662\n",
            "Epoch [67/100], Step [201/891], Loss: 9190.0342\n",
            "Epoch [67/100], Step [301/891], Loss: 8685.7754\n",
            "Epoch [67/100], Step [401/891], Loss: 9110.7627\n",
            "Epoch [67/100], Step [501/891], Loss: 8708.9258\n",
            "Epoch [67/100], Step [601/891], Loss: 8423.0479\n",
            "Epoch [67/100], Step [701/891], Loss: 9289.8281\n",
            "Epoch [67/100], Step [801/891], Loss: 8821.3994\n",
            "Epoch [68/100], Step [1/891], Loss: 9020.9336\n",
            "Epoch [68/100], Step [101/891], Loss: 8932.7910\n",
            "Epoch [68/100], Step [201/891], Loss: 8477.6562\n",
            "Epoch [68/100], Step [301/891], Loss: 8579.2480\n",
            "Epoch [68/100], Step [401/891], Loss: 8649.3730\n",
            "Epoch [68/100], Step [501/891], Loss: 8397.0273\n",
            "Epoch [68/100], Step [601/891], Loss: 8371.3223\n",
            "Epoch [68/100], Step [701/891], Loss: 8779.0010\n",
            "Epoch [68/100], Step [801/891], Loss: 8531.5527\n",
            "Epoch [69/100], Step [1/891], Loss: 8583.5107\n",
            "Epoch [69/100], Step [101/891], Loss: 8410.4102\n",
            "Epoch [69/100], Step [201/891], Loss: 8690.9033\n",
            "Epoch [69/100], Step [301/891], Loss: 9027.2754\n",
            "Epoch [69/100], Step [401/891], Loss: 8695.6240\n",
            "Epoch [69/100], Step [501/891], Loss: 9462.3691\n",
            "Epoch [69/100], Step [601/891], Loss: 8902.1094\n",
            "Epoch [69/100], Step [701/891], Loss: 8622.0205\n",
            "Epoch [69/100], Step [801/891], Loss: 8747.7842\n",
            "Epoch [70/100], Step [1/891], Loss: 8766.8242\n",
            "Epoch [70/100], Step [101/891], Loss: 8826.7832\n",
            "Epoch [70/100], Step [201/891], Loss: 8720.2754\n",
            "Epoch [70/100], Step [301/891], Loss: 8420.5664\n",
            "Epoch [70/100], Step [401/891], Loss: 8727.1514\n",
            "Epoch [70/100], Step [501/891], Loss: 9001.9219\n",
            "Epoch [70/100], Step [601/891], Loss: 8752.1436\n",
            "Epoch [70/100], Step [701/891], Loss: 8664.9902\n",
            "Epoch [70/100], Step [801/891], Loss: 8821.7852\n",
            "Epoch [71/100], Step [1/891], Loss: 8788.6240\n",
            "Epoch [71/100], Step [101/891], Loss: 8978.0996\n",
            "Epoch [71/100], Step [201/891], Loss: 8772.1621\n",
            "Epoch [71/100], Step [301/891], Loss: 9370.7744\n",
            "Epoch [71/100], Step [401/891], Loss: 8885.4961\n",
            "Epoch [71/100], Step [501/891], Loss: 8922.7998\n",
            "Epoch [71/100], Step [601/891], Loss: 8664.3027\n",
            "Epoch [71/100], Step [701/891], Loss: 9339.8955\n",
            "Epoch [71/100], Step [801/891], Loss: 8411.8018\n",
            "Epoch [72/100], Step [1/891], Loss: 8762.5361\n",
            "Epoch [72/100], Step [101/891], Loss: 9092.5234\n",
            "Epoch [72/100], Step [201/891], Loss: 8581.0264\n",
            "Epoch [72/100], Step [301/891], Loss: 8165.0293\n",
            "Epoch [72/100], Step [401/891], Loss: 9009.7969\n",
            "Epoch [72/100], Step [501/891], Loss: 9089.6719\n",
            "Epoch [72/100], Step [601/891], Loss: 8439.8438\n",
            "Epoch [72/100], Step [701/891], Loss: 8739.9004\n",
            "Epoch [72/100], Step [801/891], Loss: 8418.1299\n",
            "Epoch [73/100], Step [1/891], Loss: 8891.1074\n",
            "Epoch [73/100], Step [101/891], Loss: 8999.3408\n",
            "Epoch [73/100], Step [201/891], Loss: 8892.2822\n",
            "Epoch [73/100], Step [301/891], Loss: 8696.4189\n",
            "Epoch [73/100], Step [401/891], Loss: 8289.3145\n",
            "Epoch [73/100], Step [501/891], Loss: 8962.8828\n",
            "Epoch [73/100], Step [601/891], Loss: 9116.4775\n",
            "Epoch [73/100], Step [701/891], Loss: 8896.1367\n",
            "Epoch [73/100], Step [801/891], Loss: 8309.5840\n",
            "Epoch [74/100], Step [1/891], Loss: 8597.7891\n",
            "Epoch [74/100], Step [101/891], Loss: 8339.0820\n",
            "Epoch [74/100], Step [201/891], Loss: 8731.7891\n",
            "Epoch [74/100], Step [301/891], Loss: 8559.8301\n",
            "Epoch [74/100], Step [401/891], Loss: 8457.2949\n",
            "Epoch [74/100], Step [501/891], Loss: 9072.3838\n",
            "Epoch [74/100], Step [601/891], Loss: 8758.2822\n",
            "Epoch [74/100], Step [701/891], Loss: 8921.9707\n",
            "Epoch [74/100], Step [801/891], Loss: 8998.5254\n",
            "Epoch [75/100], Step [1/891], Loss: 8468.1660\n",
            "Epoch [75/100], Step [101/891], Loss: 8981.9902\n",
            "Epoch [75/100], Step [201/891], Loss: 9309.9795\n",
            "Epoch [75/100], Step [301/891], Loss: 9039.3555\n",
            "Epoch [75/100], Step [401/891], Loss: 8993.0801\n",
            "Epoch [75/100], Step [501/891], Loss: 8204.4844\n",
            "Epoch [75/100], Step [601/891], Loss: 9073.3770\n",
            "Epoch [75/100], Step [701/891], Loss: 8832.6201\n",
            "Epoch [75/100], Step [801/891], Loss: 9001.3457\n",
            "Epoch [76/100], Step [1/891], Loss: 7770.8760\n",
            "Epoch [76/100], Step [101/891], Loss: 8544.5254\n",
            "Epoch [76/100], Step [201/891], Loss: 8954.1504\n",
            "Epoch [76/100], Step [301/891], Loss: 9177.1084\n",
            "Epoch [76/100], Step [401/891], Loss: 8795.7930\n",
            "Epoch [76/100], Step [501/891], Loss: 8805.2500\n",
            "Epoch [76/100], Step [601/891], Loss: 9095.5996\n",
            "Epoch [76/100], Step [701/891], Loss: 8742.9668\n",
            "Epoch [76/100], Step [801/891], Loss: 9006.5889\n",
            "Epoch [77/100], Step [1/891], Loss: 8553.4443\n",
            "Epoch [77/100], Step [101/891], Loss: 9589.4404\n",
            "Epoch [77/100], Step [201/891], Loss: 8440.9561\n",
            "Epoch [77/100], Step [301/891], Loss: 8818.7451\n",
            "Epoch [77/100], Step [401/891], Loss: 9447.6836\n",
            "Epoch [77/100], Step [501/891], Loss: 9009.2900\n",
            "Epoch [77/100], Step [601/891], Loss: 9449.6455\n",
            "Epoch [77/100], Step [701/891], Loss: 8783.3506\n",
            "Epoch [77/100], Step [801/891], Loss: 8771.9590\n",
            "Epoch [78/100], Step [1/891], Loss: 8846.1826\n",
            "Epoch [78/100], Step [101/891], Loss: 8125.3535\n",
            "Epoch [78/100], Step [201/891], Loss: 8735.9307\n",
            "Epoch [78/100], Step [301/891], Loss: 8895.6973\n",
            "Epoch [78/100], Step [401/891], Loss: 8864.6016\n",
            "Epoch [78/100], Step [501/891], Loss: 8945.3418\n",
            "Epoch [78/100], Step [601/891], Loss: 9005.4150\n",
            "Epoch [78/100], Step [701/891], Loss: 8864.3779\n",
            "Epoch [78/100], Step [801/891], Loss: 8177.2095\n",
            "Epoch [79/100], Step [1/891], Loss: 8941.9248\n",
            "Epoch [79/100], Step [101/891], Loss: 8710.4023\n",
            "Epoch [79/100], Step [201/891], Loss: 8912.4102\n",
            "Epoch [79/100], Step [301/891], Loss: 9419.8887\n",
            "Epoch [79/100], Step [401/891], Loss: 8820.0635\n",
            "Epoch [79/100], Step [501/891], Loss: 8668.3818\n",
            "Epoch [79/100], Step [601/891], Loss: 9019.5459\n",
            "Epoch [79/100], Step [701/891], Loss: 8298.2314\n",
            "Epoch [79/100], Step [801/891], Loss: 9023.6416\n",
            "Epoch [80/100], Step [1/891], Loss: 8874.8975\n",
            "Epoch [80/100], Step [101/891], Loss: 8494.9697\n",
            "Epoch [80/100], Step [201/891], Loss: 8720.7256\n",
            "Epoch [80/100], Step [301/891], Loss: 8301.6133\n",
            "Epoch [80/100], Step [401/891], Loss: 9217.1777\n",
            "Epoch [80/100], Step [501/891], Loss: 8764.9072\n",
            "Epoch [80/100], Step [601/891], Loss: 9079.7676\n",
            "Epoch [80/100], Step [701/891], Loss: 8947.3037\n",
            "Epoch [80/100], Step [801/891], Loss: 8987.9512\n",
            "Epoch [81/100], Step [1/891], Loss: 8808.9023\n",
            "Epoch [81/100], Step [101/891], Loss: 8781.7607\n",
            "Epoch [81/100], Step [201/891], Loss: 8317.1670\n",
            "Epoch [81/100], Step [301/891], Loss: 9218.1611\n",
            "Epoch [81/100], Step [401/891], Loss: 8732.4287\n",
            "Epoch [81/100], Step [501/891], Loss: 8840.5967\n",
            "Epoch [81/100], Step [601/891], Loss: 7888.6475\n",
            "Epoch [81/100], Step [701/891], Loss: 8401.1455\n",
            "Epoch [81/100], Step [801/891], Loss: 9060.2627\n",
            "Epoch [82/100], Step [1/891], Loss: 8951.6523\n",
            "Epoch [82/100], Step [101/891], Loss: 8942.0684\n",
            "Epoch [82/100], Step [201/891], Loss: 8948.3936\n",
            "Epoch [82/100], Step [301/891], Loss: 9201.8154\n",
            "Epoch [82/100], Step [401/891], Loss: 9454.2510\n",
            "Epoch [82/100], Step [501/891], Loss: 8124.4883\n",
            "Epoch [82/100], Step [601/891], Loss: 8872.5000\n",
            "Epoch [82/100], Step [701/891], Loss: 9237.2246\n",
            "Epoch [82/100], Step [801/891], Loss: 8912.1621\n",
            "Epoch [83/100], Step [1/891], Loss: 8409.8027\n",
            "Epoch [83/100], Step [101/891], Loss: 8104.7236\n",
            "Epoch [83/100], Step [201/891], Loss: 8638.7051\n",
            "Epoch [83/100], Step [301/891], Loss: 8662.0664\n",
            "Epoch [83/100], Step [401/891], Loss: 9063.3418\n",
            "Epoch [83/100], Step [501/891], Loss: 8303.3906\n",
            "Epoch [83/100], Step [601/891], Loss: 9087.3486\n",
            "Epoch [83/100], Step [701/891], Loss: 8942.8076\n",
            "Epoch [83/100], Step [801/891], Loss: 8440.5713\n",
            "Epoch [84/100], Step [1/891], Loss: 8915.4053\n",
            "Epoch [84/100], Step [101/891], Loss: 8908.1602\n",
            "Epoch [84/100], Step [201/891], Loss: 8822.5146\n",
            "Epoch [84/100], Step [301/891], Loss: 9068.2627\n",
            "Epoch [84/100], Step [401/891], Loss: 8748.3242\n",
            "Epoch [84/100], Step [501/891], Loss: 8749.3389\n",
            "Epoch [84/100], Step [601/891], Loss: 8610.8086\n",
            "Epoch [84/100], Step [701/891], Loss: 8522.7129\n",
            "Epoch [84/100], Step [801/891], Loss: 8659.0674\n",
            "Epoch [85/100], Step [1/891], Loss: 8450.5869\n",
            "Epoch [85/100], Step [101/891], Loss: 9116.8477\n",
            "Epoch [85/100], Step [201/891], Loss: 8783.6699\n",
            "Epoch [85/100], Step [301/891], Loss: 9039.3867\n",
            "Epoch [85/100], Step [401/891], Loss: 8701.2803\n",
            "Epoch [85/100], Step [501/891], Loss: 8721.2510\n",
            "Epoch [85/100], Step [601/891], Loss: 9165.4404\n",
            "Epoch [85/100], Step [701/891], Loss: 8948.5020\n",
            "Epoch [85/100], Step [801/891], Loss: 9017.3301\n",
            "Epoch [86/100], Step [1/891], Loss: 9249.7988\n",
            "Epoch [86/100], Step [101/891], Loss: 9206.4150\n",
            "Epoch [86/100], Step [201/891], Loss: 8140.2490\n",
            "Epoch [86/100], Step [301/891], Loss: 9100.5195\n",
            "Epoch [86/100], Step [401/891], Loss: 8603.6543\n",
            "Epoch [86/100], Step [501/891], Loss: 8778.2139\n",
            "Epoch [86/100], Step [601/891], Loss: 9249.1289\n",
            "Epoch [86/100], Step [701/891], Loss: 8509.4424\n",
            "Epoch [86/100], Step [801/891], Loss: 8666.4424\n",
            "Epoch [87/100], Step [1/891], Loss: 8883.0645\n",
            "Epoch [87/100], Step [101/891], Loss: 8629.7705\n",
            "Epoch [87/100], Step [201/891], Loss: 8991.7471\n",
            "Epoch [87/100], Step [301/891], Loss: 9241.5371\n",
            "Epoch [87/100], Step [401/891], Loss: 8713.3262\n",
            "Epoch [87/100], Step [501/891], Loss: 8724.4346\n",
            "Epoch [87/100], Step [601/891], Loss: 9045.7295\n",
            "Epoch [87/100], Step [701/891], Loss: 8599.4609\n",
            "Epoch [87/100], Step [801/891], Loss: 8507.4961\n",
            "Epoch [88/100], Step [1/891], Loss: 8978.8369\n",
            "Epoch [88/100], Step [101/891], Loss: 9436.1240\n",
            "Epoch [88/100], Step [201/891], Loss: 8677.2852\n",
            "Epoch [88/100], Step [301/891], Loss: 8934.7227\n",
            "Epoch [88/100], Step [401/891], Loss: 8788.1689\n",
            "Epoch [88/100], Step [501/891], Loss: 8725.3135\n",
            "Epoch [88/100], Step [601/891], Loss: 8820.7246\n",
            "Epoch [88/100], Step [701/891], Loss: 8919.1992\n",
            "Epoch [88/100], Step [801/891], Loss: 9042.1104\n",
            "Epoch [89/100], Step [1/891], Loss: 9536.0303\n",
            "Epoch [89/100], Step [101/891], Loss: 8869.0938\n",
            "Epoch [89/100], Step [201/891], Loss: 8281.7031\n",
            "Epoch [89/100], Step [301/891], Loss: 8969.6904\n",
            "Epoch [89/100], Step [401/891], Loss: 8817.2988\n",
            "Epoch [89/100], Step [501/891], Loss: 8714.2236\n",
            "Epoch [89/100], Step [601/891], Loss: 8520.9512\n",
            "Epoch [89/100], Step [701/891], Loss: 8922.4414\n",
            "Epoch [89/100], Step [801/891], Loss: 9039.9766\n",
            "Epoch [90/100], Step [1/891], Loss: 9191.3613\n",
            "Epoch [90/100], Step [101/891], Loss: 8286.3018\n",
            "Epoch [90/100], Step [201/891], Loss: 8887.0303\n",
            "Epoch [90/100], Step [301/891], Loss: 8433.3486\n",
            "Epoch [90/100], Step [401/891], Loss: 8614.2930\n",
            "Epoch [90/100], Step [501/891], Loss: 9223.8838\n",
            "Epoch [90/100], Step [601/891], Loss: 8953.1250\n",
            "Epoch [90/100], Step [701/891], Loss: 8494.3750\n",
            "Epoch [90/100], Step [801/891], Loss: 8366.6816\n",
            "Epoch [91/100], Step [1/891], Loss: 8610.7070\n",
            "Epoch [91/100], Step [101/891], Loss: 8625.6660\n",
            "Epoch [91/100], Step [201/891], Loss: 9496.0371\n",
            "Epoch [91/100], Step [301/891], Loss: 8343.8848\n",
            "Epoch [91/100], Step [401/891], Loss: 8820.7236\n",
            "Epoch [91/100], Step [501/891], Loss: 9095.6270\n",
            "Epoch [91/100], Step [601/891], Loss: 8439.8359\n",
            "Epoch [91/100], Step [701/891], Loss: 8812.6992\n",
            "Epoch [91/100], Step [801/891], Loss: 9056.7920\n",
            "Epoch [92/100], Step [1/891], Loss: 8237.4355\n",
            "Epoch [92/100], Step [101/891], Loss: 8581.1738\n",
            "Epoch [92/100], Step [201/891], Loss: 8818.6904\n",
            "Epoch [92/100], Step [301/891], Loss: 8680.3008\n",
            "Epoch [92/100], Step [401/891], Loss: 8735.0762\n",
            "Epoch [92/100], Step [501/891], Loss: 8429.9043\n",
            "Epoch [92/100], Step [601/891], Loss: 8743.3711\n",
            "Epoch [92/100], Step [701/891], Loss: 8648.6133\n",
            "Epoch [92/100], Step [801/891], Loss: 8446.4307\n",
            "Epoch [93/100], Step [1/891], Loss: 8803.0156\n",
            "Epoch [93/100], Step [101/891], Loss: 9155.0684\n",
            "Epoch [93/100], Step [201/891], Loss: 9016.0645\n",
            "Epoch [93/100], Step [301/891], Loss: 8734.7637\n",
            "Epoch [93/100], Step [401/891], Loss: 8272.8945\n",
            "Epoch [93/100], Step [501/891], Loss: 8776.1387\n",
            "Epoch [93/100], Step [601/891], Loss: 9069.4307\n",
            "Epoch [93/100], Step [701/891], Loss: 8589.8447\n",
            "Epoch [93/100], Step [801/891], Loss: 8872.4766\n",
            "Epoch [94/100], Step [1/891], Loss: 9442.8066\n",
            "Epoch [94/100], Step [101/891], Loss: 8864.4297\n",
            "Epoch [94/100], Step [201/891], Loss: 9010.2861\n",
            "Epoch [94/100], Step [301/891], Loss: 8803.8770\n",
            "Epoch [94/100], Step [401/891], Loss: 8836.9688\n",
            "Epoch [94/100], Step [501/891], Loss: 8807.3213\n",
            "Epoch [94/100], Step [601/891], Loss: 8790.2578\n",
            "Epoch [94/100], Step [701/891], Loss: 7989.1865\n",
            "Epoch [94/100], Step [801/891], Loss: 8203.8994\n",
            "Epoch [95/100], Step [1/891], Loss: 8405.5508\n",
            "Epoch [95/100], Step [101/891], Loss: 8789.5635\n",
            "Epoch [95/100], Step [201/891], Loss: 8986.5293\n",
            "Epoch [95/100], Step [301/891], Loss: 8374.2158\n",
            "Epoch [95/100], Step [401/891], Loss: 8980.6738\n",
            "Epoch [95/100], Step [501/891], Loss: 9173.4043\n",
            "Epoch [95/100], Step [601/891], Loss: 8755.8174\n",
            "Epoch [95/100], Step [701/891], Loss: 8698.7080\n",
            "Epoch [95/100], Step [801/891], Loss: 8480.0430\n",
            "Epoch [96/100], Step [1/891], Loss: 9003.4863\n",
            "Epoch [96/100], Step [101/891], Loss: 8922.5664\n",
            "Epoch [96/100], Step [201/891], Loss: 8726.2471\n",
            "Epoch [96/100], Step [301/891], Loss: 8949.6992\n",
            "Epoch [96/100], Step [401/891], Loss: 8621.8584\n",
            "Epoch [96/100], Step [501/891], Loss: 9080.2607\n",
            "Epoch [96/100], Step [601/891], Loss: 8849.3623\n",
            "Epoch [96/100], Step [701/891], Loss: 8436.6465\n",
            "Epoch [96/100], Step [801/891], Loss: 8556.6895\n",
            "Epoch [97/100], Step [1/891], Loss: 8545.3613\n",
            "Epoch [97/100], Step [101/891], Loss: 8646.6406\n",
            "Epoch [97/100], Step [201/891], Loss: 8999.3965\n",
            "Epoch [97/100], Step [301/891], Loss: 8382.4082\n",
            "Epoch [97/100], Step [401/891], Loss: 8450.4863\n",
            "Epoch [97/100], Step [501/891], Loss: 8931.2471\n",
            "Epoch [97/100], Step [601/891], Loss: 8483.0098\n",
            "Epoch [97/100], Step [701/891], Loss: 8841.9277\n",
            "Epoch [97/100], Step [801/891], Loss: 8926.1016\n",
            "Epoch [98/100], Step [1/891], Loss: 8628.3877\n",
            "Epoch [98/100], Step [101/891], Loss: 8791.9805\n",
            "Epoch [98/100], Step [201/891], Loss: 8804.6201\n",
            "Epoch [98/100], Step [301/891], Loss: 9145.9561\n",
            "Epoch [98/100], Step [401/891], Loss: 8749.4756\n",
            "Epoch [98/100], Step [501/891], Loss: 9041.5537\n",
            "Epoch [98/100], Step [601/891], Loss: 8649.1592\n",
            "Epoch [98/100], Step [701/891], Loss: 8872.3340\n",
            "Epoch [98/100], Step [801/891], Loss: 9531.8076\n",
            "Epoch [99/100], Step [1/891], Loss: 8663.9189\n",
            "Epoch [99/100], Step [101/891], Loss: 9287.3994\n",
            "Epoch [99/100], Step [201/891], Loss: 9085.8447\n",
            "Epoch [99/100], Step [301/891], Loss: 8562.1582\n",
            "Epoch [99/100], Step [401/891], Loss: 8573.1562\n",
            "Epoch [99/100], Step [501/891], Loss: 8677.7871\n",
            "Epoch [99/100], Step [601/891], Loss: 8801.2568\n",
            "Epoch [99/100], Step [701/891], Loss: 8095.3379\n",
            "Epoch [99/100], Step [801/891], Loss: 9019.8389\n",
            "Epoch [100/100], Step [1/891], Loss: 8796.1963\n",
            "Epoch [100/100], Step [101/891], Loss: 8443.4961\n",
            "Epoch [100/100], Step [201/891], Loss: 8391.1934\n",
            "Epoch [100/100], Step [301/891], Loss: 8656.0518\n",
            "Epoch [100/100], Step [401/891], Loss: 8389.2393\n",
            "Epoch [100/100], Step [501/891], Loss: 8816.5020\n",
            "Epoch [100/100], Step [601/891], Loss: 8975.9404\n",
            "Epoch [100/100], Step [701/891], Loss: 9027.3857\n",
            "Epoch [100/100], Step [801/891], Loss: 9164.3623\n",
            "Accuracy of the SVM on the test images with 3000 labeled images: 95.59%\n"
          ]
        }
      ],
      "source": [
        "######################## Main ##########################\n",
        "results = []\n",
        "for dataset in [\"FashionMNIST\", \"MNIST\"]:\n",
        "    for n in N_labeled:\n",
        "        # Initialize M1 model, Transductive SVM, optimizer, and data loaders\n",
        "        m1_model = M1(input_dim, hidden_dim, latent_dim)\n",
        "        svm = SVM(kernel)\n",
        "        optimizer = torch.optim.RMSprop(m1_model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "\n",
        "        ul_train_loader, l_train_loader, test_loader = load_data(labeled_size=n, batch_size=batch_size, dataset=dataset)\n",
        "        # Train M1 model\n",
        "        train_m1_model(m1_model, optimizer, ul_train_loader, device, num_epochs, log_interval)\n",
        "        torch.save(m1_model.state_dict(), f'./checkpoints/M1_+_SVM/{dataset}/M1_{n}labeled.pth')\n",
        "\n",
        "        # Extract features from M1 model for SVM\n",
        "        X_train_features, y_train_labels = extract_m1_features(m1_model, l_train_loader, device)\n",
        "\n",
        "        # Train SVM\n",
        "        train_svm(svm, X_train_features, y_train_labels)\n",
        "        with open(f'./checkpoints/M1_+_SVM/{dataset}/svm_{n}labeled.pkl','wb') as f:\n",
        "            pickle.dump(svm,f)\n",
        "\n",
        "        # Evaluate SVM\n",
        "        correct, total = evaluate_svm(m1_model, svm, test_loader, device)\n",
        "        accuracy = 100 * correct / total\n",
        "        print(f'Accuracy of the SVM on the test images with {n} labeled images: {accuracy}%')\n",
        "\n",
        "        results.append([dataset, n, accuracy, round(100 - accuracy,2)])\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#################### Print Results #####################\n",
        "def print_results(results):\n",
        "    table = PrettyTable()\n",
        "    table.field_names = [\"Dataset\", \"N\", \"Accuracy\", \"Percentage Error\"]\n",
        "    for r in results:\n",
        "        table.add_row(r)\n",
        "\n",
        "    print(\"Results table for M1 model + SVM\")\n",
        "    print(table)\n",
        "\n",
        "print_results(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the SVM on the test images with 100 labeled images: 65.09%\n",
            "Accuracy of the SVM on the test images with 600 labeled images: 77.79%\n",
            "Accuracy of the SVM on the test images with 1000 labeled images: 78.93%\n",
            "Accuracy of the SVM on the test images with 3000 labeled images: 81.66%\n",
            "Accuracy of the SVM on the test images with 100 labeled images: 76.66%\n",
            "Accuracy of the SVM on the test images with 600 labeled images: 92.69%\n",
            "Accuracy of the SVM on the test images with 1000 labeled images: 93.59%\n",
            "Accuracy of the SVM on the test images with 3000 labeled images: 95.59%\n",
            "Results table for M1 model + SVM\n",
            "+--------------+------+----------+------------------+\n",
            "|   Dataset    |  N   | Accuracy | Percentage Error |\n",
            "+--------------+------+----------+------------------+\n",
            "| FashionMNIST | 100  |  65.09   |      34.91       |\n",
            "| FashionMNIST | 600  |  77.79   |      22.21       |\n",
            "| FashionMNIST | 1000 |  78.93   |      21.07       |\n",
            "| FashionMNIST | 3000 |  81.66   |      18.34       |\n",
            "|    MNIST     | 100  |  76.66   |      23.34       |\n",
            "|    MNIST     | 600  |  92.69   |       7.31       |\n",
            "|    MNIST     | 1000 |  93.59   |       6.41       |\n",
            "|    MNIST     | 3000 |  95.59   |       4.41       |\n",
            "+--------------+------+----------+------------------+\n"
          ]
        }
      ],
      "source": [
        "##################### Test Model #######################\n",
        "def test():\n",
        "    results = []\n",
        "    for dataset in [\"FashionMNIST\", \"MNIST\"]:\n",
        "        for n in N_labeled:\n",
        "            ul_train_loader, l_train_loader, test_loader = load_data(labeled_size=n, batch_size=batch_size, dataset=dataset)\n",
        "\n",
        "            m1_model = M1(input_dim, hidden_dim, latent_dim)\n",
        "            svm = SVM(kernel)\n",
        "            optimizer = torch.optim.RMSprop(m1_model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "\n",
        "            m1_model.load_state_dict(torch.load(f'./checkpoints/M1_+_SVM/{dataset}/M1_{n}labeled.pth'))\n",
        "            m1_model.eval()\n",
        "            with open(f'./checkpoints/M1_+_SVM/{dataset}/svm_{n}labeled.pkl', 'rb') as f:\n",
        "                svm = pickle.load(f)\n",
        "\n",
        "            correct, total = evaluate_svm(m1_model, svm, test_loader, device)\n",
        "            accuracy = 100 * correct / total\n",
        "            print(f'Accuracy of the SVM on the test images with {n} labeled images: {accuracy}%')\n",
        "\n",
        "            results.append([dataset, n, accuracy, round(100 - accuracy,2)])\n",
        "    \n",
        "    print_results(results)\n",
        "    \n",
        "\n",
        "test()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
